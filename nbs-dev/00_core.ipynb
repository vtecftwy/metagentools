{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# core\n",
    "\n",
    "> This module includes base classes and items usable across the package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from ecutilities.ipython import nb_setup\n",
    "from fastcore.test import test_fail\n",
    "from nbdev import nbdev_export, show_doc\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set autoreload mode\n"
     ]
    }
   ],
   "source": [
    "#| hide\n",
    "nb_setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import json\n",
    "import re\n",
    "import warnings\n",
    "from ecutilities.core import is_type, safe_path\n",
    "from pathlib import Path\n",
    "from typing import Any, Optional"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text file iterators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class TextFileBaseReader:\n",
    "    \"\"\"Iterator going through a text file by chunks of `nlines` lines. Iterator can be reset to file start.\n",
    "    \n",
    "    The class is mainly intented to be extended, in particular for handling sequence files or other type of data files\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        path: str|Path,  # path to the file\n",
    "        nlines: int=3,   # number of lines on one chunk\n",
    "    ):\n",
    "        self.path = safe_path(path)\n",
    "        self.nlines = nlines\n",
    "        self.fp = None\n",
    "        self.reset_iterator()\n",
    "        \n",
    "        # Attributes to manage parsing\n",
    "        # Assumes the iterator generates a dictionary with key/values, and ond\n",
    "        self.text_to_parse_key = None\n",
    "        self.re_rule_name = None\n",
    "        self.re_pattern = None       # regex pattern to use to parse text\n",
    "        self.re_keys = None          # keys (re group names) to parse text\n",
    "\n",
    "    def reset_iterator(self):\n",
    "        \"\"\"Reset the iterator to point to the first line in the file, by recreating a new file handle.\"\"\"\n",
    "        if self.fp is not None:\n",
    "            self.fp.close()\n",
    "        self.fp = open(self.path, 'r')\n",
    "        \n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "    def _safe_readline(self):\n",
    "        \"\"\"Read a new line and handle end of file tasks.\"\"\"\n",
    "        line = self.fp.readline()\n",
    "        if line == '':\n",
    "            self.fp.close()\n",
    "            raise StopIteration()\n",
    "        return line\n",
    "\n",
    "    def __next__(self):\n",
    "        \"\"\"Return one chunk at the time\"\"\"\n",
    "        lines = []\n",
    "        for i in range(self.nlines):\n",
    "            lines.append(self._safe_readline())\n",
    "        return ''.join(lines)\n",
    "    \n",
    "    def print_first_chunks(\n",
    "        self, \n",
    "        nchunks:int=3,  # number of chunks to print\n",
    "    ):\n",
    "        \"\"\"Print the first `nchunk` chunks of text from the file\"\"\"\n",
    "        self.reset_iterator()\n",
    "        for i, chunk in enumerate(self.__iter__()):\n",
    "            if i > nchunks-1: break\n",
    "            print(f\"{self.nlines}-line chunk {i+1}\")\n",
    "            print(chunk)\n",
    "        self.reset_iterator()\n",
    "            \n",
    "    def _parse_text_fn(\n",
    "        self,\n",
    "        txt:str,         # text to parse\n",
    "        pattern:str,     # regex pattern to apply to parse the text\n",
    "        keys:list[str],  # list of keys: keys are both the regex match group names and the corresponding output dict keys\n",
    "    )-> dict:        # parsed metadata in key/value format\n",
    "        \"\"\"Basic parser function parsing metadata from string, using regex pattern. Return a metadata dictionary\"\"\"\n",
    "        \n",
    "        matches = re.match(pattern, txt)\n",
    "        metadata = {}\n",
    "        if matches is not None:\n",
    "            for g in sorted(keys):\n",
    "                m = matches.group(g)\n",
    "                metadata[g] = m.replace('\\t', ' ').strip() if m is not None else None\n",
    "            return metadata\n",
    "        else:\n",
    "            raise ValueError(f\"No match on this line\")\n",
    "\n",
    "    def parse_text(\n",
    "        self,\n",
    "        txt:str,              # text to parse\n",
    "        pattern:str=None,     # If None, uses standard regex pattern to extract metadata, otherwise, uses passed regex\n",
    "        keys:list[str]=None,  # If None, uses standard regex list of keys, otherwise, uses passed list of keys (str)\n",
    "    )-> dict:                 # parsed metadata in key/value format\n",
    "        \"\"\"Parse text using regex pattern and key. Return a metadata dictionary\n",
    "        \n",
    "        The passed text is parsed using the regex pattern. The method return a dictionary in the format:\n",
    "            {\n",
    "                'key_1': 'metadata 1',\n",
    "                'key_2': 'metadata 2',\n",
    "                ...\n",
    "            }\n",
    "        \n",
    "        \"\"\"\n",
    "        if pattern is None and keys is None:\n",
    "            if self.re_pattern is not None and self.re_keys is not None:\n",
    "                return self._parse_text_fn(txt, self.re_pattern, self.re_keys)\n",
    "            else:\n",
    "                raise ValueError('attribute re_pattern and re_keys are still None')\n",
    "        elif pattern is None or keys is None:\n",
    "            raise ValueError('pattern and keys must be either both None or both have a value')\n",
    "        else:\n",
    "            return self._parse_text_fn(txt, pattern, keys)\n",
    "        \n",
    "        \n",
    "    def set_parsing_rules(\n",
    "        self,\n",
    "        pattern: str|bool=None,   # regex pattern to apply to parse the text, search in parsing rules json if None\n",
    "        keys: list[str]=None,     # list of keys/group for regex, search in parsing rules json if None\n",
    "        verbose: bool=False       # when True, provides information on each rule\n",
    "    )-> None:\n",
    "        \"\"\"Set the standard regex parsing rule for the file.\n",
    "        \n",
    "        Rules can be set:\n",
    "        \n",
    "        1. manually by passing specific custom values for `pattern` and `keys`\n",
    "        2. automatically, by testing all parsing rules saved in `parsing_rule.json` \n",
    "        \n",
    "        Automatic selection of parsing rules works by testing each rule saved in `parsing_rule.json` on the first \n",
    "        definition line of the fasta file, and selecting the one rule that generates the most metadata matches.\n",
    "        \n",
    "        Rules consists of two parameters:\n",
    "        \n",
    "        - The regex pattern including one `group` for each metadata item, e.g `(?P<group_name>regex_code)`\n",
    "        - The list of keys, i.e. the list with the name of each regex groups, used as key in the metadata dictionary\n",
    "        \n",
    "        This method updates the three following class attributes: `re_rule_name`, `re_pattern`, `re_keys`\n",
    "      \n",
    "        \"\"\"\n",
    "        # get the first definition line in the file to test the pattern\n",
    "        # in base class, text_to_parse_key is not defined and automatic rule selection cannot be used\n",
    "        # this must be handled in children classes\n",
    "        if self.text_to_parse_key is None:\n",
    "            msg = \"\"\"\n",
    "            `text_to_parse_key` is not defined in this class. \n",
    "            It is not possible to set a parsing rule.\n",
    "            \"\"\"\n",
    "            warnings.warn(msg, category=UserWarning)\n",
    "            return\n",
    "\n",
    "        self.reset_iterator()\n",
    "        first_output = next(self)\n",
    "        text_to_parse = first_output[self.text_to_parse_key]\n",
    "        divider_line = f\"{'-'*80}\"\n",
    "\n",
    "        if pattern is not None and keys is not None:\n",
    "            try:\n",
    "                metadata_dict = self.parse_text(text_to_parse, pattern, keys)\n",
    "                self.re_rule_name = 'Custom Rule'\n",
    "                self.re_pattern = pattern\n",
    "                self.re_keys = keys\n",
    "                if verbose:\n",
    "                    print(divider_line)\n",
    "                    print(f\"Custom rule was set for this instance.\")\n",
    "            except Exception as err: \n",
    "                raise ValueError(f\"The pattern generates the following error:\\n{err}\")\n",
    "                \n",
    "        else:\n",
    "            # Load all existing rules from json file\n",
    "            with open(Path('../parsing_rules.json'), 'r') as fp:\n",
    "                parsing_rules = json.load(fp)\n",
    "                \n",
    "            # test all existing rules and keep the one with highest number of matches\n",
    "            max_nbr_matches = 0\n",
    "            for k, v in parsing_rules.items():\n",
    "                re_pattern = v['pattern']\n",
    "                re_keys = v['keys'].split(' ')\n",
    "                try:\n",
    "                    metadata_dict = self.parse_text(text_to_parse, re_pattern, re_keys)\n",
    "                    nbr_matches = len(metadata_dict)\n",
    "                    if verbose:\n",
    "                        print(divider_line)\n",
    "                        print(f\"Rule <{k}> generated {nbr_matches:,d} matches\")\n",
    "                        print(divider_line)\n",
    "                        print(re_pattern)\n",
    "                        print(re_keys)\n",
    "\n",
    "                    if len(metadata_dict) > max_nbr_matches:\n",
    "                        self.re_pattern = re_pattern\n",
    "                        self.re_keys = re_keys\n",
    "                        self.re_rule_name = k    \n",
    "                except Exception as err:\n",
    "                    if verbose:\n",
    "                        print(divider_line)\n",
    "                        print(f\"Rule <{k}> generated an error\")\n",
    "                        print(err)\n",
    "                    else:\n",
    "                        pass\n",
    "            if self.re_rule_name is None:\n",
    "                msg = \"\"\"\n",
    "        None of the saved parsing rules were able to extract metadata from the first line in this file.\n",
    "        You must set a custom rule (pattern + keys) before parsing text, by using:\n",
    "            `self.set_parsing_rules(custom_pattern, custom_list_of_keys)`\n",
    "                \"\"\"\n",
    "                warnings.warn(msg, category=UserWarning)\n",
    "            \n",
    "            if verbose:\n",
    "                print(divider_line)\n",
    "                print(f\"Selected rule with most matches: {self.re_rule_name}\")\n",
    "\n",
    "            # We used the iterator, now we need to reset it to make all lines available\n",
    "            self.reset_iterator()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/vtecftwy/metagentools/blob/main/metagentools/core.py#L15){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### TextFileBaseReader\n",
       "\n",
       ">      TextFileBaseReader (path:str|pathlib.Path, nlines:int=3)\n",
       "\n",
       "Iterator going through a text file by chunks of `nlines` lines. Iterator can be reset to file start.\n",
       "\n",
       "The class is mainly intented to be extended, in particular for handling sequence files or other type of data files\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| path | str \\| pathlib.Path |  | path to the file |\n",
       "| nlines | int | 3 | number of lines on one chunk |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/vtecftwy/metagentools/blob/main/metagentools/core.py#L15){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### TextFileBaseReader\n",
       "\n",
       ">      TextFileBaseReader (path:str|pathlib.Path, nlines:int=3)\n",
       "\n",
       "Iterator going through a text file by chunks of `nlines` lines. Iterator can be reset to file start.\n",
       "\n",
       "The class is mainly intented to be extended, in particular for handling sequence files or other type of data files\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| path | str \\| pathlib.Path |  | path to the file |\n",
       "| nlines | int | 3 | number of lines on one chunk |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(TextFileBaseReader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once initialized, the iterator runs over each chunk of line(s) in the text file, sequentially."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TCAAAATAATCAGAAATGTTGAACCTAGGGTTGGACACATAATGACCAGC\t76\t0\n",
      "ATTGTTTAACAATTTGTGCTCGTCCCGGTCACCCGCATCCAATCTTGATG\t4\t9\n",
      "AATCTTGTCCTATCCTACCCGCAGGGGAATTGATGATAGANGTGCTTTTA\t181\t0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "p2textfile = Path('data_dev/train_short')\n",
    "it = TextFileBaseReader(path=p2textfile, nlines=3)\n",
    "\n",
    "one_iteration = next(it)\n",
    "\n",
    "print(one_iteration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TCAAAATAATCAGAAATGTTGAACCTAGGGTTGGACACATAATGACCAGC\t76\t0\n",
      "ATTGTTTAACAATTTGTGCTCGTCCCGGTCACCCGCATCCAATCTTGATG\t4\t9\n",
      "AATCTTGTCCTATCCTACCCGCAGGGGAATTGATGATAGANGTGCTTTTA\t181\t0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "it = TextFileBaseReader(path=p2textfile, nlines=3)\n",
    "\n",
    "one_iteration = next(it)\n",
    "print(one_iteration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GGAGCGGAGCCAACCCCTATGCTCACTTGCAACCCAAGGGGCGTTCCAGT\t74\t3\n",
      "TGGATCCTGCGCGGGACGTCCTTTGTCTACGTCCCGTCGGCGCATCCCGC\t60\t3\n",
      "GAGAGACTTACTAAAAAGCTGGCACTTACCATCAGTGTTTCACCTACATG\t44\t0\n",
      "\n",
      "ACACACGACACTAGAGATAATGTGTCAGTGGATTATAAACAAACCAAGTT\t43\t7\n",
      "TTGTAGCATAAGAACTGGTCTTCGCTGAAATTCTTGTCTTGATCTCATCT\t35\t2\n",
      "TGGCCCTGCGGTCTGGGGCCCAGAAGCATATGTCAAGTCCTTTGAGAAGT\t73\t4\n",
      "\n"
     ]
    }
   ],
   "source": [
    "another_iteration = next(it)\n",
    "print(another_iteration)\n",
    "one_more_iteration = next(it)\n",
    "print(one_more_iteration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to access the start of the file, we need to re-initialize the file handle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/vtecftwy/metagentools/blob/main/metagentools/core.py#L38){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### TextFileBaseReader.reset_iterator\n",
       "\n",
       ">      TextFileBaseReader.reset_iterator ()\n",
       "\n",
       "Reset the iterator to point to the first line in the file, by recreating a new file handle."
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/vtecftwy/metagentools/blob/main/metagentools/core.py#L38){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### TextFileBaseReader.reset_iterator\n",
       "\n",
       ">      TextFileBaseReader.reset_iterator ()\n",
       "\n",
       "Reset the iterator to point to the first line in the file, by recreating a new file handle."
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(TextFileBaseReader.reset_iterator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`it.reset_iterator()` creates a new file handle to point to the first line of the file, again, but does not require to create a new instance of the reader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TCAAAATAATCAGAAATGTTGAACCTAGGGTTGGACACATAATGACCAGC\t76\t0\n",
      "ATTGTTTAACAATTTGTGCTCGTCCCGGTCACCCGCATCCAATCTTGATG\t4\t9\n",
      "AATCTTGTCCTATCCTACCCGCAGGGGAATTGATGATAGANGTGCTTTTA\t181\t0\n",
      "\n",
      "GGAGCGGAGCCAACCCCTATGCTCACTTGCAACCCAAGGGGCGTTCCAGT\t74\t3\n",
      "TGGATCCTGCGCGGGACGTCCTTTGTCTACGTCCCGTCGGCGCATCCCGC\t60\t3\n",
      "GAGAGACTTACTAAAAAGCTGGCACTTACCATCAGTGTTTCACCTACATG\t44\t0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "it.reset_iterator()\n",
    "one_iteration = next(it)\n",
    "print(one_iteration)\n",
    "another_iteration = next(it)\n",
    "print(another_iteration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/vtecftwy/metagentools/blob/main/metagentools/core.py#L62){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### TextFileBaseReader.print_first_chunks\n",
       "\n",
       ">      TextFileBaseReader.print_first_chunks (nchunks:int=3)\n",
       "\n",
       "Print the first `nchunk` chunks of text from the file\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| nchunks | int | 3 | number of chunks to print |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/vtecftwy/metagentools/blob/main/metagentools/core.py#L62){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### TextFileBaseReader.print_first_chunks\n",
       "\n",
       ">      TextFileBaseReader.print_first_chunks (nchunks:int=3)\n",
       "\n",
       "Print the first `nchunk` chunks of text from the file\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| nchunks | int | 3 | number of chunks to print |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(TextFileBaseReader.print_first_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3-line chunk 1\n",
      "TCAAAATAATCAGAAATGTTGAACCTAGGGTTGGACACATAATGACCAGC\t76\t0\n",
      "ATTGTTTAACAATTTGTGCTCGTCCCGGTCACCCGCATCCAATCTTGATG\t4\t9\n",
      "AATCTTGTCCTATCCTACCCGCAGGGGAATTGATGATAGANGTGCTTTTA\t181\t0\n",
      "\n",
      "3-line chunk 2\n",
      "GGAGCGGAGCCAACCCCTATGCTCACTTGCAACCCAAGGGGCGTTCCAGT\t74\t3\n",
      "TGGATCCTGCGCGGGACGTCCTTTGTCTACGTCCCGTCGGCGCATCCCGC\t60\t3\n",
      "GAGAGACTTACTAAAAAGCTGGCACTTACCATCAGTGTTTCACCTACATG\t44\t0\n",
      "\n",
      "3-line chunk 3\n",
      "ACACACGACACTAGAGATAATGTGTCAGTGGATTATAAACAAACCAAGTT\t43\t7\n",
      "TTGTAGCATAAGAACTGGTCTTCGCTGAAATTCTTGTCTTGATCTCATCT\t35\t2\n",
      "TGGCCCTGCGGTCTGGGGCCCAGAAGCATATGTCAAGTCCTTTGAGAAGT\t73\t4\n",
      "\n"
     ]
    }
   ],
   "source": [
    "it = TextFileBaseReader(path=p2textfile, nlines=3)\n",
    "\n",
    "it.print_first_chunks(nchunks=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/vtecftwy/metagentools/blob/main/metagentools/core.py#L92){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### TextFileBaseReader.parse_text\n",
       "\n",
       ">      TextFileBaseReader.parse_text (txt:str, pattern:str=None,\n",
       ">                                     keys:list[str]=None)\n",
       "\n",
       "Parse text using regex pattern and key. Return a metadata dictionary\n",
       "\n",
       "The passed text is parsed using the regex pattern. The method return a dictionary in the format:\n",
       "    {\n",
       "        'key_1': 'metadata 1',\n",
       "        'key_2': 'metadata 2',\n",
       "        ...\n",
       "    }\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| txt | str |  | text to parse |\n",
       "| pattern | str | None | If None, uses standard regex pattern to extract metadata, otherwise, uses passed regex |\n",
       "| keys | list | None | If None, uses standard regex list of keys, otherwise, uses passed list of keys (str) |\n",
       "| **Returns** | **dict** |  | **parsed metadata in key/value format** |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/vtecftwy/metagentools/blob/main/metagentools/core.py#L92){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### TextFileBaseReader.parse_text\n",
       "\n",
       ">      TextFileBaseReader.parse_text (txt:str, pattern:str=None,\n",
       ">                                     keys:list[str]=None)\n",
       "\n",
       "Parse text using regex pattern and key. Return a metadata dictionary\n",
       "\n",
       "The passed text is parsed using the regex pattern. The method return a dictionary in the format:\n",
       "    {\n",
       "        'key_1': 'metadata 1',\n",
       "        'key_2': 'metadata 2',\n",
       "        ...\n",
       "    }\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| txt | str |  | text to parse |\n",
       "| pattern | str | None | If None, uses standard regex pattern to extract metadata, otherwise, uses passed regex |\n",
       "| keys | list | None | If None, uses standard regex list of keys, otherwise, uses passed list of keys (str) |\n",
       "| **Returns** | **dict** |  | **parsed metadata in key/value format** |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(TextFileBaseReader.parse_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '2591237', 'nb': '1', 'source': 'ncbi'}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = '>2591237:ncbi:1'\n",
    "pattern = r\"^>(?P<id>\\d+):(?P<source>ncbi):(?P<nb>\\d*)\"\n",
    "keys = \"id source nb\".split(' ')\n",
    "\n",
    "it.parse_text(text, pattern, keys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extending the base class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`TextFileBaseReader` is a base class.\n",
    "\n",
    "The following methods will typically be extended to match data file and other structured text files formats:\n",
    "\n",
    "- `__next__` method in order to customize how the iterator parses files into \"elements\". For instance, in a FASTA file, one element consists of two lines: a \"definition line\" and the sequence itself. Extending `TextFileBaseReader` allows to read pairs of lines sequentially and return an element as a dictionary. For instance, `FastaFileReader` iterate over the pairs of lines in a Fasta file and return each pairs as the following dictionary:\n",
    "\n",
    "```\n",
    "    {\n",
    "    'definition line': '>2591237:ncbi:1 [MK211378]\\t2591237\\tncbi\\t1 [MK211378] '\n",
    "                    '2591237\\tCoronavirus BtRs-BetaCoV/YN2018D\\t\\tscientific '\n",
    "                    'name\\n',\n",
    "    'sequence': 'TATTAGGTTTTCTACCTACCCAGGA'\n",
    "    }\n",
    "```\n",
    "- Methods for parsing metadata from the file. For instance, `parse_file` method will handle how the reader will iterate over the full file and return a dictionary for the entire file. \n",
    "- Extended classes will also define a few attributes (`text_to_parse_key`, `re_pattern`, `re_keys`, ...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/vtecftwy/metagentools/blob/main/metagentools/core.py#L119){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### TextFileBaseReader.set_parsing_rules\n",
       "\n",
       ">      TextFileBaseReader.set_parsing_rules (pattern:str|bool=None,\n",
       ">                                            keys:list[str]=None,\n",
       ">                                            verbose:bool=False)\n",
       "\n",
       "Set the standard regex parsing rule for the file.\n",
       "\n",
       "Rules can be set:\n",
       "\n",
       "1. manually by passing specific custom values for `pattern` and `keys`\n",
       "2. automatically, by testing all parsing rules saved in `parsing_rule.json` \n",
       "\n",
       "Automatic selection of parsing rules works by testing each rule saved in `parsing_rule.json` on the first \n",
       "definition line of the fasta file, and selecting the one rule that generates the most metadata matches.\n",
       "\n",
       "Rules consists of two parameters:\n",
       "\n",
       "- The regex pattern including one `group` for each metadata item, e.g `(?P<group_name>regex_code)`\n",
       "- The list of keys, i.e. the list with the name of each regex groups, used as key in the metadata dictionary\n",
       "\n",
       "This method updates the three following class attributes: `re_rule_name`, `re_pattern`, `re_keys`\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| pattern | str \\| bool | None | regex pattern to apply to parse the text, search in parsing rules json if None |\n",
       "| keys | list | None | list of keys/group for regex, search in parsing rules json if None |\n",
       "| verbose | bool | False | when True, provides information on each rule |\n",
       "| **Returns** | **None** |  |  |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/vtecftwy/metagentools/blob/main/metagentools/core.py#L119){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### TextFileBaseReader.set_parsing_rules\n",
       "\n",
       ">      TextFileBaseReader.set_parsing_rules (pattern:str|bool=None,\n",
       ">                                            keys:list[str]=None,\n",
       ">                                            verbose:bool=False)\n",
       "\n",
       "Set the standard regex parsing rule for the file.\n",
       "\n",
       "Rules can be set:\n",
       "\n",
       "1. manually by passing specific custom values for `pattern` and `keys`\n",
       "2. automatically, by testing all parsing rules saved in `parsing_rule.json` \n",
       "\n",
       "Automatic selection of parsing rules works by testing each rule saved in `parsing_rule.json` on the first \n",
       "definition line of the fasta file, and selecting the one rule that generates the most metadata matches.\n",
       "\n",
       "Rules consists of two parameters:\n",
       "\n",
       "- The regex pattern including one `group` for each metadata item, e.g `(?P<group_name>regex_code)`\n",
       "- The list of keys, i.e. the list with the name of each regex groups, used as key in the metadata dictionary\n",
       "\n",
       "This method updates the three following class attributes: `re_rule_name`, `re_pattern`, `re_keys`\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| pattern | str \\| bool | None | regex pattern to apply to parse the text, search in parsing rules json if None |\n",
       "| keys | list | None | list of keys/group for regex, search in parsing rules json if None |\n",
       "| verbose | bool | False | when True, provides information on each rule |\n",
       "| **Returns** | **None** |  |  |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(TextFileBaseReader.set_parsing_rules)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Important Note**\n",
    ">\n",
    "> Method `set_parsing_rules` is there to allow `TextFileBaseReader`'s descendant classes to automatically select parsing rule by applying rules saved in a json file to a string extracted from the first element in the file.\n",
    ">\n",
    "> It assumes that the iterator returns its elements as dictionaries `{section_name:section, ...}` and not as a pure string. The key `self.text_to_parse_key` will then be used to extract the text to parse for testing the rules.\n",
    "> The base class iterator returns a simple string and `self.text_to_parse_key` is set to `None`.\n",
    ">\n",
    "> To make setting up a default parsing rule for the reader instance, the iterator must return a dictionary and `self.text_to_parse_key` must be set to the key in the dictionary corresponding the the text to parse. \n",
    ">\n",
    "> See implementation in `FastaFileReader`.\n",
    ">\n",
    "> Calling `set_parsing_rules` on a class that does not satisfy with these characteristics will do nothing and return a warning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_12126/1412150687.py:138: UserWarning: \n",
      "            `text_to_parse_key` is not defined in this class. \n",
      "            It is not possible to set a parsing rule.\n",
      "            \n",
      "  warnings.warn(msg, category=UserWarning)\n"
     ]
    }
   ],
   "source": [
    "it.set_parsing_rules()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utility Classes and Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class JsonFileReader:\n",
    "    def __init__(self, path:str|Path # path to the json file\n",
    "                ):\n",
    "        self.path = safe_path(path)\n",
    "        with open(path, 'r') as fp:\n",
    "            self.d = json.load(fp)\n",
    "    \n",
    "    def add_item(self, \n",
    "                 key:str,  # key for the new item\n",
    "                 item:dict # new item to add to the json as a dict\n",
    "                ):\n",
    "        self.d[key] = item\n",
    "        return self.d\n",
    "\n",
    "    def save_to_file(self, path=None):\n",
    "        if path is None: \n",
    "            path = self.path\n",
    "        else:\n",
    "            path = safe_path(path)\n",
    "\n",
    "        with open(path, 'w') as fp:\n",
    "            json.dump(self.d, fp, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'another item': {'keys': 'key key key key', 'pattern': 'another pattern'},\n",
      " 'item 1': {'keys': 'key key key key', 'pattern': 'pattern 1'},\n",
      " 'item 2': {'keys': 'key key key key', 'pattern': 'pattern 2'},\n",
      " 'item 3': {'keys': 'key key key key', 'pattern': 'pattern 3'}}\n"
     ]
    }
   ],
   "source": [
    "jd = JsonFileReader('data_dev/test.json')\n",
    "pprint(jd.d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'item 1': {'pattern': 'pattern 1', 'keys': 'key key key key'},\n",
       " 'item 2': {'pattern': 'pattern 2', 'keys': 'key key key key'},\n",
       " 'item 3': {'pattern': 'pattern 3', 'keys': 'key key key key'},\n",
       " 'another item': {'keys': 'key key key key', 'pattern': 'another pattern'}}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_item = {'keys': 'key key key key', 'pattern': 'another pattern'}\n",
    "jd.add_item(key='another item', item=new_item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jd.save_to_file()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'another item': {'keys': 'key key key key', 'pattern': 'another pattern'},\n",
      " 'item 1': {'keys': 'key key key key', 'pattern': 'pattern 1'},\n",
      " 'item 2': {'keys': 'key key key key', 'pattern': 'pattern 2'},\n",
      " 'item 3': {'keys': 'key key key key', 'pattern': 'pattern 3'}}\n"
     ]
    }
   ],
   "source": [
    "jd = JsonFileReader('data_dev/test.json')\n",
    "pprint(jd.d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deprecated Items"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When any of the following classes and functions os called, it will raise an exception with an error message indicating how to handle the required code refactoring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class TextFileBaseIterator:\n",
    "    \"\"\"`TextFileBaseIterator` is a deprecated class, to be replaced by `TextFileBaseReader`\"\"\"\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        msg = \"\"\"`TextFileBaseIterator` is deprecated. Use `TextFileBaseReader` instead, with same capabilities and more.\"\"\"\n",
    "        raise DeprecationWarning(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "p2textfile = Path('data_dev/train_short')\n",
    "test_fail(TextFileBaseIterator, msg=\"Should generate DeprecationWarning\", contains=\"`TextFileBaseIterator` is deprecated.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
