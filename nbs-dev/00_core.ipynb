{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# core\n",
    "\n",
    "> Base classes, functions and other objects used across the package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from ecutilities.ipython import nb_setup\n",
    "from ecutilities.core import path_to_parent_dir\n",
    "from fastcore.test import test_fail\n",
    "from nbdev import nbdev_export, show_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "Set autoreload mode\n"
     ]
    }
   ],
   "source": [
    "#| hide\n",
    "nb_setup()\n",
    "NBS_ROOT = path_to_parent_dir('nbs')\n",
    "assert NBS_ROOT.is_dir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import configparser\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "import warnings\n",
    "from ecutilities.core import is_type, validate_path, safe_path\n",
    "from IPython.display import display, Markdown, HTML\n",
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "from typing import Any, Optional\n",
    "\n",
    "try: from google.colab import drive\n",
    "except: pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "# Retrieve the package root\n",
    "from metagentools import __file__\n",
    "CODE_ROOT = Path(__file__).parents[0]\n",
    "PACKAGE_ROOT = Path(__file__).parents[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This module includes all base classes, functions and other objects that are used across the package. It is imported by all other modules in the package.\n",
    "\n",
    "`core` includes utility classes and functions to make it easier to work with the complex file systems adopted for the project, as well as base classes such as a file reader with additional functionality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utility Classes and Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling files and file structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utility classes to represent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class ProjectFileSystem:\n",
    "    \"\"\"Represent a project file system, return paths to key directories, provide methods to manage the file system.\n",
    "\n",
    "    - Paths to key directories are based on whether the code is running locally or in the cloud.\n",
    "    - First time it is used on a local computer, it must be registered as local and a project root path must be set.\n",
    "    - A user configuration file is created in the user's home directory to store the project root path and whether the machine is local or not.\n",
    "\n",
    "    >Technical note: `ProjectFileSystem` is a simpleton class\n",
    "    \"\"\"\n",
    "\n",
    "    _instance = None\n",
    "    _config_dir = '.metagentools'\n",
    "    _config_fname = 'metagentools.cfg'\n",
    "    _shared_project_dir = 'Metagenomics'\n",
    "    \n",
    "    def __new__(cls, *args, **kwargs):\n",
    "        # Create instance if it does not exist yet\n",
    "        if cls._instance is None:\n",
    "            cls.home = Path.home().resolve()\n",
    "            cls.p2config = cls.home / cls._config_dir / cls._config_fname\n",
    "            cls._instance = super().__new__(cls)\n",
    "        return cls._instance\n",
    "    \n",
    "    def __init__(\n",
    "        self, \n",
    "        mount_gdrive:bool=True,  # True to mount Google Drive if running on Colab\n",
    "        project_file:Path=None  # Path to the project file. If None, use the one saved in the config file\n",
    "        ):\n",
    "            self.is_colab = 'google.colab' in sys.modules       \n",
    "            if self.is_colab and mount_gdrive:\n",
    "                drive.mount('/content/gdrive')\n",
    "                self.gdrive = Path('/content/gdrive/MyDrive')\n",
    "\n",
    "            self.is_kaggle = 'kaggle_web_client' in sys.modules\n",
    "            if self.is_kaggle:\n",
    "                raise NotImplemented(f\"ProjectFileSystem is not implemented for Kaggle yet\")\n",
    "\n",
    "            if not self.is_colab and not self.is_kaggle and not self.is_local:\n",
    "                msg = \"\"\"\n",
    "                      Code does not seem to run on the cloud but computer is not registered as local\n",
    "                      If you are running on a local computer, you must register it as local by running\n",
    "                        `ProjectFileSystem().register_as_local()`\n",
    "                      before you can use the ProjectFileSystem class.\n",
    "                      \"\"\"\n",
    "                warnings.warn(msg, UserWarning)\n",
    "\n",
    "            self._project_root = Path()\n",
    "            if self.is_local:\n",
    "                cfg = self.read_config()\n",
    "                path_str = cfg.get('Infra', 'project_root', fallback=None)\n",
    "                if path_str is None: \n",
    "                    msg = \"\"\"\n",
    "                    Project root is not yet set in config file.\n",
    "                    To set it, use `ProjectFileSystem().set_project_root()`.\n",
    "                    \"\"\"\n",
    "                    warnings.warn(msg)\n",
    "                else:\n",
    "                    self._project_root = Path(path_str)\n",
    "\n",
    "    def __call__(self): return self.is_local\n",
    "\n",
    "    def info(self):\n",
    "        \"\"\"Print basic info on the file system and the device\"\"\"\n",
    "        print(f\"Running {self.os} on {self.running_on}\")\n",
    "        print(f\"Device's home directory: {self.home}\")\n",
    "        print(f\"Project file structure:\")\n",
    "        print(f\" - Root ........ {self.project_root} \\n - Data Dir .... {self.data} \\n - Notebooks ... {self.nbs}\")\n",
    "    \n",
    "    def read_config(self):\n",
    "        \"\"\"Read config from the configuration file if it exists and return an empty config if does not\"\"\"\n",
    "        cfg = configparser.ConfigParser()\n",
    "        if self.p2config.is_file(): \n",
    "            cfg.read(self.p2config)\n",
    "        else:\n",
    "            cfg.add_section('Infra')\n",
    "        return cfg\n",
    "\n",
    "    def register_as_local(self):\n",
    "        \"\"\"Update the configuration file to register the machine as local machine\"\"\"\n",
    "        cfg = self.read_config()\n",
    "        os.makedirs(self.home/self._config_dir, exist_ok=True)\n",
    "        cfg['Infra']['registered_as_local'] = 'True'\n",
    "        with open(self.p2config, 'w') as fp:\n",
    "            cfg.write(fp)\n",
    "        return cfg\n",
    "\n",
    "    def set_project_root(\n",
    "        self, \n",
    "        p2project: str|Path   # string or Path to the project directory. Can be absolute or relative to home\n",
    "        ):\n",
    "        \"\"\"Update the configuration file to set the project root\"\"\"\n",
    "        # Build and validate the path to the project root\n",
    "        if isinstance(p2project, str): \n",
    "            p2project = Path(p2project)\n",
    "            if not p2project.is_absolute():\n",
    "                p2project = self.home / p2project\n",
    "        if not p2project.is_dir(): raise FileNotFoundError(f\"{p2project.absolute()} does not exist\")\n",
    "        \n",
    "        # Update the configuration file        \n",
    "        cfg = self.read_config()\n",
    "        os.makedirs(self.home/self._config_dir, exist_ok=True)\n",
    "        cfg['Infra']['project_root'] = str(p2project.absolute())\n",
    "        with open(self.p2config, 'w') as fp:\n",
    "            cfg.write(fp)\n",
    "        self._project_root = p2project\n",
    "        print(f\"Project Root set to {p2project.absolute()}\")\n",
    "        return cfg\n",
    "\n",
    "    @property\n",
    "    def os(self): return sys.platform\n",
    "\n",
    "    @property\n",
    "    def project_root(self):\n",
    "        if self.is_local:\n",
    "            return self._project_root\n",
    "        elif self.is_colab:\n",
    "            return self.gdrive / self._shared_project_dir\n",
    "        elif self.is_kaggle:\n",
    "            raise NotImplemented(f\"ProjectFileSystem is not implemented for Kaggle yet\")\n",
    "        else:\n",
    "            raise ValueError('Not running locally, on Colab or on Kaggle')\n",
    "\n",
    "    @property\n",
    "    def data(self): return self.project_root / 'data'\n",
    "\n",
    "    @property\n",
    "    def nbs(self): return self.project_root / 'nbs'        \n",
    "\n",
    "    @property\n",
    "    def p2config(self): return self.home / self._config_dir / self._config_fname\n",
    "        \n",
    "    @property\n",
    "    def is_local(self):\n",
    "        \"\"\"Return `True` if the current machine was registered as a local machine\"\"\"\n",
    "        cfg = self.read_config()\n",
    "        return cfg['Infra'].getboolean('registered_as_local', False)\n",
    "\n",
    "    @property\n",
    "    def running_on(self):\n",
    "        \"\"\"Return the device on which this is run: local, colab, kaggle, ...\"\"\"\n",
    "        if self.is_local: device = 'local computer'\n",
    "        elif self.is_colab: device = 'colab'\n",
    "        elif self.is_kaggle: device = 'kaggle'\n",
    "        else: device = 'unknown cloud server'\n",
    "        return device\n",
    "    \n",
    "    def readme(\n",
    "        self, \n",
    "        dir_path:Path =None, # Path to the directory to inquire. If None, display readme file from project_root.\n",
    "        ):\n",
    "        \"\"\"Display `readme.md` file or any other `.md` file in `dir_path`. \n",
    "\n",
    "        This provides a convenient way to get information on each direcotry content\n",
    "        \"\"\"\n",
    "        if dir_path is None: \n",
    "            path = self.data\n",
    "        elif validate_path(dir_path, path_type='dir'):\n",
    "            path = dir_path\n",
    "        else:\n",
    "            raise ValueError(f\"'dir_path' is not a directory: {dir_path.absolute()}\")\n",
    "\n",
    "        \n",
    "        if path.is_relative_to(self.project_root):\n",
    "            path_to_display = path.relative_to(self.project_root)\n",
    "        else:\n",
    "            path_to_display = path.absolute()\n",
    "        display(HTML('<hr>'))\n",
    "        # display(Markdown(f\"ReadMe file for directory `{path.relative_to(self.project_root)}`:\"))\n",
    "        display(Markdown(f\"ReadMe file for directory `{path_to_display}`:\"))\n",
    "        mdfiles = {p.stem: p for p in path.glob('*.md')}\n",
    "        if mdfiles:\n",
    "            mdfile = mdfiles.get('readme', None)\n",
    "            if mdfile is None:\n",
    "                mdfile = mdfiles.get(list(mdfiles.keys())[0])\n",
    "            display(HTML('<hr>'))\n",
    "            display(Markdown(filename=mdfile))\n",
    "            display(HTML('<hr>'))\n",
    "        else:\n",
    "            print('No markdown file in this folder')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/vtecftwy/metagentools/blob/main/metagentools/core.py#L55){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### ProjectFileSystem\n",
       "\n",
       ">      ProjectFileSystem (*args, **kwargs)\n",
       "\n",
       "Represent a project file system, return paths to key directories, provide methods to manage the file system.\n",
       "\n",
       "- Paths to key directories are based on whether the code is running locally or in the cloud.\n",
       "- First time it is used on a local computer, it must be registered as local and a project root path must be set.\n",
       "- A user configuration file is created in the user's home directory to store the project root path and whether the machine is local or not.\n",
       "\n",
       "Technical note: this is a simpleton class"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/vtecftwy/metagentools/blob/main/metagentools/core.py#L55){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### ProjectFileSystem\n",
       "\n",
       ">      ProjectFileSystem (*args, **kwargs)\n",
       "\n",
       "Represent a project file system, return paths to key directories, provide methods to manage the file system.\n",
       "\n",
       "- Paths to key directories are based on whether the code is running locally or in the cloud.\n",
       "- First time it is used on a local computer, it must be registered as local and a project root path must be set.\n",
       "- A user configuration file is created in the user's home directory to store the project root path and whether the machine is local or not.\n",
       "\n",
       "Technical note: this is a simpleton class"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(ProjectFileSystem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reference Project File System**:\n",
    "\n",
    "This project adopts a unified file structure to make coding and colaboration easier. In addition, we can run the code locally (from a `project-root` directory) or in the cloud (colab, kaggle, others).\n",
    "\n",
    "The unified file structure when running localy is:\n",
    "```text\n",
    "    project-root   \n",
    "        |--- data\n",
    "        |      |--- CNN_Virus_data  (all data from CNN Virus original paper)\n",
    "        |      |--- saved           (trained and finetuned models, saved preprocessed datasets)\n",
    "        |      |--- ....            (raw or pre-processed data from various sources, results, ... )  \n",
    "        |      \n",
    "        |--- nbs  (all reference and work notebooks)\n",
    "        |      |--- cnn_virus\n",
    "        |      |        |--- notebooks.ipynb\n",
    "```\n",
    "\n",
    "When running on *google colab*, it is assumed that a google drive is mounted on the colab server instance, and that this google drive root includes a shortcut named `Metagenomics` and pointing to the project shared directory. The project shared directory is accessible [here](/https://drive.google.com/drive/folders/134uei5fmt08TpzhmjG4sW0FQ06kn2ZfZ) if you are an authorized project member."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`ProjectFileSystem` at work**:\n",
    "\n",
    "If you use this class for the first time on a local computer, read the two **Important Notes** below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "pfs = ProjectFileSystem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "if not pfs.is_local and not pfs.is_kaggle: \n",
    "    pfs.register_as_local()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once created, the instance of `ProjectFileSystem` gives access to key directories' paths:\n",
    "\n",
    "- `project root`: `Path` to the project root directory\n",
    "- `data`: `Path` to the data directory\n",
    "- `nbs`: `Path` to the notebooks directory\n",
    "\n",
    "It also provides additional information regarding the computer on which the code is running:\n",
    "\n",
    "- `os`: a string providing the name of the operating system the code is running on\n",
    "- `is_colab`: True if the code is running on google colab\n",
    "- `is_kaggle`: True if the code is running on kaggle server (NOT IMPLEMENTED YET)\n",
    "- `is_local`: True if the code is running on a computer registered as local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/vtec/projects/bio/metagentools\n",
      "/home/vtec/projects/bio/metagentools/data\n",
      "/home/vtec/projects/bio/metagentools/nbs\n"
     ]
    }
   ],
   "source": [
    "for p in [pfs.project_root, pfs.data, pfs.nbs]:\n",
    "    print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OSL linux\n",
      "Local: True, Colab: False, Kaggle: False\n"
     ]
    }
   ],
   "source": [
    "print(f\"OSL {pfs.os}\")\n",
    "print(f\"Local: {pfs.is_local}, Colab: {pfs.is_colab}, Kaggle: {pfs.is_kaggle}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/vtecftwy/metagentools/blob/main/metagentools/core.py#L109){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### ProjectFileSystem.info\n",
       "\n",
       ">      ProjectFileSystem.info ()\n",
       "\n",
       "Print basic info on the file system and the device"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/vtecftwy/metagentools/blob/main/metagentools/core.py#L109){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### ProjectFileSystem.info\n",
       "\n",
       ">      ProjectFileSystem.info ()\n",
       "\n",
       "Print basic info on the file system and the device"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(ProjectFileSystem.info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running linux on local computer\n",
      "Device's home directory: /home/vtec\n",
      "Project file structure:\n",
      " - Root ........ /home/vtec/projects/bio/metagentools \n",
      " - Data Dir .... /home/vtec/projects/bio/metagentools/data \n",
      " - Notebooks ... /home/vtec/projects/bio/metagentools/nbs\n"
     ]
    }
   ],
   "source": [
    "pfs.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/vtecftwy/metagentools/blob/main/metagentools/core.py#L194){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### ProjectFileSystem.readme\n",
       "\n",
       ">      ProjectFileSystem.readme (dir_path=None)\n",
       "\n",
       "Display `readme.md` file or any other `.md` file in `dir_path`, to get information on the directory content"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/vtecftwy/metagentools/blob/main/metagentools/core.py#L194){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### ProjectFileSystem.readme\n",
       "\n",
       ">      ProjectFileSystem.readme (dir_path=None)\n",
       "\n",
       "Display `readme.md` file or any other `.md` file in `dir_path`, to get information on the directory content"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(ProjectFileSystem.readme)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<hr>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "ReadMe file for directory `/home/vtec/projects/bio/metagentools/nbs-dev/data_dev`:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<hr>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### Data directory for `metagentools` development \n",
       "This directory includes all the data required to test and validate `metagentools` code.\n",
       "\n",
       "```text\n",
       "data_dev\n",
       " |--- paired_1seq_150bp\n",
       " |--- single_1seq_150bp\n",
       " |--- 50mer_ds_100_seq\n",
       " |--- 150mer_ds_100_seq\n",
       " |--- another_sequence.fa\n",
       " |--- ....           \n",
       " |--- jsondict-test.json\n",
       " |--- readme.md           \n",
       " |--- .... \n",
       " |--- train_short     \n",
       " |--- val_short\n",
       " |--- weight_of_classes\n",
       "     \n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<hr>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pfs.readme(Path('data_dev'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Important Note 1**:\n",
    ">\n",
    ">When using the package on a local computer for the **first time**, you must register the computer as a local computer. Otherwise, `ProjectFileSystem` will raise an error. Once registered, the configuration file will be updated and `ProjectFileSystem` will detect that and run without error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/vtecftwy/metagentools/blob/main/metagentools/core.py#L125){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### ProjectFileSystem.register_as_local\n",
       "\n",
       ">      ProjectFileSystem.register_as_local ()\n",
       "\n",
       "Update the configuration file to register the machine as local machine"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/vtecftwy/metagentools/blob/main/metagentools/core.py#L125){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### ProjectFileSystem.register_as_local\n",
       "\n",
       ">      ProjectFileSystem.register_as_local ()\n",
       "\n",
       "Update the configuration file to register the machine as local machine"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(ProjectFileSystem.register_as_local)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = pfs.register_as_local()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Important Note 2**:\n",
    ">\n",
    ">When using the package on a local computer for the **first time**, it is also required to *set the project root directory*. This is necessary to allow users to locate their local project folder anywhare. Once set, the path to the project root will be saved in the configuratin file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/vtecftwy/metagentools/blob/main/metagentools/core.py#L134){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### ProjectFileSystem.set_project_root\n",
       "\n",
       ">      ProjectFileSystem.set_project_root (p2project:str|pathlib.Path)\n",
       "\n",
       "Update the configuration file to set the project root\n",
       "\n",
       "|    | **Type** | **Details** |\n",
       "| -- | -------- | ----------- |\n",
       "| p2project | str \\| pathlib.Path | string or Path to the project directory. Can be absolute or relative to home |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/vtecftwy/metagentools/blob/main/metagentools/core.py#L134){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### ProjectFileSystem.set_project_root\n",
       "\n",
       ">      ProjectFileSystem.set_project_root (p2project:str|pathlib.Path)\n",
       "\n",
       "Update the configuration file to set the project root\n",
       "\n",
       "|    | **Type** | **Details** |\n",
       "| -- | -------- | ----------- |\n",
       "| p2project | str \\| pathlib.Path | string or Path to the project directory. Can be absolute or relative to home |"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(ProjectFileSystem.set_project_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project Root set to /home/vtec/projects/bio/metagentools\n"
     ]
    }
   ],
   "source": [
    "pfs.set_project_root('/home/vtec/projects/bio/metagentools/');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/vtecftwy/metagentools/blob/main/metagentools/core.py#L116){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### ProjectFileSystem.read_config\n",
       "\n",
       ">      ProjectFileSystem.read_config ()\n",
       "\n",
       "Read config from the configuration file if it exists and return an empty config if does not"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/vtecftwy/metagentools/blob/main/metagentools/core.py#L116){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### ProjectFileSystem.read_config\n",
       "\n",
       ">      ProjectFileSystem.read_config ()\n",
       "\n",
       "Read config from the configuration file if it exists and return an empty config if does not"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(ProjectFileSystem.read_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'True'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg = pfs.read_config()\n",
    "cfg['Infra']['registered_as_local']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/vtec/projects/bio/metagentools'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg['Infra']['project_root']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other utility classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "class JsonDict(dict):\n",
    "    \"\"\"Dictionary whose current value is mirrored in a json file and can be initated from a json file\n",
    "    \n",
    "    `JsonDict` requires a path to json file at creation. An optional dict can be passed as argument.\n",
    "\n",
    "    Behavior at creation:\n",
    "    \n",
    "    - `JsonDict(p2json, dict)` will create a `JsonDict` with key-values from `dict`, and mirrored in `p2json`\n",
    "    - `JsonDict(p2json)` will create a `JsonDict` with empty dictionary and load json content if file exists\n",
    "\n",
    "    Once created, `JsonDict` instances behave exactly as a dictionary\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self, \n",
    "        p2json: str|Path,       # path to the json file to mirror with the dictionary \n",
    "        dictionary: dict =None  # optional dictionary to initialize the JsonDict\n",
    "        ):\n",
    "        \"\"\"Create dict from a passed dict or from json. Create the json file if required\"\"\"\n",
    "        self.p2json = Path(p2json) if isinstance(p2json, str) else p2json\n",
    "        if dictionary is None:\n",
    "            if self.p2json.is_file():\n",
    "                dictionary = self.load()\n",
    "                self.initial_dict_from_json = True\n",
    "            else:\n",
    "                dictionary = dict()\n",
    "                self.initial_dict_from_json = False\n",
    "        super().__init__(dictionary)\n",
    "        self.save()\n",
    "    \n",
    "    def __setitem__(self, __k:Any, v:Any) -> None:\n",
    "        super().__setitem__(__k, v)\n",
    "        self.save()\n",
    "\n",
    "    def __delitem__(self, k:Any):\n",
    "        super().__delitem__(k)\n",
    "        self.save()\n",
    "\n",
    "    def __repr__(self):\n",
    "        txt1 = super().__repr__()\n",
    "        txt2 = f\"\\ndict mirrored in {self.p2json.absolute()}\"\n",
    "        return txt1 + txt2\n",
    "\n",
    "    def load(self):\n",
    "        with open(self.p2json, 'r') as fp:\n",
    "            return json.load(fp)\n",
    "\n",
    "    def save(self):\n",
    "        with open(self.p2json, 'w') as fp:\n",
    "            json.dump(self, fp, indent=4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a new dictionary mirrored to a JSON file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a': 1, 'b': 2, 'c': 3}\n",
       "dict mirrored in /home/vtec/projects/bio/metagentools/nbs-dev/data_dev/jsondict-test.json"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = {'a': 1, 'b': 2, 'c': 3}\n",
    "p2json = Path('data_dev/jsondict-test.json')\n",
    "jsond = JsonDict(p2json, d)\n",
    "jsond"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once created, the `JsonFile` instance behaves exactly like a dictionary, with the added benefit that any change to the dictionary is automatically saved to the JSON file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 2, 3)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jsond['a'], jsond['b'], jsond['c']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "key: a; value: 1\n",
      "key: b; value: 2\n",
      "key: c; value: 3\n"
     ]
    }
   ],
   "source": [
    "for k, v in jsond.items():\n",
    "    print(f\"key: {k}; value: {v}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding or removing a value from the dictionary works in the same way as for a normal dictionary. But the json file is automatically updated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a': 1, 'b': 2, 'c': 3, 'd': 4}\n",
       "dict mirrored in /home/vtec/projects/bio/metagentools/nbs-dev/data_dev/jsondict-test.json"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jsond['d'] = 4\n",
    "jsond"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"a\": 1,\n",
      "    \"b\": 2,\n",
      "    \"c\": 3,\n",
      "    \"d\": 4\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "with open(p2json, 'r') as fp:\n",
    "    print(fp.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'b': 2, 'c': 3, 'd': 4}\n",
       "dict mirrored in /home/vtec/projects/bio/metagentools/nbs-dev/data_dev/jsondict-test.json"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del jsond['a']\n",
    "jsond"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"b\": 2,\n",
      "    \"c\": 3,\n",
      "    \"d\": 4\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "with open(p2json, 'r') as fp:\n",
    "    print(fp.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class JsonFileReader:\n",
    "    \"\"\"Mirror a JSON file and a dictionary\"\"\"\n",
    "    def __init__(self, \n",
    "                 path:str|Path # path to the json file\n",
    "                ):\n",
    "        self.path = safe_path(path)\n",
    "        with open(path, 'r') as fp:\n",
    "            self.d = json.load(fp)\n",
    "    \n",
    "    def add_item(self, \n",
    "                 key:str,  # key for the new item\n",
    "                 item:dict # new item to add to the json as a dict\n",
    "                ):\n",
    "        self.d[key] = item\n",
    "        return self.d\n",
    "\n",
    "    def save_to_file(self, path=None):\n",
    "        if path is None: \n",
    "            path = self.path\n",
    "        else:\n",
    "            path = safe_path(path)\n",
    "\n",
    "        with open(path, 'w') as fp:\n",
    "            json.dump(self.d, fp, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/vtecftwy/metagentools/blob/main/metagentools/core.py#L29){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### JsonFileReader\n",
       "\n",
       ">      JsonFileReader (path:str|pathlib.Path)\n",
       "\n",
       "Mirror a JSON file and a dictionary\n",
       "\n",
       "|    | **Type** | **Details** |\n",
       "| -- | -------- | ----------- |\n",
       "| path | str \\| pathlib.Path | path to the json file |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/vtecftwy/metagentools/blob/main/metagentools/core.py#L29){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### JsonFileReader\n",
       "\n",
       ">      JsonFileReader (path:str|pathlib.Path)\n",
       "\n",
       "Mirror a JSON file and a dictionary\n",
       "\n",
       "|    | **Type** | **Details** |\n",
       "| -- | -------- | ----------- |\n",
       "| path | str \\| pathlib.Path | path to the json file |"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(JsonFileReader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'item 1': {'keys': 'key key key key', 'pattern': 'pattern 1'},\n",
      " 'item 2': {'keys': 'key key key key', 'pattern': 'pattern 2'},\n",
      " 'item 3': {'keys': 'key key key key', 'pattern': 'pattern 3'}}\n"
     ]
    }
   ],
   "source": [
    "jd = JsonFileReader('data_dev/test.json')\n",
    "pprint(jd.d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can add an item to the dictionary/json "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'item 1': {'keys': 'key key key key', 'pattern': 'pattern 1'},\n",
       " 'item 2': {'keys': 'key key key key', 'pattern': 'pattern 2'},\n",
       " 'item 3': {'keys': 'key key key key', 'pattern': 'pattern 3'},\n",
       " 'another item': {'keys': 'key key key key', 'pattern': 'another pattern'}}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "new_item = {'keys': 'key key key key', 'pattern': 'another pattern'}\n",
    "jd.add_item(key='another item', item=new_item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After saving the updated JSON file, we can load it again and see the changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jd.save_to_file()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'another item': {'keys': 'key key key key', 'pattern': 'another pattern'},\n",
      " 'item 1': {'keys': 'key key key key', 'pattern': 'pattern 1'},\n",
      " 'item 2': {'keys': 'key key key key', 'pattern': 'pattern 2'},\n",
      " 'item 3': {'keys': 'key key key key', 'pattern': 'pattern 3'}}\n"
     ]
    }
   ],
   "source": [
    "jd = JsonFileReader('data_dev/test.json')\n",
    "pprint(jd.d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "initial_json = {\n",
    "    'item 1': {'keys': 'key key key key', 'pattern': 'pattern 1'},\n",
    "    'item 2': {'keys': 'key key key key', 'pattern': 'pattern 2'},\n",
    "    'item 3': {'keys': 'key key key key', 'pattern': 'pattern 3'}\n",
    "    }\n",
    "with open('data_dev/test.json', 'w') as fp:\n",
    "    json.dump(initial_json, fp, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Base Classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## File Readers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Base classes to be extended in order to create readers for specific file formats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class TextFileBaseReader:\n",
    "    \"\"\"Iterator going through a text file by chunks of `nlines` lines. Iterator can be reset to file start.\n",
    "    \n",
    "    The class is mainly intented to be extended, as it is for handling sequence files of various formats such as `FastaFileReader`.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        path: str|Path,  # path to the file\n",
    "        nlines: int=1,   # number of lines on one chunk\n",
    "    ):\n",
    "        self.path = safe_path(path)\n",
    "        self.nlines = nlines\n",
    "        self.fp = None\n",
    "        self.reset_iterator()\n",
    "        \n",
    "        # Attributes related to metadata parsing\n",
    "        # Currently assumes the iterator generates a dictionary with key/values\n",
    "        # TODO: extend to iterator output as simple string.\n",
    "        self.text_to_parse_key = None\n",
    "        project_root = ProjectFileSystem().project_root\n",
    "        self.parsing_rules_json = project_root / 'default_parsing_rules.json'\n",
    "        # self.parsing_rules_json = Path(f\"{PACKAGE_ROOT}/default_parsing_rules.json\")\n",
    "        self.re_rule_name = None\n",
    "        self.re_pattern = None       # regex pattern to use to parse text\n",
    "        self.re_keys = None          # keys (re group names) to parse text\n",
    "\n",
    "    def reset_iterator(self):\n",
    "        \"\"\"Reset the iterator to point to the first line in the file.\"\"\"\n",
    "        if self.fp is not None:\n",
    "            self.fp.close()\n",
    "        self.fp = open(self.path, 'r')\n",
    "        \n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "    def _safe_readline(self):\n",
    "        \"\"\"Read a new line and handle end of file tasks.\"\"\"\n",
    "        line = self.fp.readline()\n",
    "        if line == '':\n",
    "            self.fp.close()\n",
    "            raise StopIteration()\n",
    "        return line\n",
    "\n",
    "    def __next__(self):\n",
    "        \"\"\"Return one chunk of `nlines` text lines at the time\"\"\"\n",
    "        lines = []\n",
    "        for i in range(self.nlines):\n",
    "            lines.append(self._safe_readline())\n",
    "        return ''.join(lines)\n",
    "    \n",
    "    def print_first_chunks(\n",
    "        self, \n",
    "        nchunks:int=3,  # number of chunks to print\n",
    "    ):\n",
    "        \"\"\"Print the first `nchunk` chunks of text from the file.\n",
    "\n",
    "        After printing, the iterator is reset again to its first line.\n",
    "        \"\"\"\n",
    "        self.reset_iterator()\n",
    "        for i, chunk in enumerate(self.__iter__()):\n",
    "            if i > nchunks-1: break\n",
    "            print(f\"{self.nlines}-line chunk {i+1}\")\n",
    "            print(chunk)\n",
    "        self.reset_iterator()\n",
    "            \n",
    "    def _parse_text_fn(\n",
    "        self,\n",
    "        txt:str,         # text to parse\n",
    "        pattern:str,     # regex pattern to apply to parse the text\n",
    "        keys:list[str],  # list of keys: keys are both the regex match group names and the corresponding output dict keys\n",
    "    )-> dict:        # parsed metadata in key/value format\n",
    "        \"\"\"Basic parser function parsing metadata from string, using regex pattern. Return a metadata dictionary.\"\"\"\n",
    "        \n",
    "        matches = re.match(pattern, txt)\n",
    "        metadata = {}\n",
    "        if matches is not None:\n",
    "            for g in sorted(keys):\n",
    "                m = matches.group(g)\n",
    "                metadata[g] = m.replace('\\t', ' ').strip() if m is not None else None\n",
    "        \n",
    "        else: \n",
    "            # TODO: review hack below, to avoid error when missing metadata such as 'species name'.\n",
    "            # Current code tries to recover by saving the entire line in the fist key, expected to be the seqid or refseid\n",
    "            # if txt:\n",
    "            #     metadata[keys[0]] = txt.replace('\\t', ' ').strip() if txt is not None else None\n",
    "            # else:\n",
    "            raise ValueError(f\"No match on this line\")\n",
    "        return metadata\n",
    "\n",
    "    def parse_text(\n",
    "        self,\n",
    "        txt:str,                    # text to parse\n",
    "        pattern:str=None,           # If None, uses standard regex pattern to extract metadata, otherwise, uses passed regex\n",
    "        keys:list[str]=None,        # If None, uses standard regex list of keys, otherwise, uses passed list of keys (str)\n",
    "    )-> dict:                       # parsed metadata in key/value format\n",
    "        \"\"\"Parse text using regex pattern and keys. Return a metadata dictionary.\n",
    "        \n",
    "        The passed text is parsed using the regex pattern. The method return a dictionary in the format:\n",
    "\n",
    "            {\n",
    "                'key_1': 'metadata 1',\n",
    "                'key_2': 'metadata 2',\n",
    "                ...\n",
    "            }\n",
    "        \n",
    "        \"\"\"\n",
    "        if pattern is None and keys is None:\n",
    "            if self.re_pattern is not None and self.re_keys is not None:\n",
    "                return self._parse_text_fn(txt, self.re_pattern, self.re_keys)\n",
    "            else:\n",
    "                raise ValueError('attribute re_pattern and re_keys are still None')\n",
    "        elif pattern is None or keys is None:\n",
    "            raise ValueError('pattern and keys must be either both None or both have a value')\n",
    "        else:\n",
    "            return self._parse_text_fn(txt, pattern, keys)\n",
    "        \n",
    "        \n",
    "    def set_parsing_rules(\n",
    "        self,\n",
    "        pattern: str|bool=None,   # regex pattern to apply to parse the text, search in parsing rules json if None\n",
    "        keys: list[str]=None,     # list of keys/group for regex, search in parsing rules json if None\n",
    "        verbose: bool=False       # when True, provides information on each rule\n",
    "    )-> None:\n",
    "        \"\"\"Set the standard regex parsing rule for the file.\n",
    "        \n",
    "        Rules can be set:\n",
    "        \n",
    "        1. manually by passing specific custom values for `pattern` and `keys`\n",
    "        2. automatically, by testing all parsing rules saved in `parsing_rule.json` \n",
    "        \n",
    "        Automatic selection of parsing rules works by testing each rule saved in `parsing_rule.json` on the first \n",
    "        definition line of the fasta file, and selecting the one rule that generates the most metadata matches.\n",
    "        \n",
    "        Rules consists of two parameters:\n",
    "        \n",
    "        - The regex pattern including one `group` for each metadata item, e.g `(?P<group_name>regex_code)`\n",
    "        - The list of keys, i.e. the list with the name of each regex groups, used as key in the metadata dictionary\n",
    "        \n",
    "        This method updates the three following class attributes: `re_rule_name`, `re_pattern`, `re_keys`\n",
    "      \n",
    "        \"\"\"\n",
    "        # get the first definition line in the file to test the pattern\n",
    "        # in base class, text_to_parse_key is not defined and automatic rule selection cannot be used\n",
    "        # this must be handled in children classes\n",
    "        if self.text_to_parse_key is None:\n",
    "            msg = \"\"\"\n",
    "            `text_to_parse_key` is not defined in this class. \n",
    "            It is not possible to set a parsing rule.\n",
    "            \"\"\"\n",
    "            warnings.warn(msg, category=UserWarning)\n",
    "            return\n",
    "\n",
    "        self.reset_iterator()\n",
    "        first_output = next(self)\n",
    "        text_to_parse = first_output[self.text_to_parse_key]\n",
    "        divider_line = f\"{'-'*80}\"\n",
    "\n",
    "        if pattern is not None and keys is not None:\n",
    "            try:\n",
    "                metadata_dict = self.parse_text(text_to_parse, pattern, keys)\n",
    "                self.re_rule_name = 'Custom Rule'\n",
    "                self.re_pattern = pattern\n",
    "                self.re_keys = keys\n",
    "                if verbose:\n",
    "                    print(divider_line)\n",
    "                    print(f\"Custom rule was set for this instance.\")\n",
    "            except Exception as err: \n",
    "                raise ValueError(f\"The pattern generates the following error:\\n{err}\")\n",
    "                \n",
    "        else:\n",
    "            # Load all existing rules from json file\n",
    "            with open(self.parsing_rules_json, 'r') as fp:\n",
    "                parsing_rules = json.load(fp)\n",
    "                \n",
    "            # test all existing rules and keep the one with highest number of matches\n",
    "            max_nbr_matches = 0\n",
    "            for k, v in parsing_rules.items():\n",
    "                re_pattern = v['pattern']\n",
    "                re_keys = v['keys'].split(' ')\n",
    "                try:\n",
    "                    metadata_dict = self.parse_text(text_to_parse, re_pattern, re_keys)\n",
    "                    nbr_matches = len(metadata_dict)\n",
    "                    if verbose:\n",
    "                        print(divider_line)\n",
    "                        print(f\"Rule <{k}> generated {nbr_matches:,d} matches\")\n",
    "                        print(divider_line)\n",
    "                        print(re_pattern)\n",
    "                        print(re_keys)\n",
    "\n",
    "                    if len(metadata_dict) > max_nbr_matches:\n",
    "                        self.re_pattern = re_pattern\n",
    "                        self.re_keys = re_keys\n",
    "                        self.re_rule_name = k    \n",
    "                except Exception as err:\n",
    "                    if verbose:\n",
    "                        print(divider_line)\n",
    "                        print(f\"Rule <{k}> generated an error\")\n",
    "                        print(err)\n",
    "                    else:\n",
    "                        pass\n",
    "            if self.re_rule_name is None:\n",
    "                msg = \"\"\"\n",
    "        None of the saved parsing rules were able to extract metadata from the first line in this file.\n",
    "        You must set a custom rule (pattern + keys) before parsing text, by using:\n",
    "            `self.set_parsing_rules(custom_pattern, custom_list_of_keys)`\n",
    "                \"\"\"\n",
    "                warnings.warn(msg, category=UserWarning)\n",
    "            \n",
    "            if verbose:\n",
    "                print(divider_line)\n",
    "                print(f\"Selected rule with most matches: {self.re_rule_name}\")\n",
    "\n",
    "            # We used the iterator, now we need to reset it to make all lines available\n",
    "            self.reset_iterator()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once initialized, the iterator runs over each chunk of line(s) in the text file, sequentially."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TCAAAATAATCAGAAATGTTGAACCTAGGGTTGGACACATAATGACCAGC\t76\t0\n",
      "ATTGTTTAACAATTTGTGCTCGTCCCGGTCACCCGCATCCAATCTTGATG\t4\t9\n",
      "AATCTTGTCCTATCCTACCCGCAGGGGAATTGATGATAGANGTGCTTTTA\t181\t0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "p2textfile = Path('data_dev/train_short')\n",
    "it = TextFileBaseReader(path=p2textfile, nlines=3)\n",
    "\n",
    "one_iteration = next(it)\n",
    "\n",
    "print(one_iteration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a new instance of the file reader, and get several iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TCAAAATAATCAGAAATGTTGAACCTAGGGTTGGACACATAATGACCAGC\t76\t0\n",
      "ATTGTTTAACAATTTGTGCTCGTCCCGGTCACCCGCATCCAATCTTGATG\t4\t9\n",
      "AATCTTGTCCTATCCTACCCGCAGGGGAATTGATGATAGANGTGCTTTTA\t181\t0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "it = TextFileBaseReader(path=p2textfile, nlines=3)\n",
    "\n",
    "one_iteration = next(it)\n",
    "print(one_iteration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GGAGCGGAGCCAACCCCTATGCTCACTTGCAACCCAAGGGGCGTTCCAGT\t74\t3\n",
      "TGGATCCTGCGCGGGACGTCCTTTGTCTACGTCCCGTCGGCGCATCCCGC\t60\t3\n",
      "GAGAGACTTACTAAAAAGCTGGCACTTACCATCAGTGTTTCACCTACATG\t44\t0\n",
      "\n",
      "ACACACGACACTAGAGATAATGTGTCAGTGGATTATAAACAAACCAAGTT\t43\t7\n",
      "TTGTAGCATAAGAACTGGTCTTCGCTGAAATTCTTGTCTTGATCTCATCT\t35\t2\n",
      "TGGCCCTGCGGTCTGGGGCCCAGAAGCATATGTCAAGTCCTTTGAGAAGT\t73\t4\n",
      "\n"
     ]
    }
   ],
   "source": [
    "another_iteration = next(it)\n",
    "print(another_iteration)\n",
    "one_more_iteration = next(it)\n",
    "print(one_more_iteration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to access the start of the file again, we need to re-initialize the file handle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/vtecftwy/metagentools/blob/main/metagentools/core.py#L245){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### TextFileBaseReader.reset_iterator\n",
       "\n",
       ">      TextFileBaseReader.reset_iterator ()\n",
       "\n",
       "Reset the iterator to point to the first line in the file."
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/vtecftwy/metagentools/blob/main/metagentools/core.py#L245){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### TextFileBaseReader.reset_iterator\n",
       "\n",
       ">      TextFileBaseReader.reset_iterator ()\n",
       "\n",
       "Reset the iterator to point to the first line in the file."
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(TextFileBaseReader.reset_iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TCAAAATAATCAGAAATGTTGAACCTAGGGTTGGACACATAATGACCAGC\t76\t0\n",
      "ATTGTTTAACAATTTGTGCTCGTCCCGGTCACCCGCATCCAATCTTGATG\t4\t9\n",
      "AATCTTGTCCTATCCTACCCGCAGGGGAATTGATGATAGANGTGCTTTTA\t181\t0\n",
      "\n",
      "GGAGCGGAGCCAACCCCTATGCTCACTTGCAACCCAAGGGGCGTTCCAGT\t74\t3\n",
      "TGGATCCTGCGCGGGACGTCCTTTGTCTACGTCCCGTCGGCGCATCCCGC\t60\t3\n",
      "GAGAGACTTACTAAAAAGCTGGCACTTACCATCAGTGTTTCACCTACATG\t44\t0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "it.reset_iterator()\n",
    "one_iteration = next(it)\n",
    "print(one_iteration)\n",
    "another_iteration = next(it)\n",
    "print(another_iteration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/vtecftwy/metagentools/blob/main/metagentools/core.py#L269){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### TextFileBaseReader.print_first_chunks\n",
       "\n",
       ">      TextFileBaseReader.print_first_chunks (nchunks:int=3)\n",
       "\n",
       "Print the first `nchunk` chunks of text from the file.\n",
       "\n",
       "After printing, the iterator is reset again to its first line.\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| nchunks | int | 3 | number of chunks to print |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/vtecftwy/metagentools/blob/main/metagentools/core.py#L269){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### TextFileBaseReader.print_first_chunks\n",
       "\n",
       ">      TextFileBaseReader.print_first_chunks (nchunks:int=3)\n",
       "\n",
       "Print the first `nchunk` chunks of text from the file.\n",
       "\n",
       "After printing, the iterator is reset again to its first line.\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| nchunks | int | 3 | number of chunks to print |"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(TextFileBaseReader.print_first_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3-line chunk 1\n",
      "TCAAAATAATCAGAAATGTTGAACCTAGGGTTGGACACATAATGACCAGC\t76\t0\n",
      "ATTGTTTAACAATTTGTGCTCGTCCCGGTCACCCGCATCCAATCTTGATG\t4\t9\n",
      "AATCTTGTCCTATCCTACCCGCAGGGGAATTGATGATAGANGTGCTTTTA\t181\t0\n",
      "\n",
      "3-line chunk 2\n",
      "GGAGCGGAGCCAACCCCTATGCTCACTTGCAACCCAAGGGGCGTTCCAGT\t74\t3\n",
      "TGGATCCTGCGCGGGACGTCCTTTGTCTACGTCCCGTCGGCGCATCCCGC\t60\t3\n",
      "GAGAGACTTACTAAAAAGCTGGCACTTACCATCAGTGTTTCACCTACATG\t44\t0\n",
      "\n",
      "3-line chunk 3\n",
      "ACACACGACACTAGAGATAATGTGTCAGTGGATTATAAACAAACCAAGTT\t43\t7\n",
      "TTGTAGCATAAGAACTGGTCTTCGCTGAAATTCTTGTCTTGATCTCATCT\t35\t2\n",
      "TGGCCCTGCGGTCTGGGGCCCAGAAGCATATGTCAAGTCCTTTGAGAAGT\t73\t4\n",
      "\n"
     ]
    }
   ],
   "source": [
    "it = TextFileBaseReader(path=p2textfile, nlines=3)\n",
    "\n",
    "it.print_first_chunks(nchunks=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/vtecftwy/metagentools/blob/main/metagentools/core.py#L308){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### TextFileBaseReader.parse_text\n",
       "\n",
       ">      TextFileBaseReader.parse_text (txt:str, pattern:str=None,\n",
       ">                                     keys:list[str]=None)\n",
       "\n",
       "Parse text using regex pattern and keys. Return a metadata dictionary.\n",
       "\n",
       "The passed text is parsed using the regex pattern. The method return a dictionary in the format:\n",
       "\n",
       "    {\n",
       "        'key_1': 'metadata 1',\n",
       "        'key_2': 'metadata 2',\n",
       "        ...\n",
       "    }\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| txt | str |  | text to parse |\n",
       "| pattern | str | None | If None, uses standard regex pattern to extract metadata, otherwise, uses passed regex |\n",
       "| keys | list | None | If None, uses standard regex list of keys, otherwise, uses passed list of keys (str) |\n",
       "| **Returns** | **dict** |  | **parsed metadata in key/value format** |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/vtecftwy/metagentools/blob/main/metagentools/core.py#L308){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### TextFileBaseReader.parse_text\n",
       "\n",
       ">      TextFileBaseReader.parse_text (txt:str, pattern:str=None,\n",
       ">                                     keys:list[str]=None)\n",
       "\n",
       "Parse text using regex pattern and keys. Return a metadata dictionary.\n",
       "\n",
       "The passed text is parsed using the regex pattern. The method return a dictionary in the format:\n",
       "\n",
       "    {\n",
       "        'key_1': 'metadata 1',\n",
       "        'key_2': 'metadata 2',\n",
       "        ...\n",
       "    }\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| txt | str |  | text to parse |\n",
       "| pattern | str | None | If None, uses standard regex pattern to extract metadata, otherwise, uses passed regex |\n",
       "| keys | list | None | If None, uses standard regex list of keys, otherwise, uses passed list of keys (str) |\n",
       "| **Returns** | **dict** |  | **parsed metadata in key/value format** |"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(TextFileBaseReader.parse_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '2591237', 'nb': '1', 'source': 'ncbi'}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = '>2591237:ncbi:1'\n",
    "pattern = r\"^>(?P<id>\\d+):(?P<source>ncbi):(?P<nb>\\d*)\"\n",
    "keys = \"id source nb\".split(' ')\n",
    "\n",
    "it.parse_text(text, pattern, keys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extending the base class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`TextFileBaseReader` is a base class, intended to be extended into specific file format readers.\n",
    "\n",
    "The following methods will typically be extended to match data file and other structured text files formats:\n",
    "\n",
    "- `__next__` method in order to customize how the iterator parses files into \"elements\". For instance, in a FASTA file, one element consists of two lines: a *\"definition line\"* and the *sequence* itself. Extending `TextFileBaseReader` allows to read pairs of lines sequentially and return an element as a dictionary. For instance, `FastaFileReader` iterates over each pairs of lines in a Fasta file and return each pair as a dictionary as follows:\n",
    "\n",
    "```text\n",
    "    {\n",
    "    'definition line': '>2591237:ncbi:1 [MK211378]\\t2591237\\tncbi\\t1 [MK211378] '\n",
    "                       '2591237\\tCoronavirus BtRs-BetaCoV/YN2018D\\t\\tscientific '\n",
    "                       'name\\n',\n",
    "    'sequence':        'TATTAGGTTTTCTACCTACCCAGGA'\n",
    "    }\n",
    "```\n",
    "- Methods for parsing metadata from the file. For instance, `parse_file` method will handle how the reader will iterate over the full file and return a dictionary for the entire file. \n",
    "- Extended classes will also define a specific attributes (`text_to_parse_key`, `re_pattern`, `re_keys`, ...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/vtecftwy/metagentools/blob/main/metagentools/core.py#L336){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### TextFileBaseReader.set_parsing_rules\n",
       "\n",
       ">      TextFileBaseReader.set_parsing_rules (pattern:str|bool=None,\n",
       ">                                            keys:list[str]=None,\n",
       ">                                            verbose:bool=False)\n",
       "\n",
       "Set the standard regex parsing rule for the file.\n",
       "\n",
       "Rules can be set:\n",
       "\n",
       "1. manually by passing specific custom values for `pattern` and `keys`\n",
       "2. automatically, by testing all parsing rules saved in `parsing_rule.json` \n",
       "\n",
       "Automatic selection of parsing rules works by testing each rule saved in `parsing_rule.json` on the first \n",
       "definition line of the fasta file, and selecting the one rule that generates the most metadata matches.\n",
       "\n",
       "Rules consists of two parameters:\n",
       "\n",
       "- The regex pattern including one `group` for each metadata item, e.g `(?P<group_name>regex_code)`\n",
       "- The list of keys, i.e. the list with the name of each regex groups, used as key in the metadata dictionary\n",
       "\n",
       "This method updates the three following class attributes: `re_rule_name`, `re_pattern`, `re_keys`\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| pattern | str \\| bool | None | regex pattern to apply to parse the text, search in parsing rules json if None |\n",
       "| keys | list | None | list of keys/group for regex, search in parsing rules json if None |\n",
       "| verbose | bool | False | when True, provides information on each rule |\n",
       "| **Returns** | **None** |  |  |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/vtecftwy/metagentools/blob/main/metagentools/core.py#L336){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### TextFileBaseReader.set_parsing_rules\n",
       "\n",
       ">      TextFileBaseReader.set_parsing_rules (pattern:str|bool=None,\n",
       ">                                            keys:list[str]=None,\n",
       ">                                            verbose:bool=False)\n",
       "\n",
       "Set the standard regex parsing rule for the file.\n",
       "\n",
       "Rules can be set:\n",
       "\n",
       "1. manually by passing specific custom values for `pattern` and `keys`\n",
       "2. automatically, by testing all parsing rules saved in `parsing_rule.json` \n",
       "\n",
       "Automatic selection of parsing rules works by testing each rule saved in `parsing_rule.json` on the first \n",
       "definition line of the fasta file, and selecting the one rule that generates the most metadata matches.\n",
       "\n",
       "Rules consists of two parameters:\n",
       "\n",
       "- The regex pattern including one `group` for each metadata item, e.g `(?P<group_name>regex_code)`\n",
       "- The list of keys, i.e. the list with the name of each regex groups, used as key in the metadata dictionary\n",
       "\n",
       "This method updates the three following class attributes: `re_rule_name`, `re_pattern`, `re_keys`\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| pattern | str \\| bool | None | regex pattern to apply to parse the text, search in parsing rules json if None |\n",
       "| keys | list | None | list of keys/group for regex, search in parsing rules json if None |\n",
       "| verbose | bool | False | when True, provides information on each rule |\n",
       "| **Returns** | **None** |  |  |"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(TextFileBaseReader.set_parsing_rules)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Important Note to Developpers**\n",
    ">\n",
    "> Method `set_parsing_rules` is there to allow `TextFileBaseReader`'s descendant classes to automatically select parsing rule by applying rules saved in a json file to a string extracted from the first element in the file.\n",
    ">\n",
    "> It assumes that the iterator returns its elements as dictionaries `{section_name:section, ...}` and not as a pure string. The key `self.text_to_parse_key` will then be used to extract the text to parse for testing the rules.\n",
    "> The base class iterator returns a simple string and `self.text_to_parse_key` is set to `None`.\n",
    ">\n",
    "> To make setting up a default parsing rule for the reader instance, the iterator must return a dictionary and `self.text_to_parse_key` must be set to the key in the dictionary corresponding the the text to parse. \n",
    ">\n",
    "> See implementation in `FastaFileReader`.\n",
    ">\n",
    "> Calling `set_parsing_rules` on a class that does not satisfy with these characteristics will do nothing and return a warning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_25059/3717154356.py:152: UserWarning: \n",
      "            `text_to_parse_key` is not defined in this class. \n",
      "            It is not possible to set a parsing rule.\n",
      "            \n",
      "  warnings.warn(msg, category=UserWarning)\n"
     ]
    }
   ],
   "source": [
    "it.set_parsing_rules()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deprecated Items"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When any of the following classes and functions is called, it will raise an exception with an error message indicating how to handle the required code refactoring.\n",
    "\n",
    "Example:\n",
    "\n",
    "```python\n",
    "DeprecationWarning                        Traceback (most recent call last)\n",
    "Input In [140], in <cell line: 1>()\n",
    "----> 1 TextFileBaseIterator(p2textfile)\n",
    "\n",
    "Input In [139], in TextFileBaseIterator.__init__(self, *args, **kwargs)\n",
    "      4 def __init__(self, *args, **kwargs):\n",
    "      5     msg = \\\"\\\"\\\"\n",
    "      6     `TextFileBaseIterator` is deprecated. \n",
    "      7     Use `TextFileBaseReader` instead, with same capabilities and more.\\\"\\\"\\\"\n",
    "----> 8     raise DeprecationWarning(msg)\n",
    "\n",
    "DeprecationWarning: \n",
    "        `TextFileBaseIterator` is deprecated. \n",
    "        Use `TextFileBaseReader` instead, with same capabilities and more.\" \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class TextFileBaseIterator:\n",
    "    \"\"\"`TextFileBaseIterator` is a deprecated class, to be replaced by `TextFileBaseReader`\"\"\"\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        msg = \"\"\"\n",
    "        `TextFileBaseIterator` is deprecated. \n",
    "        Use `TextFileBaseReader` instead, with same capabilities and more.\"\"\"\n",
    "        raise DeprecationWarning(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "p2textfile = Path('data_dev/train_short')\n",
    "test_fail(TextFileBaseIterator, msg=\"Should generate DeprecationWarning\", contains=\"`TextFileBaseIterator` is deprecated.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
