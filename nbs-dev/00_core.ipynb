{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# core\n",
    "\n",
    "> This module includes base classes and items usable across the package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from ecutilities.ipython import nb_setup\n",
    "from ecutilities.core import path_to_parent_dir\n",
    "from fastcore.test import test_fail\n",
    "from nbdev import nbdev_export, show_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "Set autoreload mode\n"
     ]
    }
   ],
   "source": [
    "#| hide\n",
    "nb_setup()\n",
    "NBS_ROOT = path_to_parent_dir('nbs')\n",
    "assert NBS_ROOT.is_dir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import configparser\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "import warnings\n",
    "from ecutilities.core import is_type, safe_path\n",
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "from typing import Any, Optional\n",
    "\n",
    "try: from google.colab import drive\n",
    "except: pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "# Retrieve the package root\n",
    "from metagentools import __file__\n",
    "CODE_ROOT = Path(__file__).parents[0]\n",
    "PACKAGE_ROOT = Path(__file__).parents[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text file iterators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class TextFileBaseReader:\n",
    "    \"\"\"Iterator going through a text file by chunks of `nlines` lines. Iterator can be reset to file start.\n",
    "    \n",
    "    The class is mainly intented to be extended, in particular for handling sequence files or other type of data files\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        path: str|Path,  # path to the file\n",
    "        nlines: int=3,   # number of lines on one chunk\n",
    "    ):\n",
    "        self.path = safe_path(path)\n",
    "        self.nlines = nlines\n",
    "        self.fp = None\n",
    "        self.reset_iterator()\n",
    "        \n",
    "        # Attributes related to metadata parsing\n",
    "        # Currently assumes the iterator generates a dictionary with key/values\n",
    "        # TODO: extend to iterator output as simple string.\n",
    "        self.text_to_parse_key = None\n",
    "        self.parsing_rules_json = Path(f\"{PACKAGE_ROOT}/default_parsing_rules.json\")\n",
    "        self.re_rule_name = None\n",
    "        self.re_pattern = None       # regex pattern to use to parse text\n",
    "        self.re_keys = None          # keys (re group names) to parse text\n",
    "\n",
    "    def reset_iterator(self):\n",
    "        \"\"\"Reset the iterator to point to the first line in the file, by recreating a new file handle.\"\"\"\n",
    "        if self.fp is not None:\n",
    "            self.fp.close()\n",
    "        self.fp = open(self.path, 'r')\n",
    "        \n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "    def _safe_readline(self):\n",
    "        \"\"\"Read a new line and handle end of file tasks.\"\"\"\n",
    "        line = self.fp.readline()\n",
    "        if line == '':\n",
    "            self.fp.close()\n",
    "            raise StopIteration()\n",
    "        return line\n",
    "\n",
    "    def __next__(self):\n",
    "        \"\"\"Return one chunk at the time\"\"\"\n",
    "        lines = []\n",
    "        for i in range(self.nlines):\n",
    "            lines.append(self._safe_readline())\n",
    "        return ''.join(lines)\n",
    "    \n",
    "    def print_first_chunks(\n",
    "        self, \n",
    "        nchunks:int=3,  # number of chunks to print\n",
    "    ):\n",
    "        \"\"\"Print the first `nchunk` chunks of text from the file\"\"\"\n",
    "        self.reset_iterator()\n",
    "        for i, chunk in enumerate(self.__iter__()):\n",
    "            if i > nchunks-1: break\n",
    "            print(f\"{self.nlines}-line chunk {i+1}\")\n",
    "            print(chunk)\n",
    "        self.reset_iterator()\n",
    "            \n",
    "    def _parse_text_fn(\n",
    "        self,\n",
    "        txt:str,         # text to parse\n",
    "        pattern:str,     # regex pattern to apply to parse the text\n",
    "        keys:list[str],  # list of keys: keys are both the regex match group names and the corresponding output dict keys\n",
    "    )-> dict:        # parsed metadata in key/value format\n",
    "        \"\"\"Basic parser function parsing metadata from string, using regex pattern. Return a metadata dictionary\"\"\"\n",
    "        \n",
    "        matches = re.match(pattern, txt)\n",
    "        metadata = {}\n",
    "        if matches is not None:\n",
    "            for g in sorted(keys):\n",
    "                m = matches.group(g)\n",
    "                metadata[g] = m.replace('\\t', ' ').strip() if m is not None else None\n",
    "            return metadata\n",
    "        else:\n",
    "            raise ValueError(f\"No match on this line\")\n",
    "\n",
    "    def parse_text(\n",
    "        self,\n",
    "        txt:str,              # text to parse\n",
    "        pattern:str=None,     # If None, uses standard regex pattern to extract metadata, otherwise, uses passed regex\n",
    "        keys:list[str]=None,  # If None, uses standard regex list of keys, otherwise, uses passed list of keys (str)\n",
    "    )-> dict:                 # parsed metadata in key/value format\n",
    "        \"\"\"Parse text using regex pattern and key. Return a metadata dictionary\n",
    "        \n",
    "        The passed text is parsed using the regex pattern. The method return a dictionary in the format:\n",
    "            {\n",
    "                'key_1': 'metadata 1',\n",
    "                'key_2': 'metadata 2',\n",
    "                ...\n",
    "            }\n",
    "        \n",
    "        \"\"\"\n",
    "        if pattern is None and keys is None:\n",
    "            if self.re_pattern is not None and self.re_keys is not None:\n",
    "                return self._parse_text_fn(txt, self.re_pattern, self.re_keys)\n",
    "            else:\n",
    "                raise ValueError('attribute re_pattern and re_keys are still None')\n",
    "        elif pattern is None or keys is None:\n",
    "            raise ValueError('pattern and keys must be either both None or both have a value')\n",
    "        else:\n",
    "            return self._parse_text_fn(txt, pattern, keys)\n",
    "        \n",
    "        \n",
    "    def set_parsing_rules(\n",
    "        self,\n",
    "        pattern: str|bool=None,   # regex pattern to apply to parse the text, search in parsing rules json if None\n",
    "        keys: list[str]=None,     # list of keys/group for regex, search in parsing rules json if None\n",
    "        verbose: bool=False       # when True, provides information on each rule\n",
    "    )-> None:\n",
    "        \"\"\"Set the standard regex parsing rule for the file.\n",
    "        \n",
    "        Rules can be set:\n",
    "        \n",
    "        1. manually by passing specific custom values for `pattern` and `keys`\n",
    "        2. automatically, by testing all parsing rules saved in `parsing_rule.json` \n",
    "        \n",
    "        Automatic selection of parsing rules works by testing each rule saved in `parsing_rule.json` on the first \n",
    "        definition line of the fasta file, and selecting the one rule that generates the most metadata matches.\n",
    "        \n",
    "        Rules consists of two parameters:\n",
    "        \n",
    "        - The regex pattern including one `group` for each metadata item, e.g `(?P<group_name>regex_code)`\n",
    "        - The list of keys, i.e. the list with the name of each regex groups, used as key in the metadata dictionary\n",
    "        \n",
    "        This method updates the three following class attributes: `re_rule_name`, `re_pattern`, `re_keys`\n",
    "      \n",
    "        \"\"\"\n",
    "        # get the first definition line in the file to test the pattern\n",
    "        # in base class, text_to_parse_key is not defined and automatic rule selection cannot be used\n",
    "        # this must be handled in children classes\n",
    "        if self.text_to_parse_key is None:\n",
    "            msg = \"\"\"\n",
    "            `text_to_parse_key` is not defined in this class. \n",
    "            It is not possible to set a parsing rule.\n",
    "            \"\"\"\n",
    "            warnings.warn(msg, category=UserWarning)\n",
    "            return\n",
    "\n",
    "        self.reset_iterator()\n",
    "        first_output = next(self)\n",
    "        text_to_parse = first_output[self.text_to_parse_key]\n",
    "        divider_line = f\"{'-'*80}\"\n",
    "\n",
    "        if pattern is not None and keys is not None:\n",
    "            try:\n",
    "                metadata_dict = self.parse_text(text_to_parse, pattern, keys)\n",
    "                self.re_rule_name = 'Custom Rule'\n",
    "                self.re_pattern = pattern\n",
    "                self.re_keys = keys\n",
    "                if verbose:\n",
    "                    print(divider_line)\n",
    "                    print(f\"Custom rule was set for this instance.\")\n",
    "            except Exception as err: \n",
    "                raise ValueError(f\"The pattern generates the following error:\\n{err}\")\n",
    "                \n",
    "        else:\n",
    "            # Load all existing rules from json file\n",
    "            with open(self.parsing_rules_json, 'r') as fp:\n",
    "                parsing_rules = json.load(fp)\n",
    "                \n",
    "            # test all existing rules and keep the one with highest number of matches\n",
    "            max_nbr_matches = 0\n",
    "            for k, v in parsing_rules.items():\n",
    "                re_pattern = v['pattern']\n",
    "                re_keys = v['keys'].split(' ')\n",
    "                try:\n",
    "                    metadata_dict = self.parse_text(text_to_parse, re_pattern, re_keys)\n",
    "                    nbr_matches = len(metadata_dict)\n",
    "                    if verbose:\n",
    "                        print(divider_line)\n",
    "                        print(f\"Rule <{k}> generated {nbr_matches:,d} matches\")\n",
    "                        print(divider_line)\n",
    "                        print(re_pattern)\n",
    "                        print(re_keys)\n",
    "\n",
    "                    if len(metadata_dict) > max_nbr_matches:\n",
    "                        self.re_pattern = re_pattern\n",
    "                        self.re_keys = re_keys\n",
    "                        self.re_rule_name = k    \n",
    "                except Exception as err:\n",
    "                    if verbose:\n",
    "                        print(divider_line)\n",
    "                        print(f\"Rule <{k}> generated an error\")\n",
    "                        print(err)\n",
    "                    else:\n",
    "                        pass\n",
    "            if self.re_rule_name is None:\n",
    "                msg = \"\"\"\n",
    "        None of the saved parsing rules were able to extract metadata from the first line in this file.\n",
    "        You must set a custom rule (pattern + keys) before parsing text, by using:\n",
    "            `self.set_parsing_rules(custom_pattern, custom_list_of_keys)`\n",
    "                \"\"\"\n",
    "                warnings.warn(msg, category=UserWarning)\n",
    "            \n",
    "            if verbose:\n",
    "                print(divider_line)\n",
    "                print(f\"Selected rule with most matches: {self.re_rule_name}\")\n",
    "\n",
    "            # We used the iterator, now we need to reset it to make all lines available\n",
    "            self.reset_iterator()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show_doc(TextFileBaseReader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once initialized, the iterator runs over each chunk of line(s) in the text file, sequentially."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TCAAAATAATCAGAAATGTTGAACCTAGGGTTGGACACATAATGACCAGC\t76\t0\n",
      "ATTGTTTAACAATTTGTGCTCGTCCCGGTCACCCGCATCCAATCTTGATG\t4\t9\n",
      "AATCTTGTCCTATCCTACCCGCAGGGGAATTGATGATAGANGTGCTTTTA\t181\t0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "p2textfile = Path('data_dev/train_short')\n",
    "it = TextFileBaseReader(path=p2textfile, nlines=3)\n",
    "\n",
    "one_iteration = next(it)\n",
    "\n",
    "print(one_iteration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TCAAAATAATCAGAAATGTTGAACCTAGGGTTGGACACATAATGACCAGC\t76\t0\n",
      "ATTGTTTAACAATTTGTGCTCGTCCCGGTCACCCGCATCCAATCTTGATG\t4\t9\n",
      "AATCTTGTCCTATCCTACCCGCAGGGGAATTGATGATAGANGTGCTTTTA\t181\t0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "it = TextFileBaseReader(path=p2textfile, nlines=3)\n",
    "\n",
    "one_iteration = next(it)\n",
    "print(one_iteration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GGAGCGGAGCCAACCCCTATGCTCACTTGCAACCCAAGGGGCGTTCCAGT\t74\t3\n",
      "TGGATCCTGCGCGGGACGTCCTTTGTCTACGTCCCGTCGGCGCATCCCGC\t60\t3\n",
      "GAGAGACTTACTAAAAAGCTGGCACTTACCATCAGTGTTTCACCTACATG\t44\t0\n",
      "\n",
      "ACACACGACACTAGAGATAATGTGTCAGTGGATTATAAACAAACCAAGTT\t43\t7\n",
      "TTGTAGCATAAGAACTGGTCTTCGCTGAAATTCTTGTCTTGATCTCATCT\t35\t2\n",
      "TGGCCCTGCGGTCTGGGGCCCAGAAGCATATGTCAAGTCCTTTGAGAAGT\t73\t4\n",
      "\n"
     ]
    }
   ],
   "source": [
    "another_iteration = next(it)\n",
    "print(another_iteration)\n",
    "one_more_iteration = next(it)\n",
    "print(one_more_iteration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to access the start of the file, we need to re-initialize the file handle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/vtecftwy/metagentools/blob/main/metagentools/core.py#L54){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### TextFileBaseReader.reset_iterator\n",
       "\n",
       ">      TextFileBaseReader.reset_iterator ()\n",
       "\n",
       "Reset the iterator to point to the first line in the file, by recreating a new file handle."
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/vtecftwy/metagentools/blob/main/metagentools/core.py#L54){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### TextFileBaseReader.reset_iterator\n",
       "\n",
       ">      TextFileBaseReader.reset_iterator ()\n",
       "\n",
       "Reset the iterator to point to the first line in the file, by recreating a new file handle."
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(TextFileBaseReader.reset_iterator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`it.reset_iterator()` creates a new file handle to point to the first line of the file, again, but does not require to create a new instance of the reader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TCAAAATAATCAGAAATGTTGAACCTAGGGTTGGACACATAATGACCAGC\t76\t0\n",
      "ATTGTTTAACAATTTGTGCTCGTCCCGGTCACCCGCATCCAATCTTGATG\t4\t9\n",
      "AATCTTGTCCTATCCTACCCGCAGGGGAATTGATGATAGANGTGCTTTTA\t181\t0\n",
      "\n",
      "GGAGCGGAGCCAACCCCTATGCTCACTTGCAACCCAAGGGGCGTTCCAGT\t74\t3\n",
      "TGGATCCTGCGCGGGACGTCCTTTGTCTACGTCCCGTCGGCGCATCCCGC\t60\t3\n",
      "GAGAGACTTACTAAAAAGCTGGCACTTACCATCAGTGTTTCACCTACATG\t44\t0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "it.reset_iterator()\n",
    "one_iteration = next(it)\n",
    "print(one_iteration)\n",
    "another_iteration = next(it)\n",
    "print(another_iteration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/vtecftwy/metagentools/blob/main/metagentools/core.py#L78){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### TextFileBaseReader.print_first_chunks\n",
       "\n",
       ">      TextFileBaseReader.print_first_chunks (nchunks:int=3)\n",
       "\n",
       "Print the first `nchunk` chunks of text from the file\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| nchunks | int | 3 | number of chunks to print |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/vtecftwy/metagentools/blob/main/metagentools/core.py#L78){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### TextFileBaseReader.print_first_chunks\n",
       "\n",
       ">      TextFileBaseReader.print_first_chunks (nchunks:int=3)\n",
       "\n",
       "Print the first `nchunk` chunks of text from the file\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| nchunks | int | 3 | number of chunks to print |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(TextFileBaseReader.print_first_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3-line chunk 1\n",
      "TCAAAATAATCAGAAATGTTGAACCTAGGGTTGGACACATAATGACCAGC\t76\t0\n",
      "ATTGTTTAACAATTTGTGCTCGTCCCGGTCACCCGCATCCAATCTTGATG\t4\t9\n",
      "AATCTTGTCCTATCCTACCCGCAGGGGAATTGATGATAGANGTGCTTTTA\t181\t0\n",
      "\n",
      "3-line chunk 2\n",
      "GGAGCGGAGCCAACCCCTATGCTCACTTGCAACCCAAGGGGCGTTCCAGT\t74\t3\n",
      "TGGATCCTGCGCGGGACGTCCTTTGTCTACGTCCCGTCGGCGCATCCCGC\t60\t3\n",
      "GAGAGACTTACTAAAAAGCTGGCACTTACCATCAGTGTTTCACCTACATG\t44\t0\n",
      "\n",
      "3-line chunk 3\n",
      "ACACACGACACTAGAGATAATGTGTCAGTGGATTATAAACAAACCAAGTT\t43\t7\n",
      "TTGTAGCATAAGAACTGGTCTTCGCTGAAATTCTTGTCTTGATCTCATCT\t35\t2\n",
      "TGGCCCTGCGGTCTGGGGCCCAGAAGCATATGTCAAGTCCTTTGAGAAGT\t73\t4\n",
      "\n"
     ]
    }
   ],
   "source": [
    "it = TextFileBaseReader(path=p2textfile, nlines=3)\n",
    "\n",
    "it.print_first_chunks(nchunks=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/vtecftwy/metagentools/blob/main/metagentools/core.py#L108){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### TextFileBaseReader.parse_text\n",
       "\n",
       ">      TextFileBaseReader.parse_text (txt:str, pattern:str=None,\n",
       ">                                     keys:list[str]=None)\n",
       "\n",
       "Parse text using regex pattern and key. Return a metadata dictionary\n",
       "\n",
       "The passed text is parsed using the regex pattern. The method return a dictionary in the format:\n",
       "    {\n",
       "        'key_1': 'metadata 1',\n",
       "        'key_2': 'metadata 2',\n",
       "        ...\n",
       "    }\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| txt | str |  | text to parse |\n",
       "| pattern | str | None | If None, uses standard regex pattern to extract metadata, otherwise, uses passed regex |\n",
       "| keys | list | None | If None, uses standard regex list of keys, otherwise, uses passed list of keys (str) |\n",
       "| **Returns** | **dict** |  | **parsed metadata in key/value format** |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/vtecftwy/metagentools/blob/main/metagentools/core.py#L108){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### TextFileBaseReader.parse_text\n",
       "\n",
       ">      TextFileBaseReader.parse_text (txt:str, pattern:str=None,\n",
       ">                                     keys:list[str]=None)\n",
       "\n",
       "Parse text using regex pattern and key. Return a metadata dictionary\n",
       "\n",
       "The passed text is parsed using the regex pattern. The method return a dictionary in the format:\n",
       "    {\n",
       "        'key_1': 'metadata 1',\n",
       "        'key_2': 'metadata 2',\n",
       "        ...\n",
       "    }\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| txt | str |  | text to parse |\n",
       "| pattern | str | None | If None, uses standard regex pattern to extract metadata, otherwise, uses passed regex |\n",
       "| keys | list | None | If None, uses standard regex list of keys, otherwise, uses passed list of keys (str) |\n",
       "| **Returns** | **dict** |  | **parsed metadata in key/value format** |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(TextFileBaseReader.parse_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '2591237', 'nb': '1', 'source': 'ncbi'}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = '>2591237:ncbi:1'\n",
    "pattern = r\"^>(?P<id>\\d+):(?P<source>ncbi):(?P<nb>\\d*)\"\n",
    "keys = \"id source nb\".split(' ')\n",
    "\n",
    "it.parse_text(text, pattern, keys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extending the base class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`TextFileBaseReader` is a base class.\n",
    "\n",
    "The following methods will typically be extended to match data file and other structured text files formats:\n",
    "\n",
    "- `__next__` method in order to customize how the iterator parses files into \"elements\". For instance, in a FASTA file, one element consists of two lines: a \"definition line\" and the sequence itself. Extending `TextFileBaseReader` allows to read pairs of lines sequentially and return an element as a dictionary. For instance, `FastaFileReader` iterate over the pairs of lines in a Fasta file and return each pairs as the following dictionary:\n",
    "\n",
    "```\n",
    "    {\n",
    "    'definition line': '>2591237:ncbi:1 [MK211378]\\t2591237\\tncbi\\t1 [MK211378] '\n",
    "                    '2591237\\tCoronavirus BtRs-BetaCoV/YN2018D\\t\\tscientific '\n",
    "                    'name\\n',\n",
    "    'sequence': 'TATTAGGTTTTCTACCTACCCAGGA'\n",
    "    }\n",
    "```\n",
    "- Methods for parsing metadata from the file. For instance, `parse_file` method will handle how the reader will iterate over the full file and return a dictionary for the entire file. \n",
    "- Extended classes will also define a few attributes (`text_to_parse_key`, `re_pattern`, `re_keys`, ...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/vtecftwy/metagentools/blob/main/metagentools/core.py#L135){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### TextFileBaseReader.set_parsing_rules\n",
       "\n",
       ">      TextFileBaseReader.set_parsing_rules (pattern:str|bool=None,\n",
       ">                                            keys:list[str]=None,\n",
       ">                                            verbose:bool=False)\n",
       "\n",
       "Set the standard regex parsing rule for the file.\n",
       "\n",
       "Rules can be set:\n",
       "\n",
       "1. manually by passing specific custom values for `pattern` and `keys`\n",
       "2. automatically, by testing all parsing rules saved in `parsing_rule.json` \n",
       "\n",
       "Automatic selection of parsing rules works by testing each rule saved in `parsing_rule.json` on the first \n",
       "definition line of the fasta file, and selecting the one rule that generates the most metadata matches.\n",
       "\n",
       "Rules consists of two parameters:\n",
       "\n",
       "- The regex pattern including one `group` for each metadata item, e.g `(?P<group_name>regex_code)`\n",
       "- The list of keys, i.e. the list with the name of each regex groups, used as key in the metadata dictionary\n",
       "\n",
       "This method updates the three following class attributes: `re_rule_name`, `re_pattern`, `re_keys`\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| pattern | str \\| bool | None | regex pattern to apply to parse the text, search in parsing rules json if None |\n",
       "| keys | list | None | list of keys/group for regex, search in parsing rules json if None |\n",
       "| verbose | bool | False | when True, provides information on each rule |\n",
       "| **Returns** | **None** |  |  |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/vtecftwy/metagentools/blob/main/metagentools/core.py#L135){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### TextFileBaseReader.set_parsing_rules\n",
       "\n",
       ">      TextFileBaseReader.set_parsing_rules (pattern:str|bool=None,\n",
       ">                                            keys:list[str]=None,\n",
       ">                                            verbose:bool=False)\n",
       "\n",
       "Set the standard regex parsing rule for the file.\n",
       "\n",
       "Rules can be set:\n",
       "\n",
       "1. manually by passing specific custom values for `pattern` and `keys`\n",
       "2. automatically, by testing all parsing rules saved in `parsing_rule.json` \n",
       "\n",
       "Automatic selection of parsing rules works by testing each rule saved in `parsing_rule.json` on the first \n",
       "definition line of the fasta file, and selecting the one rule that generates the most metadata matches.\n",
       "\n",
       "Rules consists of two parameters:\n",
       "\n",
       "- The regex pattern including one `group` for each metadata item, e.g `(?P<group_name>regex_code)`\n",
       "- The list of keys, i.e. the list with the name of each regex groups, used as key in the metadata dictionary\n",
       "\n",
       "This method updates the three following class attributes: `re_rule_name`, `re_pattern`, `re_keys`\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| pattern | str \\| bool | None | regex pattern to apply to parse the text, search in parsing rules json if None |\n",
       "| keys | list | None | list of keys/group for regex, search in parsing rules json if None |\n",
       "| verbose | bool | False | when True, provides information on each rule |\n",
       "| **Returns** | **None** |  |  |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(TextFileBaseReader.set_parsing_rules)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Important Note**\n",
    ">\n",
    "> Method `set_parsing_rules` is there to allow `TextFileBaseReader`'s descendant classes to automatically select parsing rule by applying rules saved in a json file to a string extracted from the first element in the file.\n",
    ">\n",
    "> It assumes that the iterator returns its elements as dictionaries `{section_name:section, ...}` and not as a pure string. The key `self.text_to_parse_key` will then be used to extract the text to parse for testing the rules.\n",
    "> The base class iterator returns a simple string and `self.text_to_parse_key` is set to `None`.\n",
    ">\n",
    "> To make setting up a default parsing rule for the reader instance, the iterator must return a dictionary and `self.text_to_parse_key` must be set to the key in the dictionary corresponding the the text to parse. \n",
    ">\n",
    "> See implementation in `FastaFileReader`.\n",
    ">\n",
    "> Calling `set_parsing_rules` on a class that does not satisfy with these characteristics will do nothing and return a warning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2536/954411935.py:141: UserWarning: \n",
      "            `text_to_parse_key` is not defined in this class. \n",
      "            It is not possible to set a parsing rule.\n",
      "            \n",
      "  warnings.warn(msg, category=UserWarning)\n"
     ]
    }
   ],
   "source": [
    "it.set_parsing_rules()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utility Classes and Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling files and file structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class JsonFileReader:\n",
    "    def __init__(self, path:str|Path # path to the json file\n",
    "                ):\n",
    "        self.path = safe_path(path)\n",
    "        with open(path, 'r') as fp:\n",
    "            self.d = json.load(fp)\n",
    "    \n",
    "    def add_item(self, \n",
    "                 key:str,  # key for the new item\n",
    "                 item:dict # new item to add to the json as a dict\n",
    "                ):\n",
    "        self.d[key] = item\n",
    "        return self.d\n",
    "\n",
    "    def save_to_file(self, path=None):\n",
    "        if path is None: \n",
    "            path = self.path\n",
    "        else:\n",
    "            path = safe_path(path)\n",
    "\n",
    "        with open(path, 'w') as fp:\n",
    "            json.dump(self.d, fp, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/vtecftwy/metagentools/blob/main/metagentools/core.py#L235){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### JsonFileReader\n",
       "\n",
       ">      JsonFileReader (path:str|pathlib.Path)\n",
       "\n",
       "Initialize self.  See help(type(self)) for accurate signature.\n",
       "\n",
       "|    | **Type** | **Details** |\n",
       "| -- | -------- | ----------- |\n",
       "| path | str \\| pathlib.Path | path to the json file |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/vtecftwy/metagentools/blob/main/metagentools/core.py#L235){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### JsonFileReader\n",
       "\n",
       ">      JsonFileReader (path:str|pathlib.Path)\n",
       "\n",
       "Initialize self.  See help(type(self)) for accurate signature.\n",
       "\n",
       "|    | **Type** | **Details** |\n",
       "| -- | -------- | ----------- |\n",
       "| path | str \\| pathlib.Path | path to the json file |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(JsonFileReader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'another item': {'keys': 'key key key key', 'pattern': 'another pattern'},\n",
      " 'item 1': {'keys': 'key key key key', 'pattern': 'pattern 1'},\n",
      " 'item 2': {'keys': 'key key key key', 'pattern': 'pattern 2'},\n",
      " 'item 3': {'keys': 'key key key key', 'pattern': 'pattern 3'}}\n"
     ]
    }
   ],
   "source": [
    "jd = JsonFileReader('data_dev/test.json')\n",
    "pprint(jd.d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'item 1': {'pattern': 'pattern 1', 'keys': 'key key key key'},\n",
       " 'item 2': {'pattern': 'pattern 2', 'keys': 'key key key key'},\n",
       " 'item 3': {'pattern': 'pattern 3', 'keys': 'key key key key'},\n",
       " 'another item': {'keys': 'key key key key', 'pattern': 'another pattern'}}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_item = {'keys': 'key key key key', 'pattern': 'another pattern'}\n",
    "jd.add_item(key='another item', item=new_item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jd.save_to_file()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'another item': {'keys': 'key key key key', 'pattern': 'another pattern'},\n",
      " 'item 1': {'keys': 'key key key key', 'pattern': 'pattern 1'},\n",
      " 'item 2': {'keys': 'key key key key', 'pattern': 'pattern 2'},\n",
      " 'item 3': {'keys': 'key key key key', 'pattern': 'pattern 3'}}\n"
     ]
    }
   ],
   "source": [
    "jd = JsonFileReader('data_dev/test.json')\n",
    "pprint(jd.d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class ProjectFileSystem:\n",
    "    \"\"\"Class to set paths to key directories, according to whether the code is running locally or in the cloud.\"\"\"\n",
    "\n",
    "    _instance = None\n",
    "    _config_dir = '.metagentools'\n",
    "    _config_fname = 'metagentools.cfg'\n",
    "    _shared_project_dir = 'Metagenomics'\n",
    "    \n",
    "    def __new__(cls, *args, **kwargs):\n",
    "        # Create instance if it does not exist yet\n",
    "        if cls._instance is None:\n",
    "            cls.home = Path.home().resolve()\n",
    "            cls.p2config = cls.home / cls._config_dir / cls._config_fname\n",
    "            cls._instance = super().__new__(cls)\n",
    "        return cls._instance\n",
    "    \n",
    "    def __init__(\n",
    "        self, \n",
    "        mount_gdrive:bool=True  # True to mount Google Drive if running on Colab\n",
    "        ):\n",
    "            self.is_colab = 'google.colab' in sys.modules       \n",
    "            if self.is_colab and mount_gdrive:\n",
    "                drive.mount('/content/gdrive')\n",
    "                self.gdrive = Path('/content/gdrive/MyDrive')\n",
    "\n",
    "            self.is_kaggle = 'kaggle_web_client' in sys.modules\n",
    "            if self.is_kaggle:\n",
    "                raise NotImplemented(f\"ProjectFileSystem is not implemented for Kaggle yet\")\n",
    "\n",
    "            if not self.is_colab and not self.is_kaggle and not self.is_local:\n",
    "                msg = \"\"\"\n",
    "                      Code does not seem to run on the cloud but computer is not registered as local\n",
    "                      If you are running on a local computer, you must register it as local by running\n",
    "                        `ProjectFileSystem().register_as_local()`\n",
    "                      before you can use the ProjectFileSystem class.\n",
    "                      \"\"\"\n",
    "                warnings.warn(msg, UserWarning)\n",
    "\n",
    "    def __call__(self): return self.is_local\n",
    "\n",
    "    def info(self):\n",
    "        print(f\"Running {self.os} on {self.running_on}\")\n",
    "        print(f\"Device's home directory: {self.home}\")\n",
    "        print(f\"Project file structure:\")\n",
    "        print(f\" - Root ........ {self.project_root} \\n - Data Dir .... {self.data} \\n - Notebooks ... {self.nbs}\")\n",
    "\n",
    "    \n",
    "    def read_config(self):\n",
    "        \"\"\"Read config from the configuration file if it exists and return an empty config if does not\"\"\"\n",
    "        cfg = configparser.ConfigParser()\n",
    "        if self.p2config.is_file(): \n",
    "            cfg.read(self.p2config)\n",
    "        else:\n",
    "            cfg.add_section('Infra')\n",
    "        return cfg\n",
    "\n",
    "    def register_as_local(self):\n",
    "        \"\"\"Update the configuration file to register the machine as local machine\"\"\"\n",
    "        cfg = self.read_config()\n",
    "        os.makedirs(self.home/self._config_dir, exist_ok=True)\n",
    "        cfg['Infra']['registered_as_local'] = 'True'\n",
    "        with open(self.p2config, 'w') as fp:\n",
    "            cfg.write(fp)\n",
    "        return cfg\n",
    "\n",
    "    @property\n",
    "    def os(self): return sys.platform\n",
    "\n",
    "    @property\n",
    "    def project_root(self):\n",
    "        if self.is_local:\n",
    "            return PACKAGE_ROOT\n",
    "        elif self.is_colab:\n",
    "            return self.gdrive / self._shared_project_dir\n",
    "        elif self.is_kaggle:\n",
    "            raise NotImplemented(f\"ProjectFileSystem is not implemented for Kaggle yet\")\n",
    "        else:\n",
    "            raise ValueError('Not running locally, on Colab or on Kaggle')\n",
    "\n",
    "\n",
    "    @property\n",
    "    def data(self): return self.project_root / 'data'\n",
    "\n",
    "    @property\n",
    "    def nbs(self): return self.project_root / 'nbs'        \n",
    "\n",
    "    @property\n",
    "    def p2config(self): return self.home / self._config_dir / self._config_fname\n",
    "        \n",
    "    @property\n",
    "    def is_local(self):\n",
    "        \"\"\"Return `True` if the current machine was registered as a local machine\"\"\"\n",
    "        cfg = self.read_config()\n",
    "        return cfg['Infra'].getboolean('registered_as_local', False)\n",
    "\n",
    "    @property\n",
    "    def running_on(self):\n",
    "        \"\"\"Return the device on which this is run: local, colab, kaggle, ...\"\"\"\n",
    "        if self.is_local: device = 'local computer'\n",
    "        elif self.is_colab: device = 'colab'\n",
    "        elif self.is_kaggle: device = 'kaggle'\n",
    "        else: device = 'unknown cloud server'\n",
    "        return device\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/vtecftwy/metagentools/blob/main/metagentools/core.py#L259){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### ProjectFileSystem\n",
       "\n",
       ">      ProjectFileSystem (*args, **kwargs)\n",
       "\n",
       "Class to set paths to key directories, according to whether the code is running locally or in the cloud."
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/vtecftwy/metagentools/blob/main/metagentools/core.py#L259){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### ProjectFileSystem\n",
       "\n",
       ">      ProjectFileSystem (*args, **kwargs)\n",
       "\n",
       "Class to set paths to key directories, according to whether the code is running locally or in the cloud."
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(ProjectFileSystem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The class assumes that the project is organized in line with the following file system:\n",
    "\n",
    "- if running **localy**:\n",
    "\n",
    "            project-root   \n",
    "                |--- data\n",
    "                |--- nbs\n",
    "                \n",
    "- if running on **google colab**: gdrive root includes a shortcut named **`Metagenomics`** and pointing to the following project shared directory: `/https://drive.google.com/drive/folders/134uei5fmt08TpzhmjG4sW0FQ06kn2ZfZ`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once created, the instance of `ProjectFileSystem` gives access to key directories' paths:\n",
    "\n",
    "- `project root`: `Path` to the project root directory\n",
    "- `data`: `Path` to the data directory\n",
    "- `nbs`: `Path` to the notebooks directory\n",
    "\n",
    "It also provides additional information regarding the computer on which the code is running:\n",
    "\n",
    "- `os`: a string providing the name of the operating system the code is running on\n",
    "- `is_colab`: True if the code is running on google colab\n",
    "- `is_kaggle`: True if the code is running on kaggle server (NOT IMPLEMENTED YET)\n",
    "- `is_local`: True if the code is running on a computer registered as local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pfs = ProjectFileSystem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "if not pfs.is_local and not pfs.is_kaggle: \n",
    "    pfs.register_as_local()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/vtec/projects/bio/metagentools\n",
      "/home/vtec/projects/bio/metagentools/data\n",
      "/home/vtec/projects/bio/metagentools/nbs\n"
     ]
    }
   ],
   "source": [
    "for p in [pfs.project_root, pfs.data, pfs.nbs]:\n",
    "    print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('linux', True, False, False)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pfs.os, pfs.is_local, pfs.is_colab, pfs.is_kaggle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When using the package on a local computer for the first time, it is required to register the computer as a local computer. Once registered, it is possible to read the configuration file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/vtecftwy/metagentools/blob/main/metagentools/core.py#L308){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### ProjectFileSystem.register_as_local\n",
       "\n",
       ">      ProjectFileSystem.register_as_local ()\n",
       "\n",
       "Update the configuration file to register the machine as local machine"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/vtecftwy/metagentools/blob/main/metagentools/core.py#L308){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### ProjectFileSystem.register_as_local\n",
       "\n",
       ">      ProjectFileSystem.register_as_local ()\n",
       "\n",
       "Update the configuration file to register the machine as local machine"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(ProjectFileSystem.register_as_local)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = pfs.register_as_local()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/vtecftwy/metagentools/blob/main/metagentools/core.py#L299){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### ProjectFileSystem.read_config\n",
       "\n",
       ">      ProjectFileSystem.read_config ()\n",
       "\n",
       "Read config from the configuration file if it exists and return an empty config if does not"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/vtecftwy/metagentools/blob/main/metagentools/core.py#L299){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### ProjectFileSystem.read_config\n",
       "\n",
       ">      ProjectFileSystem.read_config ()\n",
       "\n",
       "Read config from the configuration file if it exists and return an empty config if does not"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(ProjectFileSystem.read_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'True'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg = pfs.read_config()\n",
    "cfg['Infra']['registered_as_local']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deprecated Items"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When any of the following classes and functions os called, it will raise an exception with an error message indicating how to handle the required code refactoring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class TextFileBaseIterator:\n",
    "    \"\"\"`TextFileBaseIterator` is a deprecated class, to be replaced by `TextFileBaseReader`\"\"\"\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        msg = \"\"\"`TextFileBaseIterator` is deprecated. Use `TextFileBaseReader` instead, with same capabilities and more.\"\"\"\n",
    "        raise DeprecationWarning(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "p2textfile = Path('data_dev/train_short')\n",
    "test_fail(TextFileBaseIterator, msg=\"Should generate DeprecationWarning\", contains=\"`TextFileBaseIterator` is deprecated.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
