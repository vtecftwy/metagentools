{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|default_exp cnn_virus.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from __future__ import annotations\n",
    "from ecutilities.ipython import nb_setup\n",
    "from ecutilities.core import files_in_tree\n",
    "from fastcore.test import test_fail\n",
    "from nbdev import show_doc, nbdev_export\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set autoreload mode\n"
     ]
    }
   ],
   "source": [
    "#| hide\n",
    "nb_setup()\n",
    "\n",
    "# ON_COLAB, p2dataroot, p2data = setup_nb(_dev=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import tensorflow as tf\n",
    "import warnings\n",
    "\n",
    "from ecutilities.core import validate_path\n",
    "from functools import partial, partialmethod\n",
    "from metagentools.bio import q_score2prob_error\n",
    "from metagentools.core import TextFileBaseReader, ProjectFileSystem\n",
    "from pathlib import Path\n",
    "from tqdm.notebook import tqdm, trange\n",
    "from typing import Any, Optional\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' # or any {'0', '1', '2'}\n",
    "import tensorflow as tf\n",
    "from tensorflow.io import serialize_tensor, FixedLenFeature\n",
    "from tensorflow.keras.utils import Sequence, to_categorical\n",
    "from tensorflow.data import TextLineDataset, TFRecordDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "# Retrieve the package root\n",
    "from metagentools import __file__\n",
    "CODE_ROOT = Path(__file__).parents[0]\n",
    "PACKAGE_ROOT = Path(__file__).parents[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow version: 2.8.2 - Expected 2.8.2\n",
      "metagentools package location: /home/vtec/projects/bio/metagentools/metagentools/__init__.py\n"
     ]
    }
   ],
   "source": [
    "#|hide\n",
    "print(f\"Tensorflow version: {tf.__version__} - Expected 2.8.2\")\n",
    "print(f\"metagentools package location: {__file__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data\n",
    "\n",
    "> Data structure, data preprocessing and transform functions, data reader classes, datasets for CNN Virus data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data structure for CNN Virus project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many different types of files and datasets for this project. All data are located in directory `data`, under the project root. The following is an overview of the main types of data and in which directory they sit in the tree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A description of the content of each directory is recorded in `readme.md` or another `*.md` file. \n",
    "\n",
    "These `readme.md` files can be conveniently accessed using the `.readme(path)` method on `ProjectFileSystem`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<hr>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "ReadMe file for directory `data`:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<hr>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### Data structure for `metagentools`\n",
       "This directory includes all the data required for the project `metagentools`.\n",
       "\n",
       "```text\n",
       "data\n",
       " |--- CNN_Virus_data \n",
       " |--- ncbi           \n",
       " |--- ncov_data      \n",
       " |--- saved         \n",
       " |--- ....           \n",
       "     \n",
       "```\n",
       "#### Sub-directories\n",
       "- `CNN_Virus_data`: includes all the data related to the original CNN Virus paper, i.e. training data and validation data in a format that can be used by the CNN Virus code.\n",
       "- `ncbi`: includes data related to the use of CoV sequences from NCBI: reference sequences, simulated reads, inference datasets, inference results.\n",
       "- `ncov_data`: includes data related to the use of non Cov sequences from various sources: reference sequences, simulated reads, inference datasets, inference results.\n",
       "- `saved`: includes model saved parameters and preprocessing datasets.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<hr>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ProjectFileSystem().readme()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Original datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<hr>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "ReadMe file for directory `data/CNN_Virus_data`:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<hr>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### CNN Virus data\n",
       "\n",
       "This directory includes data used to train and validate the initial CNN Virus model, as well as a few smaller datasets for experimenting. \n",
       "\n",
       "\n",
       "#### File list and description:\n",
       "##### 50-mer \n",
       "50-mer reads and their labels, in *text format* with one line per sample. Each line consists of three components, separated by tabs: the 50-mer read or sequence, the virus species label and the position label:\n",
       "```text\n",
       "'TTACNAGCTCCAGTCTAAGATTGTAACTGGCCTTTTTAAAGATTGCTCTA    94    5\\n'\n",
       "``` \n",
       "Files:\n",
       "- `50mer_training`: dataset with 50,903,296 reads for training\n",
       "- `50mer_validating`: dataset with 1,000,000 reads for validation\n",
       "- `50mer_ds_100_reads`: small subset of 100 reads from the validating dataset for experiments\n",
       "\n",
       "##### 150-mer\n",
       "150-mer reads and their labels in *text format* in a similar format as above:\n",
       "```text\n",
       "'TTCTTTCACCACCACAACCAGTCGGCCGTGGAGAGGCGTCGCCGCGTCTCGTTCGTCGAGGCCGATCGACTGCCGCATGAGAGCGGGTGGTATTCTTCCGAAGACGACGGAGACCGGGACGGTGATGAGGAAACTGGAGAGAGCCACAAC    6    0\\n'\n",
       "```\n",
       "Files:\n",
       "- `ICTV_150mer_benchmarking`: dataset with 10,0000 read\n",
       "- `150mer_ds_100_reads`: small subset of 100 reads from `ICTV_150mer_benchmarking`\n",
       "\n",
       "##### Longer reads\n",
       "Reads of various length with no labels, in simple *fasta format*. Each read sequence is preceded by a definition line: `> Sequence n`, where `n` is the sequence number.\n",
       "\n",
       "Files:\n",
       "- `training_sequences_300bp.fasta`: dataset with 9,000 300-mer reads\n",
       "- `training_sequences_500bp.fasta`: dataset with 9,000 500-mer reads\n",
       "- `validation_sequences.fasta`: dataset with 564 reads of mixed lengths ranging from 163-mer to 497-mer\n",
       "\n",
       "##### Other files:\n",
       "- `virus_name_mapping`: mapping between virus species and their numerical label\n",
       "- `weight_of_classes`:  weights for each virus species class in the training dataset\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<hr>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pfs = ProjectFileSystem()\n",
    "pfs.readme(pfs.data/'CNN_Virus_data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data for simulated reads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<hr>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "ReadMe file for directory `data/ncbi`:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<hr>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### NCBI Data\n",
       "\n",
       "This directory includes all data related to the work done with CoV sequences from NCBI. The data is organized in the following subfolders:\n",
       "\n",
       "- `refsequences`: reference CoV sequences downloaded from NCBI, and related metadata\n",
       "- `simreads`: all data from simulated reads, using ART Illumina simulator and the reference sequences\n",
       "- `infer_results`: results from the inference using models with the simulated reads\n",
       "- `ds`: datasets in proper format for training or inference/prediction using the CNN Virus model\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<hr>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# | echo: false\n",
    "pfs.readme(pfs.data / 'ncbi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<hr>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "ReadMe file for directory `data/ncbi/refsequences`:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<hr>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### NCBI reference CoV sequences\n",
       "\n",
       "This directory includes several CoV sequences in fasta files, retrieved from the NCBI database:\n",
       "- the main file including all reference sequences: `cov_virus_sequences.fa`\n",
       "- smaller files with a reduced number of sequences for testing code: `cov_virus_sequences_*-seqs.fa`\n",
       "\n",
       "#### `cov_virus_sequences.fa`\n",
       "- includes 3,318 sequences of corona virus with different types of hosts.\n",
       "- the names of the virus species is listed in the file `cov_virus_sequences.txt`, in the same directory\n",
       "- the length of each sequence varies between 751 and 33,576 bases\n",
       "\n",
       "\n",
       "#### `cov_virus_sequences_*-seqs.fa`\n",
       "Smaller files are also available. Each of which includes a limited number of sequences to test the code on smaller datasets. The files are named `cov_virus_sequences_*-seqs.fa` , where `*` is the number of sequences in the file:\n",
       "- `cov_virus_sequences_001-seq1.fa` includes 1 sequence\n",
       "- `cov_virus_sequences_001-seq2.fa` includes 1 other sequence\n",
       "- `cov_virus_sequences_002-seqs.fa` includes 2 sequences\n",
       "- `cov_virus_sequences_010-seqs.fa` includes 10 sequences\n",
       "- `cov_virus_sequences_025-seqs.fa` includes 25 sequences\n",
       "- `cov_virus_sequences_100-seqs.fa` includes 100 sequences\n",
       "\n",
       "\n",
       "#### File Format:\n",
       "Like all fasta files, each sequence is preceded by a *Definition Line* starting with the character `>`. In the case of our NCBI downloaded sequences:\n",
       "\n",
       "```ascii\n",
       ">2591237:ncbi:1    [MK211378]    2591237    ncbi    1    [MK211378]    2591237    Coronavirus BtRs-BetaCoV/YN2018D    scient\n",
       "TATTAGGTTTTCTACCTACCCAGGAAAAGCCAACCAACCTCGATCTCTTGTAGATCTGTTCTCTAAACGAACTTTAAAATCTGTGTAGCTGTCGCTCGGCTGCATGCCTA ...\n",
       ">11128:ncbi:2    [LC494191]\n",
       "CATCCCGCTTCACTGATCTCTTGTTAGATCTTTTCATAATCTAAACTTTATAAAAACATCCACTCCCTGTAGTCTATGCCTATGGGCGTAGATTTTTCATAGTGGTGTCT ...\n",
       ">31631:ncbi:3 [KY967361]    31631    ncbi    3 [KY967361] 31631    Human coronavirus OC43    scientific name\n",
       "ATCTCTTGTTAGATCTTTTTGTAATCTAAACTTTATAAAAACATCCACTCCCTGTAATCTATGCTTGTGGGCGTAGATTTTTCATAGTGGTGTTTATATTCATTTCTGCT ...\n",
       ">277944:ncbi:4 [LC654455]    277944    ncbi    4 [LC654455] 277944    Human coronavirus NL63    scientific name\n",
       "ATTTTCTTATTTAGACTTTGTGTCTACTCTTCTCAACTAAACGAAATTTTTCTAGTGCTGTCATTTGTTATGGCAGTCCTAGTGTAATTGAAATTTCGTCAAGTTTGTAA ...\n",
       ">11120:ncbi:5 [MN987231]    11120    ncbi    5 [MN987231] 11120    Infectious bronchitis virus    scientific name\n",
       "TCCTAAGTGTGATATAAATATATATCATACACACTAGCCTTGCGCTAGATTTCTAACTTAACAAAACGGACTTAAATACCTACAGCTGGTCCCTATAGGTGTTCCATTGC ...\n",
       "\n",
       "....\n",
       "\n",
       ">2697049:ncbi:3318 [OM062573]    2697049    ncbi    3318 [OM062573] 2697049    Severe acute respiratory syndrome coronavirus 2        scientific name\n",
       "```\n",
       "\n",
       "**Metadata** can be parsed from the definition line for further use.\n",
       "\n",
       "|item |name| example |\n",
       "|:---:|:--:|:-------|\n",
       "|sequence id | seqid | 2697049:ncbi:3318|\n",
       "|accession |accession | OM062573|\n",
       "|taxonomy id |taxonomyid | 2697049|\n",
       "|source |source| ncbi| \n",
       "|sequence nbr |seqnb | 3318|\n",
       "|specie name |species| Severe acute respiratory syndrome coronavirus 2|\n",
       "\n",
       "Example 1:\n",
       "- Definition Line:\n",
       "```ascii\n",
       "    >2591237:ncbi:1 [MK211378]    2591237    ncbi    1 [MK211378] 2591237    Coronavirus BtRs-BetaCoV/YN2018D        scientific name\n",
       "```\n",
       "- Parsed metadata:\n",
       "    - `seqid` = `2591237:ncbi:1`\n",
       "    - `taxonomyid` = `2591237`\n",
       "    - `source` = `ncbi`\n",
       "    - `seqnb` = `1`\n",
       "    - `accession` = `MK211378`\n",
       "    - `species` = `Coronavirus BtRs-BetaCoV/YN2018D`\n",
       "\n",
       "Example 2:\n",
       "- Definition Line\n",
       "```ascii\n",
       "    >11128:ncbi:2 [LC494191]\n",
       "```\n",
       "- Parsed metadata:\n",
       "    - `seqid` = `11128:ncbi:2`\n",
       "    - `taxonomyid` = `11128`\n",
       "    - `source` = `ncbi`\n",
       "    - `seqnb` = `2`\n",
       "    - `accession` = `LC494191`\n",
       "    - `species` = `''`\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<hr>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# | echo: false\n",
    "pfs.readme(pfs.data / 'ncbi/refsequences')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<hr>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "ReadMe file for directory `data/ncbi/simreads`:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<hr>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### CoV simulated reads\n",
       "This directory includes a set of simulated read sequence files generated from NCBI CoV sequences using  ARC Illumina. \n",
       "\n",
       "```ascii\n",
       "this-directory\n",
       "    |\n",
       "    |--single_10seq_50bp\n",
       "    |    |--single_10seq_50bp.fq\n",
       "    |    |--single_10seq_50bp.alnEnd\n",
       "    |--single_100seq_50bp\n",
       "    |    |--single_100seq_50bp.fq\n",
       "    |    |--single_100seq_50bp.aln\n",
       "    |--single_100seq_150bp\n",
       "    |    |--single_100seq_150bp.fq\n",
       "    |    |--single_100seq_150bp.aln\n",
       "    |--paired_100seq_50bp\n",
       "    |    |--paired_100seq_50bp2.aln\n",
       "    |    |--paired_100seq_50bp1.aln\n",
       "    |    |--paired_100seq_50bp2.fq\n",
       "    |    |--paired_100seq_50bp1.fq\n",
       "    |-- ...\n",
       "```\n",
       "\n",
       "Each simread sub-directory is named as `<method>_<nb-seq>_<nb-bp>` where\"\n",
       "- `<method>` is either `single` or `paired` depending on the simulation method\n",
       "- `<nb-seq>` is the number of reference sequences used for simulation, and refers to the `fa` file used\n",
       "- `<nb-bp>` is the number of base pairs used to simulate reads\n",
       "\n",
       "\n",
       "Each sub-directory includes simreads files made using a simulation method and a specific number of reference sequences.\n",
       "- `xxx.fq` and `xxx.aln` files when method is `single`\n",
       "- `xxx1.fq`, `xxx2.fq`, `xxx1.aln` and `xxx2.aln` files when method is `paired`.\n",
       "\n",
       "Example:\n",
       "- `paired_10seq_50bp` means that the simreads were generated by using the `paired` method to simulate 50-bp reads, and using the `fa` file `cov_virus_sequences_010-seqs.fa`.\n",
       "- `single_100seq_50bp` means that the simreads were generated by using the `single` method to simulate 50-bp reads, and using the `fa` file `cov_virus_sequences_100-seqs.fa`. Note that this generated 20,660,104 reads !\n",
       "\n",
       "#### Simread file formats\n",
       "\n",
       "Simulated reads information is split between two files:\n",
       "- **FASTQ** (`.fq`) files providing the read sequences and their ASCII quality scores\n",
       "- **ALN** (`.aln`) files with alignment information\n",
       "\n",
       "##### FASTQ (`.fq`)\n",
       "FASTQ files generated by ART Illumina have the following structure (showing 5 reads), with 4 lines for each read:\n",
       "\n",
       "```ascii\n",
       "@2591237:ncbi:1-60400\n",
       "ACAACTCCTATTCGTAGTTGAAGTTGTTGACAAATACTTTGATTGTTACG\n",
       "+\n",
       "CCCBCGFGBGGGGGGGBGGGGGGGGG>GGG1G=/GGGGGGGGGGGGGGGG\n",
       "@2591237:ncbi:1-60399\n",
       "GATCAATGTGGCATCTACAATACAGACAGCATGAAGCACCACCAAAGGAC\n",
       "+\n",
       "BCBCCFGGGGGGGG1CGGGG<GGBGGGGGFGCGGGGGGDGGG/GG1GGGG\n",
       "@2591237:ncbi:1-60398\n",
       "ATCTACCAGTGGTAGATGGGTTCTTAATAATGAACATTATAGAGCTCTAC\n",
       "+\n",
       "CCCCCGGGEGG1GGF1G/GGEGGGGGGGGGGGGFFGGGGGGGGGGDGGDG\n",
       "@2591237:ncbi:1-60397\n",
       "CGTAAAGTAGAGGCTGTATGGTAGCTAGCACAAATGCCAGCACCAATAGG\n",
       "+\n",
       "BCCCCGGGFGGGGGGFGGGGFGG1GGGGGGG>GG1GGGGGGGGGGE<GGG\n",
       "@2591237:ncbi:1-60396\n",
       "GGTATCGGGTATCTCCTGCATCAATGCAAGGTCTTACAAAGATAAATACT\n",
       "+\n",
       "CBCCCGGG@CGGGGGGGGGGGG=GFGGGGDGGGFG1GGGGGGGG@GGGGG\n",
       "```\n",
       "The following information can be parsed from the each read sequence in the FASTQ file:\n",
       "\n",
       "- Line 1: `readid`, a unique ID for the read, under for format `@readid` \n",
       "- Line 2: `readseq`, the sequence of the read\n",
       "- Line 3: a separator `+`\n",
       "- Line 4: `read_qscores`, the base quality scores encoded in ASCII \n",
       "\n",
       "Example:\n",
       "```\n",
       "@2591237:ncbi:1-60400\n",
       "ACAACTCCTATTCGTAGTTGAAGTTGTTGACAAATACTTTGATTGTTACG\n",
       "+\n",
       "CCCBCGFGBGGGGGGGBGGGGGGGGG>GGG1G=/GGGGGGGGGGGGGGGG\n",
       "```\n",
       "- `readid` = `2591237:ncbi:1-60400`\n",
       "- `readseq` = `ACAACTCCTATTCGTAGTTGAAGTTGTTGACAAATACTTTGATTGTTACG`, a 50 bp read\n",
       "- `read_qscores` = `CCCBCGFGBGGGGGGGBGGGGGGGGG>GGG1G=/GGGGGGGGGGGGGGGG`\n",
       "\n",
       "\n",
       "#### ALN (`.aln`) \n",
       "ALN files generated by ART Illumina consist of :\n",
       "- a header with the ART-Ilumina command used for the simulation (`@CM`) and info on each of the reference sequences used for the simulations (`@SQ`). Header always starts with `##ART_Illumina` and ends with `##Header End` :\n",
       "- the body with 3 lines for each read:\n",
       "    1. definition line with `readid`, \n",
       "        - reference sequence identification number `refseqid`, \n",
       "        - the position in the read in the reference sequence `aln_start_pos` \n",
       "        - the strand the read was taken from `ref_seq_strand`. `+` for coding strand and `-` for template strand\n",
       "    2. aligned reference sequence, that is the sequence segment in the original reference corresponding to the read\n",
       "    3. aligned read sequence, that is the simmulated read sequence, where each bp corresponds to the reference sequence bp in the same position.\n",
       "\n",
       "Example of a ALN file generated by ART Illumina (showing 5 reads):\n",
       "\n",
       "```ascii\n",
       "##ART_Illumina    read_length    50\n",
       "@CM    /bin/art_illumina -i /home/vtec/projects/bio/metagentools/data/cov_data/cov_virus_sequences_ten.fa -ss HS25 -l 50 -f 100 -o /home/vtec/projects/bio/metagentools/data/cov_simreads/single_10seq_50bp/single_10seq_50bp -rs 1674660835\n",
       "@SQ    2591237:ncbi:1 [MK211378]    2591237    ncbi    1 [MK211378] 2591237    Coronavirus BtRs-BetaCoV/YN2018D    30213\n",
       "@SQ    11128:ncbi:2 [LC494191]    11128    ncbi    2 [LC494191] 11128    Bovine coronavirus    30942\n",
       "@SQ    31631:ncbi:3 [KY967361]    31631    ncbi    3 [KY967361] 31631    Human coronavirus OC43        30661\n",
       "@SQ    277944:ncbi:4 [LC654455]    277944    ncbi    4 [LC654455] 277944    Human coronavirus NL63    27516\n",
       "@SQ    11120:ncbi:5 [MN987231]    11120    ncbi    5 [MN987231] 11120    Infectious bronchitis virus    27617\n",
       "@SQ    28295:ncbi:6 [KU893866]    28295    ncbi    6 [KU893866] 28295    Porcine epidemic diarrhea virus    28043\n",
       "@SQ    28295:ncbi:7 [KJ645638]    28295    ncbi    7 [KJ645638] 28295    Porcine epidemic diarrhea virus    27998\n",
       "@SQ    28295:ncbi:8 [KJ645678]    28295    ncbi    8 [KJ645678] 28295    Porcine epidemic diarrhea virus    27998\n",
       "@SQ    28295:ncbi:9 [KR873434]    28295    ncbi    9 [KR873434] 28295    Porcine epidemic diarrhea virus    28038\n",
       "@SQ    1699095:ncbi:10 [KT368904]    1699095    ncbi    10 [KT368904] 1699095    Camel alphacoronavirus    27395\n",
       "##Header End\n",
       ">2591237:ncbi:1    2591237:ncbi:1-60400    14770    +\n",
       "ACAACTCCTATTCGTAGTTGAAGTTGTTGACAAATACTTTGATTGTTACG\n",
       "ACAACTCCTATTCGTAGTTGAAGTTGTTGACAAATACTTTGATTGTTACG\n",
       ">2591237:ncbi:1    2591237:ncbi:1-60399    17012    -\n",
       "GATCAATGTGGCATCTACAATACAGACAGCATGAAGCACCACCAAAGGAC\n",
       "GATCAATGTGGCATCTACAATACAGACAGCATGAAGCACCACCAAAGGAC\n",
       ">2591237:ncbi:1    2591237:ncbi:1-60398    9188    +\n",
       "ATCTACCAGTGGTAGATGGGTTCTTAATAATGAACATTATAGAGCTCTAC\n",
       "ATCTACCAGTGGTAGATGGGTTCTTAATAATGAACATTATAGAGCTCTAC\n",
       ".....\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<hr>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# | echo: false\n",
    "pfs.readme(pfs.data / 'ncbi/simreads')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<hr>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "ReadMe file for directory `data/ncbi/infer_results`:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<hr>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### CoV Virus Inference Results\n",
       "This folder includes results from inference using CoV simulated read sequences from `fq` and `aln` files in `cov_simreads`.\n",
       "\n",
       "#### `cnn_virus`\n",
       "\n",
       "The directory `cnn_virus` includes results from inference made with the original pretrained model. \n",
       "\n",
       "Results are saved into many individual `parquet` files during inference. Then they are merged into a single `parquet` file. \n",
       "\n",
       "Each inference experiment receives a unique 8-character unique ID (UID).\n",
       "\n",
       "Each inference experiment will therefore generate a set of files like follows, where `xxxxxxxx` is the experiment UID and `nnnn` is the index of a partial result file:\n",
       "\n",
       "- `results_nnnn_infer_xxxxxxxx.parquet`\n",
       "- `results_all_infer_xxxxxxxx.parquet`\n",
       "\n",
       "`results_all_infer_xxxxxxxx.parquet` is the file consolidating all results for one inference experiment into a single file"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<hr>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#| echo: false\n",
    "pfs.readme(pfs.data / 'ncbi/infer_results')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<hr>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "ReadMe file for directory `data/ncbi/ds`:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<hr>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### Inference and Training Datasets\n",
       "\n",
       "When using simread files (`fa` and `aln`) for inference, an inference dataset in a format required by the CNN Virus model must be build. In addition, metadata can be extracted to make it easier to analyse the result from different perspectives.\n",
       "\n",
       "This directory includes the generated inference datasets and metadata for each inference experiment.\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<hr>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#| echo: false\n",
    "pfs.readme(pfs.data / 'ncbi/ds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model related data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<hr>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "ReadMe file for directory `data/saved`:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<hr>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### Saved data related to models\n",
       "\n",
       "This directory includes all data related to models and saved:\n",
       "- saved model parameters\n",
       "- saved datasets\n",
       "\n",
       "For example:\n",
       "- `cnn_virus_original/pretrained_model.h5` is the saved model parameters for the CNN Virus model\n",
       "- `cnn_virus_datasets/*.tfrecords` are the preprocessed datasets used for inference or training, saved in TFRecord format for performance\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<hr>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#| echo: false\n",
    "pfs.readme(pfs.data / 'saved')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parsing sequence files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following classes make it easier to read and parse files of different formats into their underlying components to generated the training, validation, testing and inference datasets for the model.\n",
    "\n",
    "Each class inherits from `TextFileBaseReader` and adds:\n",
    "\n",
    "- One or several text parsing method(s) to parse metadata according to a specific format\n",
    "- A file parsing method to extract metadata from all elements in the file, returning it as a key:value dictionary and optionally save the metadata as a json file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FASTA file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extension of `TextFileBaseReader` class for fasta sequence files.\n",
    "\n",
    "Structure of a FASTA sequence file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">2591237:ncbi:1 [MK211378]\t2591237\tncbi\t1 [MK211378] 2591237\tCoronavirus BtRs-Be\n",
      "TATTAGGTTTTCTACCTACCCAGGAAAAGCCAACCAACCTCGATCTCTTGTAGATCTGTTCTCTAAACGAACTTTAAAAT\n",
      ">11128:ncbi:2 [LC494191]\n",
      "CATCCCGCTTCACTGATCTCTTGTTAGATCTTTTCATAATCTAAACTTTATAAAAACATCCACTCCCTGTAGTCTATGCC\n"
     ]
    }
   ],
   "source": [
    "#| echo: false\n",
    "p2fasta = Path('data_dev/cov_virus_sequences_two.fa').resolve()\n",
    "\n",
    "it = TextFileBaseReader(p2fasta, nlines=1)\n",
    "for i, t in enumerate(it):\n",
    "    txt = t.replace('\\n', '')[:80]\n",
    "    print(f\"{txt}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class FastaFileReader(TextFileBaseReader):\n",
    "    \"\"\"Wrap a FASTA file and retrieve its content in raw format and parsed format\"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        path: str|Path,  # path to the Fasta file\n",
    "    ):\n",
    "        super().__init__(path, nlines=1)\n",
    "        self.text_to_parse_key = 'definition line'\n",
    "        self.set_parsing_rules(verbose=False)\n",
    "        \n",
    "    def __next__(self)-> dict[str, str]:   # `{'definition line': text in dfn line, 'sequence': full sequence as str}` \n",
    "        \"\"\"Return one definition line and the corresponding sequence\"\"\"\n",
    "        lines = []\n",
    "        for i in range(2):\n",
    "            lines.append(self._safe_readline())\n",
    "        dfn_line = lines[0].replace('\\n', '')   #remove the next line symbol at the end of the line\n",
    "        sequence = lines[1].replace('\\n', '')   #remove the next line symbol at the end of the line\n",
    "        return {'definition line':dfn_line, 'sequence':f\"{sequence}\"}\n",
    "    \n",
    "    def print_first_chunks(\n",
    "        self, \n",
    "        nchunks:int=3,  # number of chunks to print out\n",
    "    ):\n",
    "        \"\"\"Print the first `nchunks` chunks of text from the file\"\"\"\n",
    "        self.reset_iterator()\n",
    "        for i, seq_dict in enumerate(self.__iter__()):\n",
    "            print(f\"\\nSequence {i+1}:\")\n",
    "            print(seq_dict['definition line'])\n",
    "            print(f\"{seq_dict['sequence'][:80]} ...\")\n",
    "            if i >= nchunks-1: break\n",
    "        self.reset_iterator()\n",
    "            \n",
    "    def parse_file(\n",
    "        self,\n",
    "        add_seq :bool=False,     # When True, add the full sequence to the parsed metadata dictionary\n",
    "        save_json: bool=False    # When True, save the file metadata as a json file of same stem name\n",
    "    )-> dict[str]:               # Metadata as Key/Values pairs\n",
    "        \"\"\"Read fasta file and return a dictionary with definition line metadata and optionally sequences\"\"\"\n",
    "    \n",
    "        self.reset_iterator()\n",
    "        parsed = {}\n",
    "        for d in self:\n",
    "            dfn_line = d['definition line']\n",
    "            seq = d['sequence']\n",
    "            metadata = self._parse_text_fn(dfn_line, self.re_pattern, self.re_keys)\n",
    "            if add_seq: metadata['sequence'] = seq         \n",
    "            parsed[metadata['seqid']] = metadata\n",
    "                        \n",
    "        if save_json:\n",
    "            p2json = self.path.parent / f\"{self.path.stem}_metadata.json\"\n",
    "            with open(p2json, 'w') as fp:\n",
    "                json.dump(parsed, fp, indent=4)\n",
    "                print(f\"Metadata for '{self.path.name}'> saved as <{p2json.name}> in  \\n{p2json.parent.absolute()}\\n\")\n",
    "\n",
    "        return parsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/vtecftwy/metagentools/blob/main/metagentools/cnn_virus/data.py#L39){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### FastaFileReader\n",
       "\n",
       ">      FastaFileReader (path:str|pathlib.Path)\n",
       "\n",
       "Wrap a FASTA file and retrieve its content in raw format and parsed format\n",
       "\n",
       "|    | **Type** | **Details** |\n",
       "| -- | -------- | ----------- |\n",
       "| path | str \\| Path | path to the Fasta file |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/vtecftwy/metagentools/blob/main/metagentools/cnn_virus/data.py#L39){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### FastaFileReader\n",
       "\n",
       ">      FastaFileReader (path:str|pathlib.Path)\n",
       "\n",
       "Wrap a FASTA file and retrieve its content in raw format and parsed format\n",
       "\n",
       "|    | **Type** | **Details** |\n",
       "| -- | -------- | ----------- |\n",
       "| path | str \\| Path | path to the Fasta file |"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(FastaFileReader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an iterator, `FastaFileReader` returns a `dict` at each step, as follows:\n",
    "```python\n",
    "{\n",
    "    'definition line': 'string in file as the definition line for the sequence',\n",
    "    'sequence': 'the full sequence'\n",
    "}\n",
    "```\n",
    "\n",
    "Illustration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">2591237:ncbi:1 [MK211378]\t2591237\tncbi\t1 [MK211378] 2591237\tCoronavirus BtRs-Be ...\n",
      "TATTAGGTTTTCTACCTACCCAGGAAAAGCCAACCAACCTCGATCTCTTGTAGATCTGTTCTCTAAACGAACTTTAAAAT ...\n"
     ]
    }
   ],
   "source": [
    "p2fasta = Path('data_dev/cov_virus_sequences_two.fa')\n",
    "it = FastaFileReader(p2fasta)\n",
    "iteration_output = next(it)\n",
    "\n",
    "print(iteration_output['definition line'][:80], '...')\n",
    "print(iteration_output['sequence'][:80], '...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output type :     <class 'dict'>\n",
      "keys :            dict_keys(['definition line', 'sequence'])\n",
      "definition line : >2591237:ncbi:1 [MK211378]\t2591237\tncbi\t1 [MK211378] 2591237\tCoronavirus BtRs-Be ...'\n",
      "sequence :       'TATTAGGTTTTCTACCTACCCAGGAAAAGCCAACCAACCTCGATCTCTTGTAGATCTGTTCTCTAAACGAACTTTAAAATCTGTGTAGCTGTCGCTCGGC ...'\n"
     ]
    }
   ],
   "source": [
    "print(f\"output type :     {type(iteration_output)}\")\n",
    "print(f\"keys :            {iteration_output.keys()}\")\n",
    "print(f\"definition line : {iteration_output['definition line'][:80]} ...'\")\n",
    "print(f\"sequence :       '{iteration_output['sequence'][:100]} ...'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `definition line` is a string, with tab separated values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'>2591237:ncbi:1 [MK211378]\\t2591237\\tncbi\\t1 [MK211378] 2591237\\tCoronavirus BtRs-BetaCoV/YN2018D\\t\\tscientific name'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(iteration_output['definition line'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/vtecftwy/metagentools/blob/main/metagentools/cnn_virus/data.py#L58){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### FastaFileReader.print_first_chunks\n",
       "\n",
       ">      FastaFileReader.print_first_chunks (nchunks:int=3)\n",
       "\n",
       "Print the first `nchunks` chunks of text from the file\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| nchunks | int | 3 | number of chunks to print out |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/vtecftwy/metagentools/blob/main/metagentools/cnn_virus/data.py#L58){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### FastaFileReader.print_first_chunks\n",
       "\n",
       ">      FastaFileReader.print_first_chunks (nchunks:int=3)\n",
       "\n",
       "Print the first `nchunks` chunks of text from the file\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| nchunks | int | 3 | number of chunks to print out |"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(FastaFileReader.print_first_chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is convenient to quickly discover and explore new fasta files in raw text format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sequence 1:\n",
      ">2591237:ncbi:1 [MK211378]\t2591237\tncbi\t1 [MK211378] 2591237\tCoronavirus BtRs-BetaCoV/YN2018D\t\tscientific name\n",
      "TATTAGGTTTTCTACCTACCCAGGAAAAGCCAACCAACCTCGATCTCTTGTAGATCTGTTCTCTAAACGAACTTTAAAAT ...\n",
      "\n",
      "Sequence 2:\n",
      ">11128:ncbi:2 [LC494191]\n",
      "CATCCCGCTTCACTGATCTCTTGTTAGATCTTTTCATAATCTAAACTTTATAAAAACATCCACTCCCTGTAGTCTATGCC ...\n"
     ]
    }
   ],
   "source": [
    "it = FastaFileReader(p2fasta)\n",
    "it.print_first_chunks(nchunks=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parsing metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The class also provides methods to parse metadata from the file content."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A regex pattern is used for parsing metadata fom the definition lines in the reference sequence fasta file (rule `fasta_cov_ncbi`):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sequence 1:\n",
    "\n",
    "- Definition Line:\n",
    "```ascii\n",
    ">2591237:ncbi:1 [MK211378]\t2591237\tncbi\t1 [MK211378] 2591237\tCoronavirus YN2018D\t\tscientific name\n",
    "```\n",
    "- Metadata:\n",
    "    - `seqid` = `2591237:ncbi:1`\n",
    "    - `taxonomyid` = `2591237`\n",
    "    - `source` = `ncbi`\n",
    "    - `seqnb` = `1`\n",
    "    - `accession` = `MK211378`\n",
    "    - `species` = `Coronavirus BtRs-BetaCoV/YN2018D`\n",
    "\n",
    "Sequence 2:\n",
    "\n",
    "- Definition Line\n",
    "```ascii\n",
    "    >11128:ncbi:2 [LC494191]\n",
    "```\n",
    "\n",
    "- Metadata:\n",
    "    - `seqid` = `11128:ncbi:2`\n",
    "    - `taxonomyid` = `11128`\n",
    "    - `source` = `ncbi`\n",
    "    - `seqnb` = `2`\n",
    "    - `accession` = `LC494191`\n",
    "    - `species` = `''`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`FastaFileReader` offers:\n",
    "- `parse_text` a method to parse the metadata\n",
    "- an option to set a default \"parsing rule\" for one instance with `set_parsing_rules`.\n",
    "- `parse_file` a method to parse the metadata from all sequences in the file and save it as a json file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/vtecftwy/metagentools/blob/main/metagentools/core.py#LNone){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### TextFileBaseReader.parse_text\n",
       "\n",
       ">      TextFileBaseReader.parse_text (txt:str, pattern:str=None,\n",
       ">                                     keys:list[str]=None)\n",
       "\n",
       "Parse text using regex pattern and key. Return a metadata dictionary\n",
       "\n",
       "The passed text is parsed using the regex pattern. The method return a dictionary in the format:\n",
       "    {\n",
       "        'key_1': 'metadata 1',\n",
       "        'key_2': 'metadata 2',\n",
       "        ...\n",
       "    }\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| txt | str |  | text to parse |\n",
       "| pattern | str | None | If None, uses standard regex pattern to extract metadata, otherwise, uses passed regex |\n",
       "| keys | list | None | If None, uses standard regex list of keys, otherwise, uses passed list of keys (str) |\n",
       "| **Returns** | **dict** |  | **parsed metadata in key/value format** |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/vtecftwy/metagentools/blob/main/metagentools/core.py#LNone){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### TextFileBaseReader.parse_text\n",
       "\n",
       ">      TextFileBaseReader.parse_text (txt:str, pattern:str=None,\n",
       ">                                     keys:list[str]=None)\n",
       "\n",
       "Parse text using regex pattern and key. Return a metadata dictionary\n",
       "\n",
       "The passed text is parsed using the regex pattern. The method return a dictionary in the format:\n",
       "    {\n",
       "        'key_1': 'metadata 1',\n",
       "        'key_2': 'metadata 2',\n",
       "        ...\n",
       "    }\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| txt | str |  | text to parse |\n",
       "| pattern | str | None | If None, uses standard regex pattern to extract metadata, otherwise, uses passed regex |\n",
       "| keys | list | None | If None, uses standard regex list of keys, otherwise, uses passed list of keys (str) |\n",
       "| **Returns** | **dict** |  | **parsed metadata in key/value format** |"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(FastaFileReader.parse_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the parser function with specifically defined `pattern` and `keys`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">2591237:ncbi:1 [MK211378]\t2591237\tncbi\t1 [MK211378] 2591237\tCoronavirus BtRs-BetaCoV/YN2018D\t\tscientific name\n"
     ]
    }
   ],
   "source": [
    "it = FastaFileReader(p2fasta)\n",
    "dfn_line, sequence = next(it).values()\n",
    "print(dfn_line.replace('\\n', ''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = r\"^>(?P<seqid>(?P<taxonomyid>\\d+):(?P<source>ncbi):(?P<seqnb>\\d*))[\\s\\t]*\\[(?P<accession>[\\w\\d]*)\\]([\\s\\t]*(?P=taxonomyid)[\\s\\t]*(?P=source)[\\s\\t]*(?P=seqnb)[\\s\\t]*\\[(?P=accession)\\][\\s\\t]*(?P=taxonomyid)[\\s\\t]*(?P<species>[\\w\\s\\-\\_\\/]*))?\"\n",
    "\n",
    "keys = 'seqid taxonomyid accession source seqnb species'.split(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accession': 'MK211378',\n",
       " 'seqid': '2591237:ncbi:1',\n",
       " 'seqnb': '1',\n",
       " 'source': 'ncbi',\n",
       " 'species': 'Coronavirus BtRs-BetaCoV/YN2018D  scientific name',\n",
       " 'taxonomyid': '2591237'}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "it.parse_text(dfn_line, pattern=pattern, keys=keys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When a `FastaFileReader` instance is created, all existing rules in the file `default_parsing_rules.json` are tested on the first definition line of the fasta file and the one rule that parses the most matches will be selected automatically and saved in instance attributes `re_rule_name`, `re_pattern` and `re_keys`. \n",
    "\n",
    "`parse_file` extract metadata from each definition line in the fasta file and return a dictionary with all metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fasta_cov_ncbi\n",
      "^>(?P<seqid>(?P<taxonomyid>\\d+):(?P<source>ncbi):(?P<seqnb>\\d*))[\\s\\t]*\\[(?P<accession>[\\w\\d]*)\\]([\\s\\t]*(?P=taxonomyid)[\\s\\t]*(?P=source)[\\s\\t]*(?P=seqnb)[\\s\\t]*\\[(?P=accession)\\][\\s\\t]*(?P=taxonomyid)[\\s\\t]*(?P<species>[\\w\\s\\-\\_\\/]*))?\n",
      "['seqid', 'taxonomyid', 'source', 'accession', 'seqnb', 'species']\n"
     ]
    }
   ],
   "source": [
    "print(it.re_rule_name)\n",
    "print(it.re_pattern)\n",
    "print(it.re_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accession': 'MK211378',\n",
       " 'seqid': '2591237:ncbi:1',\n",
       " 'seqnb': '1',\n",
       " 'source': 'ncbi',\n",
       " 'species': 'Coronavirus BtRs-BetaCoV/YN2018D  scientific name',\n",
       " 'taxonomyid': '2591237'}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "it.parse_text(dfn_line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When another fasta file, which has another definition line structure, is used, another parsing rule is selected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">1 dna_rm:primary_assembly primary_assembly:mRhiFer1_v1.p:1:1:124933378:1 REF\n"
     ]
    }
   ],
   "source": [
    "p2other = Path('data_dev/another_sequence.fa')\n",
    "assert p2other.is_file()\n",
    "\n",
    "it2 = FastaFileReader(path=p2other)\n",
    "\n",
    "dfn_line, sequence = next(it2).values()\n",
    "print(dfn_line.replace('\\n', ''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fasta_rhinolophus_ferrumequinum\n",
      "^>\\d[\\s\\t](?P<seq_type>dna_rm):(?P<id_type>[\\w\\_]*)[\\s\\w](?P=id_type):(?P<assy>[\\w\\d\\_]*)\\.(?P<seq_level>[\\w]*):\\d*:\\d*:(?P<taxonomy>\\d*):(?P<id>\\d*)[\\s\t]REF$\n",
      "['seq_type', 'id_type', 'assy', 'seq_level', 'taxonomy', 'id']\n"
     ]
    }
   ],
   "source": [
    "print(it2.re_rule_name)\n",
    "print(it2.re_pattern)\n",
    "print(it2.re_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'assy': 'mRhiFer1_v1',\n",
      " 'id': '1',\n",
      " 'id_type': 'primary_assembly',\n",
      " 'seq_level': 'p',\n",
      " 'seq_type': 'dna_rm',\n",
      " 'taxonomy': '124933378'}\n"
     ]
    }
   ],
   "source": [
    "pprint(it2.parse_text(dfn_line))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This rule selection is performed by the class method `set_parsing_rule`. The method can also be called with specific `pattern` and `keys` to force parsing rule not yet saved in the json file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/vtecftwy/metagentools/blob/main/metagentools/core.py#LNone){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### TextFileBaseReader.set_parsing_rules\n",
       "\n",
       ">      TextFileBaseReader.set_parsing_rules (pattern:str|bool=None,\n",
       ">                                            keys:list[str]=None,\n",
       ">                                            verbose:bool=False)\n",
       "\n",
       "Set the standard regex parsing rule for the file.\n",
       "\n",
       "Rules can be set:\n",
       "\n",
       "1. manually by passing specific custom values for `pattern` and `keys`\n",
       "2. automatically, by testing all parsing rules saved in `parsing_rule.json` \n",
       "\n",
       "Automatic selection of parsing rules works by testing each rule saved in `parsing_rule.json` on the first \n",
       "definition line of the fasta file, and selecting the one rule that generates the most metadata matches.\n",
       "\n",
       "Rules consists of two parameters:\n",
       "\n",
       "- The regex pattern including one `group` for each metadata item, e.g `(?P<group_name>regex_code)`\n",
       "- The list of keys, i.e. the list with the name of each regex groups, used as key in the metadata dictionary\n",
       "\n",
       "This method updates the three following class attributes: `re_rule_name`, `re_pattern`, `re_keys`\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| pattern | str \\| bool | None | regex pattern to apply to parse the text, search in parsing rules json if None |\n",
       "| keys | list | None | list of keys/group for regex, search in parsing rules json if None |\n",
       "| verbose | bool | False | when True, provides information on each rule |\n",
       "| **Returns** | **None** |  |  |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/vtecftwy/metagentools/blob/main/metagentools/core.py#LNone){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### TextFileBaseReader.set_parsing_rules\n",
       "\n",
       ">      TextFileBaseReader.set_parsing_rules (pattern:str|bool=None,\n",
       ">                                            keys:list[str]=None,\n",
       ">                                            verbose:bool=False)\n",
       "\n",
       "Set the standard regex parsing rule for the file.\n",
       "\n",
       "Rules can be set:\n",
       "\n",
       "1. manually by passing specific custom values for `pattern` and `keys`\n",
       "2. automatically, by testing all parsing rules saved in `parsing_rule.json` \n",
       "\n",
       "Automatic selection of parsing rules works by testing each rule saved in `parsing_rule.json` on the first \n",
       "definition line of the fasta file, and selecting the one rule that generates the most metadata matches.\n",
       "\n",
       "Rules consists of two parameters:\n",
       "\n",
       "- The regex pattern including one `group` for each metadata item, e.g `(?P<group_name>regex_code)`\n",
       "- The list of keys, i.e. the list with the name of each regex groups, used as key in the metadata dictionary\n",
       "\n",
       "This method updates the three following class attributes: `re_rule_name`, `re_pattern`, `re_keys`\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| pattern | str \\| bool | None | regex pattern to apply to parse the text, search in parsing rules json if None |\n",
       "| keys | list | None | list of keys/group for regex, search in parsing rules json if None |\n",
       "| verbose | bool | False | when True, provides information on each rule |\n",
       "| **Returns** | **None** |  |  |"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(FastaFileReader.set_parsing_rules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "definition line: '>2591237:ncbi:1 [MK211378]\t2591237\tncbi\t1 [MK211378] 2591237\tCoronavirus BtRs-BetaCoV/YN2018D\t\tscientific nam'\n"
     ]
    }
   ],
   "source": [
    "it = FastaFileReader(p2fasta)\n",
    "dfn_line, sequence = next(it).values()\n",
    "print(f\"definition line: '{dfn_line[:-1]}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Automatic parsing works by testing each saved rule for the value of `definition line` in the first sequence in the fasta file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "key for text to parse: definition line\n",
      "\n",
      "Text to parse for testing (extracted from first iteration):\n",
      ">2591237:ncbi:1 [MK211378]\t2591237\tncbi\t1 [MK211378] 2591237\tCoronavirus BtRs-BetaCoV/YN2018D\t\tscientific name\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Rule <fasta_cov_ncbi> generated 6 matches\n",
      "--------------------------------------------------------------------------------\n",
      "^>(?P<seqid>(?P<taxonomyid>\\d+):(?P<source>ncbi):(?P<seqnb>\\d*))[\\s\\t]*\\[(?P<accession>[\\w\\d]*)\\]([\\s\\t]*(?P=taxonomyid)[\\s\\t]*(?P=source)[\\s\\t]*(?P=seqnb)[\\s\\t]*\\[(?P=accession)\\][\\s\\t]*(?P=taxonomyid)[\\s\\t]*(?P<species>[\\w\\s\\-\\_\\/]*))?\n",
      "['seqid', 'taxonomyid', 'source', 'accession', 'seqnb', 'species']\n",
      "--------------------------------------------------------------------------------\n",
      "Rule <fasta_rhinolophus_ferrumequinum> generated an error\n",
      "No match on this line\n",
      "--------------------------------------------------------------------------------\n",
      "Rule <fastq_art_illumina> generated an error\n",
      "No match on this line\n",
      "--------------------------------------------------------------------------------\n",
      "Rule <aln_art_illumina> generated an error\n",
      "No match on this line\n",
      "--------------------------------------------------------------------------------\n",
      "Rule <aln_art_illumina-refseq> generated an error\n",
      "No match on this line\n",
      "--------------------------------------------------------------------------------\n",
      "Selected rule with most matches: fasta_cov_ncbi\n"
     ]
    }
   ],
   "source": [
    "print(f\"key for text to parse: {it.text_to_parse_key}\\n\")\n",
    "it.reset_iterator()\n",
    "print('Text to parse for testing (extracted from first iteration):')\n",
    "print(next(it)[it.text_to_parse_key])\n",
    "print()\n",
    "it.set_parsing_rules(verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If no saved rule generates a match, `re_rule_name`, `re_pattern` and `re_keys` remain `None` and a warning message is issued to ask user to add a parsing rule manually. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vtec/projects/bio/metagentools/metagentools/core.py:231: UserWarning: \n",
      "        None of the saved parsing rules were able to extract metadata from the first line in this file.\n",
      "        You must set a custom rule (pattern + keys) before parsing text, by using:\n",
      "            `self.set_parsing_rules(custom_pattern, custom_list_of_keys)`\n",
      "                \n",
      "  warnings.warn(msg, category=UserWarning)\n"
     ]
    }
   ],
   "source": [
    "it2 = FastaFileReader('data_dev/sequences_two_no_matching_rule.fa')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "it2.re_rule_name is None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But we still can set a standard rule manually, by passing a re pattern and the corresponding list of keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom Rule\n",
      "^>(?P<seqid>(?P<taxonomyid>\\d+):(?P<source>ncbi):(?P<seqnb>\\d*))\\s*(?P<text>[\\w\\s]*)$\n",
      "['seqid', 'taxonomyid', 'source', 'seqnb', 'text']\n"
     ]
    }
   ],
   "source": [
    "pat = r\"^>(?P<seqid>(?P<taxonomyid>\\d+):(?P<source>ncbi):(?P<seqnb>\\d*))\\s*(?P<text>[\\w\\s]*)$\"\n",
    "keys = \"seqid taxonomyid source seqnb text\".split()\n",
    "it2.set_parsing_rules(pattern=pat, keys=keys)\n",
    "\n",
    "print(it2.re_rule_name)\n",
    "print(it2.re_pattern)\n",
    "print(it2.re_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "definition line: '>2591237:ncbi:1 this sequence does not match any saved parsing rul'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'seqid': '2591237:ncbi:1',\n",
       " 'seqnb': '1',\n",
       " 'source': 'ncbi',\n",
       " 'taxonomyid': '2591237',\n",
       " 'text': 'this sequence does not match any saved parsing rule'}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "it2.reset_iterator()\n",
    "dfn_line, sequence = next(it2).values()\n",
    "print(f\"definition line: '{dfn_line[:-1]}'\")\n",
    "it2.parse_text(dfn_line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/vtecftwy/metagentools/blob/main/metagentools/cnn_virus/data.py#L71){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### FastaFileReader.parse_file\n",
       "\n",
       ">      FastaFileReader.parse_file (add_seq:bool=False, save_json:bool=False)\n",
       "\n",
       "Read fasta file and return a dictionary with definition line metadata and optionally sequences\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| add_seq | bool | False | When True, add the full sequence to the parsed metadata dictionary |\n",
       "| save_json | bool | False | When True, save the file metadata as a json file of same stem name |\n",
       "| **Returns** | **dict[str]** |  | **Metadata as Key/Values pairs** |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/vtecftwy/metagentools/blob/main/metagentools/cnn_virus/data.py#L71){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### FastaFileReader.parse_file\n",
       "\n",
       ">      FastaFileReader.parse_file (add_seq:bool=False, save_json:bool=False)\n",
       "\n",
       "Read fasta file and return a dictionary with definition line metadata and optionally sequences\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| add_seq | bool | False | When True, add the full sequence to the parsed metadata dictionary |\n",
       "| save_json | bool | False | When True, save the file metadata as a json file of same stem name |\n",
       "| **Returns** | **dict[str]** |  | **Metadata as Key/Values pairs** |"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(FastaFileReader.parse_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'11128:ncbi:2': {'accession': 'LC494191',\n",
      "                  'seqid': '11128:ncbi:2',\n",
      "                  'seqnb': '2',\n",
      "                  'source': 'ncbi',\n",
      "                  'species': None,\n",
      "                  'taxonomyid': '11128'},\n",
      " '2591237:ncbi:1': {'accession': 'MK211378',\n",
      "                    'seqid': '2591237:ncbi:1',\n",
      "                    'seqnb': '1',\n",
      "                    'source': 'ncbi',\n",
      "                    'species': 'Coronavirus BtRs-BetaCoV/YN2018D  scientific '\n",
      "                               'name',\n",
      "                    'taxonomyid': '2591237'}}\n"
     ]
    }
   ],
   "source": [
    "it = FastaFileReader(p2fasta)\n",
    "pprint(it.parse_file())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metadata for 'cov_virus_sequences_two.fa'> saved as <cov_virus_sequences_two_metadata.json> in  \n",
      "/home/vtec/projects/bio/metagentools/nbs-dev/data_dev\n",
      "\n"
     ]
    }
   ],
   "source": [
    "it.parse_file(save_json=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'aln_art_illumina': {'keys': 'refseqid '\n",
      "                              'reftaxonomyid '\n",
      "                              'refsource '\n",
      "                              'refseqnb '\n",
      "                              'readid '\n",
      "                              'readnb '\n",
      "                              'aln_start_pos '\n",
      "                              'refseq_strand',\n",
      "                      'pattern': '^>(?P<refseqid>(?P<reftaxonomyid>\\\\d*):(?P<refsource>\\\\w*):(?P<refseqnb>\\\\d*))(\\\\s|\\t'\n",
      "                                 ')*(?P<readid>(?P=reftaxonomyid):(?P=refsource):(?P=refseqnb)-(?P<readnb>\\\\d*(\\\\/\\\\d(-\\\\d)?)?))(\\\\s|\\t'\n",
      "                                 ')(?P<aln_start_pos>\\\\d*)(\\\\s|\\t'\n",
      "                                 ')(?P<refseq_strand>(-|\\\\+))$'},\n",
      " 'aln_art_illumina-refseq': {'keys': 'refseqid '\n",
      "                                     'reftaxonomyid '\n",
      "                                     'refsource '\n",
      "                                     'refseqnb '\n",
      "                                     'refseq_accession '\n",
      "                                     'species '\n",
      "                                     'refseq_length',\n",
      "                             'pattern': '^@SQ[\\\\t\\\\s]*(?P<refseqid>(?P<reftaxonomyid>\\\\d*):(?P<refsource>\\\\w*):(?P<refseqnb>\\\\d*))[\\\\t\\\\s]*\\\\[(?P<refseq_accession>[\\\\d\\\\w]*)\\\\][\\\\t\\\\s]*(?P=reftaxonomyid)[\\\\s\\\\t]*(?P=refsource)[\\\\s\\\\t]*(?P=refseqnb)[\\\\s\\\\t]*\\\\[(?P=refseq_accession)\\\\][\\\\s\\\\t]*(?P=reftaxonomyid)[\\\\s\\\\t]*(?P<species>\\\\w[\\\\w\\\\d\\\\/\\\\s\\\\-\\\\.]*)[\\\\s\\\\t](?P<refseq_length>\\\\d*)$'},\n",
      " 'fasta_cov_ncbi': {'keys': 'seqid '\n",
      "                            'taxonomyid '\n",
      "                            'source '\n",
      "                            'accession '\n",
      "                            'seqnb '\n",
      "                            'species',\n",
      "                    'pattern': '^>(?P<seqid>(?P<taxonomyid>\\\\d+):(?P<source>ncbi):(?P<seqnb>\\\\d*))[\\\\s\\\\t]*\\\\[(?P<accession>[\\\\w\\\\d]*)\\\\]([\\\\s\\\\t]*(?P=taxonomyid)[\\\\s\\\\t]*(?P=source)[\\\\s\\\\t]*(?P=seqnb)[\\\\s\\\\t]*\\\\[(?P=accession)\\\\][\\\\s\\\\t]*(?P=taxonomyid)[\\\\s\\\\t]*(?P<species>[\\\\w\\\\s\\\\-\\\\_\\\\/]*))?'},\n",
      " 'fasta_rhinolophus_ferrumequinum': {'keys': 'seq_type '\n",
      "                                             'id_type '\n",
      "                                             'assy '\n",
      "                                             'seq_level '\n",
      "                                             'taxonomy '\n",
      "                                             'id',\n",
      "                                     'pattern': '^>\\\\d[\\\\s\\\\t](?P<seq_type>dna_rm):(?P<id_type>[\\\\w\\\\_]*)[\\\\s\\\\w](?P=id_type):(?P<assy>[\\\\w\\\\d\\\\_]*)\\\\.(?P<seq_level>[\\\\w]*):\\\\d*:\\\\d*:(?P<taxonomy>\\\\d*):(?P<id>\\\\d*)[\\\\s\\t'\n",
      "                                                ']REF$'},\n",
      " 'fastq_art_illumina': {'keys': 'readid '\n",
      "                                'reftaxonomyid '\n",
      "                                'refsource '\n",
      "                                'refseqnb '\n",
      "                                'readnb',\n",
      "                        'pattern': '^@(?P<readid>(?P<reftaxonomyid>\\\\d*):(?P<refsource>\\\\w*):(?P<refseqnb>\\\\d*)-(?P<readnb>\\\\d*))$'}}\n"
     ]
    }
   ],
   "source": [
    "with open('../default_parsing_rules.json', 'r') as fp:\n",
    "    pprint(json.load(fp), width=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metadata for 'cov_virus_sequence_one.fa'> saved as <cov_virus_sequence_one_metadata.json> in  \n",
      "/home/vtec/projects/bio/metagentools/nbs-dev/data_dev\n",
      "\n",
      "{'2591237:ncbi:1': {'accession': 'MK211378',\n",
      "                    'seqid': '2591237:ncbi:1',\n",
      "                    'seqnb': '1',\n",
      "                    'source': 'ncbi',\n",
      "                    'species': 'Coronavirus BtRs-BetaCoV/YN2018D  scientific '\n",
      "                               'name',\n",
      "                    'taxonomyid': '2591237'}}\n"
     ]
    }
   ],
   "source": [
    "p2fasta = Path('data_dev/cov_virus_sequence_one.fa').resolve()\n",
    "it = FastaFileReader(p2fasta)\n",
    "fasta_meta = it.parse_file(save_json=True)\n",
    "pprint(fasta_meta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FASTQ file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extension of `TextFileBaseReader` class for fastq sequence files.\n",
    "\n",
    "Structure of a FASTQ sequence file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@2591237:ncbi:1-20100\n",
      "TTGCGCCTCCTTCACGATGTCCACACACTTAATGGCAACATTGTCAGTAAGTTTTAAATAACCAGTAAACTGGTTAACTG\n",
      "+\n",
      "CC=GGGG8GGGGG=JJJGJJJJJGJJJCJG1JJGJJGGJJJCJGGGGJGJJJGG=GJGGGJG=GGGGG=CGGCCCGGG8G\n",
      "@2591237:ncbi:1-20099\n",
      "TAATTTAGGTCCACAAACTGTAGCAGGTGCATTTAGAAGTTCAAATGAAAGCACAACAACCCTAGTAGCCTGATATTCAA\n",
      "+\n",
      "CCC1GGGGGGGGGJGJJJJJJJ1J=GJC=JJJJJJJJJGGGJJCGJGJJGJCJJ=JJJ=JG8GJJGJGGCJCCGCGGGGC\n",
      "@2591237:ncbi:1-20098\n",
      "CCACAAAGTGCTCCTCAGATGTCTTTGATGACGAAGTGAGGTATCCATTATATGTAGTAACAGCAGCTGGTGATGATACT\n",
      "+\n",
      "CCCCGGGGGGGGGGJJJGGJGCJJJCCJJGJGJCJG8GGJCJJJ8GJJJCJJGGGJGGG=GCGJC(CCCGGCCGCCGCCG\n"
     ]
    }
   ],
   "source": [
    "#| echo: false\n",
    "p2fastq = Path('data_dev/single_1seq_150bp/single_1seq_150bp.fq').resolve()\n",
    "\n",
    "it = TextFileBaseReader(p2fastq, nlines=1)\n",
    "for i, t in enumerate(it):\n",
    "    txt = t.replace('\\n', '')[:80]\n",
    "    print(f\"{txt}\")\n",
    "    if i >= 11: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class FastqFileReader(TextFileBaseReader):\n",
    "    \"\"\"Iterator going through a fastq file's sequences and return each section + prob error as a dict\"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        path:str|Path,   # path to the fastq file\n",
    "    )-> dict:           # key/value with keys: definition line; sequence; q score; prob error\n",
    "        self.nlines = 4\n",
    "        super().__init__(path, nlines=self.nlines)\n",
    "        self.text_to_parse_key = 'definition line'\n",
    "        self.set_parsing_rules(verbose=False)        \n",
    "    \n",
    "    def __next__(self):\n",
    "        \"\"\"Return definition line, sequence and quality scores\"\"\"\n",
    "        lines = []\n",
    "        for i in range(self.nlines):\n",
    "            lines.append(self._safe_readline().replace('\\n', ''))\n",
    "        \n",
    "        output = {\n",
    "            'definition line':lines[0], \n",
    "            'sequence':f\"{lines[1]}\", \n",
    "            'read_qscores': f\"{lines[3]}\",\n",
    "        }\n",
    "        output['probs error'] = np.array([q_score2prob_error(q) for q in output['read_qscores']])\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    def print_first_chunks(\n",
    "        self, \n",
    "        nchunks:int=3,  # number of chunks to print out\n",
    "    ):\n",
    "        \"\"\"Print the first `nchunks` chunks of text from the file\"\"\"\n",
    "        for i, seq_dict in enumerate(self.__iter__()):\n",
    "            print(f\"\\nSequence {i+1}:\")\n",
    "            print(seq_dict['definition line'])\n",
    "            print(f\"{seq_dict['sequence'][:80]} ...\")\n",
    "            if i >= nchunks: break\n",
    "            \n",
    "    def parse_file(\n",
    "        self,\n",
    "        add_readseq :bool=False,    # When True, add the full sequence to the parsed metadata dictionary\n",
    "        add_qscores:bool=False,     # Add the read ASCII Q Scores to the parsed dictionary when True\n",
    "        add_probs_error:bool=False, # Add the read probability of error to the parsed dictionary when True\n",
    "        save_json: bool=False       # When True, save the file metadata as a json file of same stem name\n",
    "    )-> dict[str]:                  # Metadata as Key/Values pairs\n",
    "        \"\"\"Read fastq file, return a dict with definition line metadata and optionally read sequence and q scores, ...\"\"\"\n",
    "    \n",
    "        self.reset_iterator()\n",
    "        parsed = {}\n",
    "        for d in self:\n",
    "            dfn_line = d['definition line']\n",
    "            seq, q_scores, prob_e = d['sequence'], d['read_qscores'], d['probs error']\n",
    "            metadata = self._parse_text_fn(dfn_line, self.re_pattern, self.re_keys)\n",
    "            if add_readseq: metadata['readseq'] = seq         \n",
    "            if add_qscores: metadata['read_qscores'] = q_scores\n",
    "            if add_probs_error: metadata['probs error'] = prob_e\n",
    "            parsed[metadata['readid']] = metadata \n",
    "                        \n",
    "        if save_json:\n",
    "            p2json = self.path.parent / f\"{self.path.stem}_metadata.json\"\n",
    "            with open(p2json, 'w') as fp:\n",
    "                json.dump(parsed, fp, indent=4)\n",
    "                print(f\"Metadata for '{self.path.name}'> saved as <{p2json.name}> in  \\n{p2json.parent.absolute()}\\n\")\n",
    "\n",
    "        return parsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/vtecftwy/metagentools/blob/main/metagentools/cnn_virus/data.py#L96){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### FastqFileReader\n",
       "\n",
       ">      FastqFileReader (path:str|pathlib.Path)\n",
       "\n",
       "Iterator going through a fastq file's sequences and return each section + prob error as a dict\n",
       "\n",
       "|    | **Type** | **Details** |\n",
       "| -- | -------- | ----------- |\n",
       "| path | str \\| Path | path to the fastq file |\n",
       "| **Returns** | **dict** | **key/value with keys: definition line; sequence; q score; prob error** |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/vtecftwy/metagentools/blob/main/metagentools/cnn_virus/data.py#L96){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### FastqFileReader\n",
       "\n",
       ">      FastqFileReader (path:str|pathlib.Path)\n",
       "\n",
       "Iterator going through a fastq file's sequences and return each section + prob error as a dict\n",
       "\n",
       "|    | **Type** | **Details** |\n",
       "| -- | -------- | ----------- |\n",
       "| path | str \\| Path | path to the fastq file |\n",
       "| **Returns** | **dict** | **key/value with keys: definition line; sequence; q score; prob error** |"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(FastqFileReader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n",
      "dict_keys(['definition line', 'sequence', 'read_qscores', 'probs error'])\n",
      "Definition line:  @2591237:ncbi:1-20100\n",
      "Read sequence:    TTGCGCCTCCTTCACGATGTCCACACACTTAATGGCAACATTGTCAGTAAGTTTTAAATAACCAGTAAACTGGTTAACTGGTTCCTCAGGTGTTGGTTCTGATTCTAGTTCATGCTCCGATAATTCGGTAGCATCACCAAGCCAGTCCTC\n",
      "Q scores (ASCII): CC=GGGG8GGGGG=JJJGJJJJJGJJJCJG1JJGJJGGJJJCJGGGGJGJJJGG=GJGGGJG=GGGGG=CGGCCCGGG8GGGGGGGCGCGG=G1G=GCGGCCGGGG=CC=8G=GGGCGGG=GGGGCGGCGGGGCCGGCGCCGGCCGGGCG\n",
      "Prob error:       0.0004,0.0004,0.0016,0.0002,0.0002,0.0002,0.0002,0.0050,0.0002,0.0002,0.0002,0.0002,0.0002,0.0016,0.0001,0.0001,0.0001,0.0002,0.0001,0.0001,0.0001,0.0001,0.0001,0.0002,0.0001,0.0001,0.0001,0.0004,0.0001,0.0002,0.0251,0.0001,0.0001,0.0002,0.0001,0.0001,0.0002,0.0002,0.0001,0.0001,0.0001,0.0004,0.0001,0.0002,0.0002,0.0002,0.0002,0.0001,0.0002,0.0001,0.0001,0.0001,0.0002,0.0002,0.0016,0.0002,0.0001,0.0002,0.0002,0.0002,0.0001,0.0002,0.0016,0.0002,0.0002,0.0002,0.0002,0.0002,0.0016,0.0004,0.0002,0.0002,0.0004,0.0004,0.0004,0.0002,0.0002,0.0002,0.0050,0.0002,0.0002,0.0002,0.0002,0.0002,0.0002,0.0002,0.0004,0.0002,0.0004,0.0002,0.0002,0.0016,0.0002,0.0251,0.0002,0.0016,0.0002,0.0004,0.0002,0.0002,0.0004,0.0004,0.0002,0.0002,0.0002,0.0002,0.0016,0.0004,0.0004,0.0016,0.0050,0.0002,0.0016,0.0002,0.0002,0.0002,0.0004,0.0002,0.0002,0.0002,0.0016,0.0002,0.0002,0.0002,0.0002,0.0004,0.0002,0.0002,0.0004,0.0002,0.0002,0.0002,0.0002,0.0004,0.0004,0.0002,0.0002,0.0004,0.0002,0.0004,0.0004,0.0002,0.0002,0.0004,0.0004,0.0002,0.0002,0.0002,0.0004,0.0002\n"
     ]
    }
   ],
   "source": [
    "it = FastqFileReader(p2fastq)\n",
    "iteration_output = next(it)\n",
    "\n",
    "print(type(iteration_output))\n",
    "print(iteration_output.keys())\n",
    "print(f\"Definition line:  {iteration_output['definition line']}\")\n",
    "print(f\"Read sequence:    {iteration_output['sequence']}\")\n",
    "print(f\"Q scores (ASCII): {iteration_output['read_qscores']}\")\n",
    "print(f\"Prob error:       {','.join([f'{p:.4f}' for p in iteration_output['probs error']])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Five largest probabilities of error:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00501187, 0.00501187, 0.00501187, 0.02511886, 0.02511886])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sort(iteration_output['probs error'])[-5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  7, 110,  78,  93,  30])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argsort(iteration_output['probs error'])[-5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'readid': '2591237:ncbi:1-20100',\n",
       " 'readnb': '20100',\n",
       " 'refseqnb': '1',\n",
       " 'refsource': 'ncbi',\n",
       " 'reftaxonomyid': '2591237'}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfn_line = iteration_output['definition line']\n",
    "meta = it.parse_text(dfn_line)\n",
    "meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['definition line', 'sequence', 'read_qscores', 'probs error'])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fastq = FastqFileReader(p2fastq)\n",
    "next(fastq).keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/vtecftwy/metagentools/blob/main/metagentools/cnn_virus/data.py#L133){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### FastqFileReader.parse_file\n",
       "\n",
       ">      FastqFileReader.parse_file (add_readseq:bool=False,\n",
       ">                                  add_qscores:bool=False,\n",
       ">                                  add_probs_error:bool=False,\n",
       ">                                  save_json:bool=False)\n",
       "\n",
       "Read fastq file, return a dict with definition line metadata and optionally read sequence and q scores, ...\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| add_readseq | bool | False | When True, add the full sequence to the parsed metadata dictionary |\n",
       "| add_qscores | bool | False | Add the read ASCII Q Scores to the parsed dictionary when True |\n",
       "| add_probs_error | bool | False | Add the read probability of error to the parsed dictionary when True |\n",
       "| save_json | bool | False | When True, save the file metadata as a json file of same stem name |\n",
       "| **Returns** | **dict[str]** |  | **Metadata as Key/Values pairs** |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/vtecftwy/metagentools/blob/main/metagentools/cnn_virus/data.py#L133){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### FastqFileReader.parse_file\n",
       "\n",
       ">      FastqFileReader.parse_file (add_readseq:bool=False,\n",
       ">                                  add_qscores:bool=False,\n",
       ">                                  add_probs_error:bool=False,\n",
       ">                                  save_json:bool=False)\n",
       "\n",
       "Read fastq file, return a dict with definition line metadata and optionally read sequence and q scores, ...\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| add_readseq | bool | False | When True, add the full sequence to the parsed metadata dictionary |\n",
       "| add_qscores | bool | False | Add the read ASCII Q Scores to the parsed dictionary when True |\n",
       "| add_probs_error | bool | False | Add the read probability of error to the parsed dictionary when True |\n",
       "| save_json | bool | False | When True, save the file metadata as a json file of same stem name |\n",
       "| **Returns** | **dict[str]** |  | **Metadata as Key/Values pairs** |"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(FastqFileReader.parse_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2591237:ncbi:1-20100\n",
      "{'readid': '2591237:ncbi:1-20100',\n",
      " 'readnb': '20100',\n",
      " 'refseqnb': '1',\n",
      " 'refsource': 'ncbi',\n",
      " 'reftaxonomyid': '2591237'}\n",
      "2591237:ncbi:1-20099\n",
      "{'readid': '2591237:ncbi:1-20099',\n",
      " 'readnb': '20099',\n",
      " 'refseqnb': '1',\n",
      " 'refsource': 'ncbi',\n",
      " 'reftaxonomyid': '2591237'}\n",
      "2591237:ncbi:1-20098\n",
      "{'readid': '2591237:ncbi:1-20098',\n",
      " 'readnb': '20098',\n",
      " 'refseqnb': '1',\n",
      " 'refsource': 'ncbi',\n",
      " 'reftaxonomyid': '2591237'}\n",
      "2591237:ncbi:1-20097\n",
      "{'readid': '2591237:ncbi:1-20097',\n",
      " 'readnb': '20097',\n",
      " 'refseqnb': '1',\n",
      " 'refsource': 'ncbi',\n",
      " 'reftaxonomyid': '2591237'}\n"
     ]
    }
   ],
   "source": [
    "parsed = fastq.parse_file(add_readseq=False, add_qscores=False, add_probs_error=False)\n",
    "for i, (k, v) in enumerate(parsed.items()):\n",
    "    print(k)\n",
    "    pprint(v)\n",
    "    if i >=3: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>readid</th>\n",
       "      <th>readnb</th>\n",
       "      <th>refseqnb</th>\n",
       "      <th>refsource</th>\n",
       "      <th>reftaxonomyid</th>\n",
       "      <th>readseq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2591237:ncbi:1-20100</th>\n",
       "      <td>2591237:ncbi:1-20100</td>\n",
       "      <td>20100</td>\n",
       "      <td>1</td>\n",
       "      <td>ncbi</td>\n",
       "      <td>2591237</td>\n",
       "      <td>TTGCGCCTCCTTCACGATGTCCACACACTTAATGGCAACATTGTCA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2591237:ncbi:1-20099</th>\n",
       "      <td>2591237:ncbi:1-20099</td>\n",
       "      <td>20099</td>\n",
       "      <td>1</td>\n",
       "      <td>ncbi</td>\n",
       "      <td>2591237</td>\n",
       "      <td>TAATTTAGGTCCACAAACTGTAGCAGGTGCATTTAGAAGTTCAAAT...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2591237:ncbi:1-20098</th>\n",
       "      <td>2591237:ncbi:1-20098</td>\n",
       "      <td>20098</td>\n",
       "      <td>1</td>\n",
       "      <td>ncbi</td>\n",
       "      <td>2591237</td>\n",
       "      <td>CCACAAAGTGCTCCTCAGATGTCTTTGATGACGAAGTGAGGTATCC...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2591237:ncbi:1-20097</th>\n",
       "      <td>2591237:ncbi:1-20097</td>\n",
       "      <td>20097</td>\n",
       "      <td>1</td>\n",
       "      <td>ncbi</td>\n",
       "      <td>2591237</td>\n",
       "      <td>ATGTAAAAGTGTTACCATCACAAGTGTTCTTGTAGGTACCATAATC...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2591237:ncbi:1-20096</th>\n",
       "      <td>2591237:ncbi:1-20096</td>\n",
       "      <td>20096</td>\n",
       "      <td>1</td>\n",
       "      <td>ncbi</td>\n",
       "      <td>2591237</td>\n",
       "      <td>AGAAGCACCAGCACATATGTCAACAATAGGTGTCTGCACAATGACT...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2591237:ncbi:1-20095</th>\n",
       "      <td>2591237:ncbi:1-20095</td>\n",
       "      <td>20095</td>\n",
       "      <td>1</td>\n",
       "      <td>ncbi</td>\n",
       "      <td>2591237</td>\n",
       "      <td>GACTGGTTTGTAAAAATTGGACCTCGCAAGTCTGCTCGCCTAGTAC...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2591237:ncbi:1-20094</th>\n",
       "      <td>2591237:ncbi:1-20094</td>\n",
       "      <td>20094</td>\n",
       "      <td>1</td>\n",
       "      <td>ncbi</td>\n",
       "      <td>2591237</td>\n",
       "      <td>TGCTTGTGTTTTCCACATAGGCAGCCATAAGATCCTCATGACCTAA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2591237:ncbi:1-20093</th>\n",
       "      <td>2591237:ncbi:1-20093</td>\n",
       "      <td>20093</td>\n",
       "      <td>1</td>\n",
       "      <td>ncbi</td>\n",
       "      <td>2591237</td>\n",
       "      <td>CTGCTGACATTGTAGTCTTTGATGAAATCTCTATGGCTACCAATTA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2591237:ncbi:1-20092</th>\n",
       "      <td>2591237:ncbi:1-20092</td>\n",
       "      <td>20092</td>\n",
       "      <td>1</td>\n",
       "      <td>ncbi</td>\n",
       "      <td>2591237</td>\n",
       "      <td>CTCATCAACTGGCACTTTCTTCAAAGCTCTTGAGAGCATCTCTGTA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2591237:ncbi:1-20091</th>\n",
       "      <td>2591237:ncbi:1-20091</td>\n",
       "      <td>20091</td>\n",
       "      <td>1</td>\n",
       "      <td>ncbi</td>\n",
       "      <td>2591237</td>\n",
       "      <td>TGCTACAGCTCATAGCGAGCTGGCAAAGGGTGTAGCTTTAGATGGT...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    readid readnb refseqnb refsource  \\\n",
       "2591237:ncbi:1-20100  2591237:ncbi:1-20100  20100        1      ncbi   \n",
       "2591237:ncbi:1-20099  2591237:ncbi:1-20099  20099        1      ncbi   \n",
       "2591237:ncbi:1-20098  2591237:ncbi:1-20098  20098        1      ncbi   \n",
       "2591237:ncbi:1-20097  2591237:ncbi:1-20097  20097        1      ncbi   \n",
       "2591237:ncbi:1-20096  2591237:ncbi:1-20096  20096        1      ncbi   \n",
       "2591237:ncbi:1-20095  2591237:ncbi:1-20095  20095        1      ncbi   \n",
       "2591237:ncbi:1-20094  2591237:ncbi:1-20094  20094        1      ncbi   \n",
       "2591237:ncbi:1-20093  2591237:ncbi:1-20093  20093        1      ncbi   \n",
       "2591237:ncbi:1-20092  2591237:ncbi:1-20092  20092        1      ncbi   \n",
       "2591237:ncbi:1-20091  2591237:ncbi:1-20091  20091        1      ncbi   \n",
       "\n",
       "                     reftaxonomyid  \\\n",
       "2591237:ncbi:1-20100       2591237   \n",
       "2591237:ncbi:1-20099       2591237   \n",
       "2591237:ncbi:1-20098       2591237   \n",
       "2591237:ncbi:1-20097       2591237   \n",
       "2591237:ncbi:1-20096       2591237   \n",
       "2591237:ncbi:1-20095       2591237   \n",
       "2591237:ncbi:1-20094       2591237   \n",
       "2591237:ncbi:1-20093       2591237   \n",
       "2591237:ncbi:1-20092       2591237   \n",
       "2591237:ncbi:1-20091       2591237   \n",
       "\n",
       "                                                                readseq  \n",
       "2591237:ncbi:1-20100  TTGCGCCTCCTTCACGATGTCCACACACTTAATGGCAACATTGTCA...  \n",
       "2591237:ncbi:1-20099  TAATTTAGGTCCACAAACTGTAGCAGGTGCATTTAGAAGTTCAAAT...  \n",
       "2591237:ncbi:1-20098  CCACAAAGTGCTCCTCAGATGTCTTTGATGACGAAGTGAGGTATCC...  \n",
       "2591237:ncbi:1-20097  ATGTAAAAGTGTTACCATCACAAGTGTTCTTGTAGGTACCATAATC...  \n",
       "2591237:ncbi:1-20096  AGAAGCACCAGCACATATGTCAACAATAGGTGTCTGCACAATGACT...  \n",
       "2591237:ncbi:1-20095  GACTGGTTTGTAAAAATTGGACCTCGCAAGTCTGCTCGCCTAGTAC...  \n",
       "2591237:ncbi:1-20094  TGCTTGTGTTTTCCACATAGGCAGCCATAAGATCCTCATGACCTAA...  \n",
       "2591237:ncbi:1-20093  CTGCTGACATTGTAGTCTTTGATGAAATCTCTATGGCTACCAATTA...  \n",
       "2591237:ncbi:1-20092  CTCATCAACTGGCACTTTCTTCAAAGCTCTTGAGAGCATCTCTGTA...  \n",
       "2591237:ncbi:1-20091  TGCTACAGCTCATAGCGAGCTGGCAAAGGGTGTAGCTTTAGATGGT...  "
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata = it.parse_file(add_readseq=True)\n",
    "df = pd.DataFrame(metadata).T\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "Rule <fasta_cov_ncbi> generated an error\n",
      "No match on this line\n",
      "--------------------------------------------------------------------------------\n",
      "Rule <fasta_rhinolophus_ferrumequinum> generated an error\n",
      "No match on this line\n",
      "--------------------------------------------------------------------------------\n",
      "Rule <fastq_art_illumina> generated 5 matches\n",
      "--------------------------------------------------------------------------------\n",
      "^@(?P<readid>(?P<reftaxonomyid>\\d*):(?P<refsource>\\w*):(?P<refseqnb>\\d*)-(?P<readnb>\\d*))$\n",
      "['readid', 'reftaxonomyid', 'refsource', 'refseqnb', 'readnb']\n",
      "--------------------------------------------------------------------------------\n",
      "Rule <aln_art_illumina> generated an error\n",
      "No match on this line\n",
      "--------------------------------------------------------------------------------\n",
      "Rule <aln_art_illumina-refseq> generated an error\n",
      "No match on this line\n",
      "--------------------------------------------------------------------------------\n",
      "Selected rule with most matches: fastq_art_illumina\n"
     ]
    }
   ],
   "source": [
    "fastq.set_parsing_rules(verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ALN Alignment Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extension of `TextFileBaseReader` class for ALN read/sequence alignment files.\n",
    "\n",
    "Structure of a ALN sequence file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##ART_Illumina\tread_length\t150\n",
      "@CM\t/bin/art_illumina -i /home/vtec/projects/bio/metagentools/nbs-dev/data_dev/c\n",
      "@SQ\t2591237:ncbi:1 [MK211378]\t2591237\tncbi\t1 [MK211378] 2591237\tCoronavirus BtRs\n",
      "##Header End\n",
      ">2591237:ncbi:1\t2591237:ncbi:1-20100\t26865\t-\n",
      "TTGCGCCTCCTTCACGATGTCCACACACTTAATGGCAACATTGTCAGTAAGTTTTAAATAACCAGTAAACTGGTTAACTG\n",
      "TTGCGCCTCCTTCACGATGTCCACACACTTAATGGCAACATTGTCAGTAAGTTTTAAATAACCAGTAAACTGGTTAACTG\n",
      ">2591237:ncbi:1\t2591237:ncbi:1-20099\t7219\t-\n",
      "TAATTTAGGTCCACAAACTGTAGCAGGTGCATTTAGAAGTTCAAATGAAAGCACAACAACCCTAGTAGCCTGATATTCAA\n",
      "TAATTTAGGTCCACAAACTGTAGCAGGTGCATTTAGAAGTTCAAATGAAAGCACAACAACCCTAGTAGCCTGATATTCAA\n",
      ">2591237:ncbi:1\t2591237:ncbi:1-20098\t25514\t-\n",
      "CCACAAAGTGCTCCTCAGATGTCTTTGATGACGAAGTGAGGTATCCATTATATGTAGTAACAGCATCTGGTGATGATACT\n",
      "CCACAAAGTGCTCCTCAGATGTCTTTGATGACGAAGTGAGGTATCCATTATATGTAGTAACAGCAGCTGGTGATGATACT\n"
     ]
    }
   ],
   "source": [
    "#| echo: false\n",
    "p2aln = Path('data_dev/single_1seq_150bp/single_1seq_150bp.aln').resolve()\n",
    "assert p2aln.is_file()\n",
    "\n",
    "it = TextFileBaseReader(p2aln, nlines=1)\n",
    "for i, t in enumerate(it):\n",
    "    txt = t.replace('\\n', '')[:80]\n",
    "    print(f\"{txt}\")\n",
    "    if i >= 12: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#| export\n",
    "class AlnFileReader(TextFileBaseReader):\n",
    "    \"\"\"Iterator going through an ALN file\"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        path:str|Path,   # path to the aln file\n",
    "    )-> dict:            # key/value with keys: \n",
    "        \"\"\"Set TextFileBaseReader attributes and specific class attributes\"\"\"\n",
    "        self.nlines = 1\n",
    "        super().__init__(path, nlines=self.nlines)\n",
    "        self.header = self.read_header()\n",
    "        self.nlines = 3\n",
    "        self.text_to_parse_key = 'definition line'\n",
    "        self.set_parsing_rules(verbose=False)\n",
    "        self.set_header_parsing_rules(verbose=False)\n",
    "        self.ref_sequences = self.parse_header_reference_sequences()\n",
    "\n",
    "    def __next__(self):\n",
    "        \"\"\"Return definition line, sequence and quality scores\"\"\"\n",
    "        lines = []\n",
    "        for i in range(self.nlines):\n",
    "            lines.append(self._safe_readline().replace('\\n', ''))\n",
    "\n",
    "        output = {\n",
    "            'definition line':lines[0], \n",
    "            'ref_seq_aligned':f\"{lines[1]}\", \n",
    "            'read_seq_aligned': f\"{lines[2]}\",\n",
    "        }   \n",
    "        return output\n",
    "    \n",
    "    def read_header(self):\n",
    "        \"\"\"Read ALN file Header and return each section parsed in a dictionary\"\"\"\n",
    "        \n",
    "        header = {}\n",
    "        if self.fp is not None:\n",
    "            self.fp.close()\n",
    "        self.fp = open(self.path, 'r')\n",
    "        \n",
    "        line = self._safe_readline().replace('\\n', '')\n",
    "        if not line.startswith('##ART_Illumina'): \n",
    "            raise ValueError(f\"Header of this file does not start with ##ART_Illumina\")\n",
    "        line = self._safe_readline().replace('\\n', '')\n",
    "        if not line.startswith('@CM'): \n",
    "            raise ValueError(f\"First header line should start with @CM\")\n",
    "        else: \n",
    "            header['command'] = line[3:].replace('\\t', '').strip()\n",
    "\n",
    "        refseqs = []\n",
    "        while True:\n",
    "            line = self._safe_readline().replace('\\n', '')\n",
    "            if line.startswith('##Header End'): break\n",
    "            else:\n",
    "                refseqs.append(line)\n",
    "        header['reference sequences'] = refseqs\n",
    "        \n",
    "        return header\n",
    "    \n",
    "    def reset_iterator(self):\n",
    "        \"\"\"Reset the iterator to point to the first line in the file, by recreating a new file handle.\n",
    "        \n",
    "        `AlnFileReader` requires a specific `reset_iterator` method, in order to skip the header every time it is reset\n",
    "        \"\"\"\n",
    "        if self.fp is not None:\n",
    "            self.fp.close()\n",
    "        self.fp = open(self.path, 'r')\n",
    "        while True:\n",
    "            line = self._safe_readline().replace('\\n', '')\n",
    "            if line.startswith('##Header End'): break\n",
    "\n",
    "    def parse_definition_line_with_position(\n",
    "        self, \n",
    "        dfn_line:str    # fefinition line string to be parsed\n",
    "        )-> dict:       # parsed metadata in key/value format + relative position of the read\n",
    "        \"\"\"Parse definition line and adds relative position\"\"\"\n",
    "        read_meta = self.parse_text(dfn_line)\n",
    "        read_refseqid = read_meta['refseqid']\n",
    "        read_start_pos = int(read_meta['aln_start_pos'])\n",
    "        read_refseq_lentgh = int(self.ref_sequences[read_refseqid]['refseq_length'])\n",
    "        read_meta['read_pos'] = (read_start_pos *10)// read_refseq_lentgh + 1\n",
    "        return read_meta\n",
    "    \n",
    "    def parse_file(\n",
    "        self, \n",
    "        add_ref_seq_aligned:bool=False,   # Add the reference sequence aligned to the parsed dictionary when True\n",
    "        add_read_seq_aligned:bool=False,  # Add the read sequence aligned to the parsed dictionary when True\n",
    "    )-> dict[str]: \n",
    "        # Key/Values. Keys: \n",
    "        # `readid`,`seqid`,`seq_nbr`,`read_nbr`,`aln_start_pos`,`ref_seq_strand`\n",
    "        # optionaly `ref_seq_aligned`,`read_seq_aligned`\n",
    "        \"\"\"Read ALN file, return a dict w/ alignment info for each read and optionaly aligned reference sequence & read\"\"\"\n",
    "        self.reset_iterator()\n",
    "        parsed = {}\n",
    "        for d in self:\n",
    "            dfn_line = d['definition line']\n",
    "            ref_seq_aligned, read_seq_aligned = d['ref_seq_aligned'], d['read_seq_aligned']\n",
    "            metadata = self.parse_text(dfn_line)\n",
    "            if add_ref_seq_aligned: metadata['ref_seq_aligned'] = ref_seq_aligned         \n",
    "            if add_read_seq_aligned: metadata['read_seq_aligned'] = read_seq_aligned\n",
    "            parsed[metadata['readid']] = metadata \n",
    "        return parsed\n",
    "\n",
    "    def parse_header_reference_sequences(\n",
    "        self,\n",
    "        pattern:str|None=None,     # regex pattern to apply to parse the reference sequence info\n",
    "        keys:list[str]|None=None,  # list of keys: keys are both regex match group names and corresponding output dict keys \n",
    "        )->dict[str]:                  # parsed metadata in key/value format\n",
    "        \"\"\"Extract metadata from all header reference sequences\"\"\"\n",
    "        if pattern is None and keys is None:\n",
    "            pattern, keys = self.re_header_pattern, self.re_header_keys\n",
    "        parsed = {}\n",
    "        for seq_dfn_line in self.header['reference sequences']:\n",
    "            metadata = self.parse_text(seq_dfn_line, pattern, keys)\n",
    "            parsed[metadata['refseqid']] = metadata\n",
    "            \n",
    "        return parsed       \n",
    "        \n",
    "    def set_header_parsing_rules(\n",
    "        self,\n",
    "        pattern: str|bool=None,   # regex pattern to apply to parse the text, search in parsing rules json if None\n",
    "        keys: list[str]=None,     # list of keys/group for regex, search in parsing rules json if None\n",
    "        verbose: bool=False       # when True, provides information on each rule\n",
    "    )-> None:\n",
    "        \"\"\"Set the regex parsing rule for reference sequence in ALN header.\n",
    "               \n",
    "        Updates 3 class attributes: `re_header_rule_name`, `re_header_pattern`, `re_header_keys`\n",
    "        \n",
    "        TODO: refactor this and the method in Core: to use a single function for the common part and a parameter for the text_to_parse \n",
    "        \"\"\"\n",
    "        \n",
    "        P2JSON = Path(f\"{PACKAGE_ROOT}/default_parsing_rules.json\")\n",
    "        \n",
    "        self.re_header_rule_name = None\n",
    "        self.re_header_pattern = None\n",
    "        self.re_header_keys = None\n",
    "        \n",
    "        # get the first reference sequence definition line in header\n",
    "        text_to_parse = self.header['reference sequences'][0]\n",
    "        divider_line = f\"{'-'*80}\"\n",
    "\n",
    "        if pattern is not None and keys is not None:  # When specific pattern and keys are passed\n",
    "            try:\n",
    "                metadata_dict = self.parse_text(text_to_parse, pattern, keys)\n",
    "                self.re_header_rule_name = 'Custom Rule'\n",
    "                self.re_header_pattern = pattern\n",
    "                self.re_header_keys = keys\n",
    "                if verbose:\n",
    "                    print(divider_line)\n",
    "                    print(f\"Custom rule was set for header in this instance.\")\n",
    "            except Exception as err: \n",
    "                raise ValueError(f\"The pattern generates the following error:\\n{err}\")\n",
    "                \n",
    "        else:  # automatic rule selection among rules saved in json file\n",
    "            # Load all existing rules from json file\n",
    "            with open(P2JSON, 'r') as fp:\n",
    "                parsing_rules = json.load(fp)\n",
    "                \n",
    "            # test all existing rules and keep the one with highest number of matches\n",
    "            max_nbr_matches = 0\n",
    "            for k, v in parsing_rules.items():\n",
    "                re_header_pattern = v['pattern']\n",
    "                re_header_keys = v['keys'].split(' ')\n",
    "                try:\n",
    "                    metadata_dict = self.parse_text(text_to_parse, re_header_pattern, re_header_keys)\n",
    "                    nbr_matches = len(metadata_dict)\n",
    "                    if verbose:\n",
    "                        print(divider_line)\n",
    "                        print(f\"Rule <{k}> generated {nbr_matches:,d} matches\")\n",
    "                        print(divider_line)\n",
    "                        print(re_header_pattern)\n",
    "                        print(re_header_keys)\n",
    "\n",
    "                    if len(metadata_dict) > max_nbr_matches:\n",
    "                        self.re_header_pattern = re_header_pattern\n",
    "                        self.re_header_keys = re_header_keys\n",
    "                        self.re_header_rule_name = k    \n",
    "                except Exception as err:\n",
    "                    if verbose:\n",
    "                        print(divider_line)\n",
    "                        print(f\"Rule <{k}> generated an error\")\n",
    "                        print(err)\n",
    "                    else:\n",
    "                        pass\n",
    "            if self.re_header_rule_name is None:\n",
    "                msg = \"\"\"\n",
    "        None of the saved parsing rules were able to extract metadata from the first line in this file.\n",
    "        You must set a custom rule (pattern + keys) before parsing text, by using:\n",
    "            `self.set_parsing_rules(custom_pattern, custom_list_of_keys)`\n",
    "                \"\"\"\n",
    "                warnings.warn(msg, category=UserWarning)\n",
    "            \n",
    "            if verbose:\n",
    "                print(divider_line)\n",
    "                print(f\"Selected rule with most matches: {self.re_header_rule_name}\")\n",
    "\n",
    "            # We used the iterator, now we need to reset it to make all lines available\n",
    "            self.reset_iterator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/vtecftwy/metagentools/blob/main/metagentools/cnn_virus/data.py#L162){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### AlnFileReader\n",
       "\n",
       ">      AlnFileReader (path:str|pathlib.Path)\n",
       "\n",
       "Iterator going through an ALN file\n",
       "\n",
       "|    | **Type** | **Details** |\n",
       "| -- | -------- | ----------- |\n",
       "| path | str \\| Path | path to the aln file |\n",
       "| **Returns** | **dict** | **key/value with keys:** |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/vtecftwy/metagentools/blob/main/metagentools/cnn_virus/data.py#L162){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### AlnFileReader\n",
       "\n",
       ">      AlnFileReader (path:str|pathlib.Path)\n",
       "\n",
       "Iterator going through an ALN file\n",
       "\n",
       "|    | **Type** | **Details** |\n",
       "| -- | -------- | ----------- |\n",
       "| path | str \\| Path | path to the aln file |\n",
       "| **Returns** | **dict** | **key/value with keys:** |"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(AlnFileReader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "it = AlnFileReader(p2aln)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`AlnFileReader` iterator returns elements one by one, as dictionaries with each data line related to the read, accessible through the following keys: \n",
    "\n",
    "- key `'definition line'`: **read definition line**, including read metadata \n",
    "- key `'ref_seq_aligned'`: **aligned reference sequence**, that is the sequence segment in the original reference corresponding to the read\n",
    "- key `'read_seq_aligned'`: **aligned read**, that is the simmulated read sequence, where each bp corresponds to the reference sequence bp in the same position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['definition line', 'ref_seq_aligned', 'read_seq_aligned'])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_iteration = next(it)\n",
    "one_iteration.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'definition line': '>2591237:ncbi:1\\t2591237:ncbi:1-20100\\t26865\\t-',\n",
      " 'read_seq_aligned': 'TTGCGCCTCCTTCACGATGTCCACACACTTAATGGCAACATTGTCAGTAAGTTTTAAATAACCAGTAAACTGGTTAACTGGTTCCTCAGGTGTTGGTTCTGATTCTAGTTCATGCTCCGATAATTCGGTAGCATCACCAAGCCAGTCCTC',\n",
      " 'ref_seq_aligned': 'TTGCGCCTCCTTCACGATGTCCACACACTTAATGGCAACATTGTCAGTAAGTTTTAAATAACCAGTAAACTGGTTAACTGGTTCCTCAGGTGTTGGTTCTGATTCTAGTTCATGCTCCGATAATTCGGTAGCATCACCAAGCCAGTCCTC'}\n"
     ]
    }
   ],
   "source": [
    "pprint(one_iteration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfn_line, ref_seq_aligned, read_seq_aligned = one_iteration.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'>2591237:ncbi:1\\t2591237:ncbi:1-20100\\t26865\\t-'"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfn_line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'TTGCGCCTCCTTCACGATGTCCACACACTTAATGGCAACATTGTCAGTAAGTTTTAAATAACCAGTAAACTGGTTAACTGGTTCCTCAGGTGTTGGTTCT'"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ref_seq_aligned[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'TTGCGCCTCCTTCACGATGTCCACACACTTAATGGCAACATTGTCAGTAAGTTTTAAATAACCAGTAAACTGGTTAACTGGTTCCTCAGGTGTTGGTTCT'"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "read_seq_aligned[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'definition line': '>2591237:ncbi:1\\t2591237:ncbi:1-20099\\t7219\\t-',\n",
      " 'read_seq_aligned': 'TAATTTAGGTCCACAAACTGTAGCAGGTGCATTTAGAAGTTCAAATGAAAGCACAACAACCCTAGTAGCCTGATATTCAATAGGCACATTAGGATAGAAGTCATAAGTACTAAGAGTACGTACACCATTTTCGTCAGAAGTTAGATCCCT',\n",
      " 'ref_seq_aligned': 'TAATTTAGGTCCACAAACTGTAGCAGGTGCATTTAGAAGTTCAAATGAAAGCACAACAACCCTAGTAGCCTGATATTCAATAGGCACATTAGGATAGAAGTCATAAGTACTAAGAGTACGTACACCATTTTCGTCAGAAGTTAGATCCCT'}\n"
     ]
    }
   ],
   "source": [
    "another_iteration = next(it)\n",
    "pprint(another_iteration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">2591237:ncbi:1\t2591237:ncbi:1-20100\t26865\t-\n",
      "TTGCGCCTCCTTCACGATGTCCACACACTTAATGGCAACATTGTCAGTAAGTTTTAAATAACCAGTAAACTGGTTAACTG ...\n",
      "TTGCGCCTCCTTCACGATGTCCACACACTTAATGGCAACATTGTCAGTAAGTTTTAAATAACCAGTAAACTGGTTAACTG ...\n",
      "\n",
      ">2591237:ncbi:1\t2591237:ncbi:1-20099\t7219\t-\n",
      "TAATTTAGGTCCACAAACTGTAGCAGGTGCATTTAGAAGTTCAAATGAAAGCACAACAACCCTAGTAGCCTGATATTCAA ...\n",
      "TAATTTAGGTCCACAAACTGTAGCAGGTGCATTTAGAAGTTCAAATGAAAGCACAACAACCCTAGTAGCCTGATATTCAA ...\n",
      "\n",
      ">2591237:ncbi:1\t2591237:ncbi:1-20098\t25514\t-\n",
      "CCACAAAGTGCTCCTCAGATGTCTTTGATGACGAAGTGAGGTATCCATTATATGTAGTAACAGCATCTGGTGATGATACT ...\n",
      "CCACAAAGTGCTCCTCAGATGTCTTTGATGACGAAGTGAGGTATCCATTATATGTAGTAACAGCAGCTGGTGATGATACT ...\n",
      "\n",
      ">2591237:ncbi:1\t2591237:ncbi:1-20097\t17747\t-\n",
      "ATGTAAAAGTGTTACCATCACAAGTGTTCTTGTAGGTACCATAATCAGGGACAACAACCATAAGTTTGGCTGCTGTAGTC ...\n",
      "ATGTAAAAGTGTTACCATCACAAGTGTTCTTGTAGGTACCATAATCAGGGACAACAACCATAAGTTTGGCTGCTGTAGTC ...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "it.reset_iterator()\n",
    "for i, d in enumerate(it):\n",
    "    print(d['definition line'])\n",
    "    print(d['ref_seq_aligned'][:80], '...')\n",
    "    print(d['read_seq_aligned'][:80], '...\\n')\n",
    "    if i >= 3: break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once instantiated, the `AlnFileReader` iterator gives access to the file's header information through `header` instance attribute. It is a dictionary with two keys: `'command'` and `'reference sequences'`:\n",
    "\n",
    "```\n",
    "    {'command':             'art-illumina command used to create the reads',\n",
    "     'reference sequences': ['@SQ metadata on reference sequence 1 used for the reads',\n",
    "                             '@SQ metadata on reference sequence 2 used for the reads', \n",
    "                             ...\n",
    "                            ]\n",
    "    }\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/art_illumina -i /home/vtec/projects/bio/metagentools/nbs-dev/data_dev/cov_virus_sequence_one.fa -ss HS25 -l 150 -f 100 -o /home/vtec/projects/bio/metagentools/nbs-dev/data_dev/single_1seq_150bp/single_1seq_150bp -rs 1704169422\n"
     ]
    }
   ],
   "source": [
    "print(it.header['command'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@SQ\t2591237:ncbi:1 [MK211378]\t2591237\tncbi\t1 [MK211378] 2591237\tCoronavirus BtRs-BetaCoV/YN2018D\t\tscientific name\t30213\n"
     ]
    }
   ],
   "source": [
    "for seq_info in it.header['reference sequences']:\n",
    "    print(seq_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **read definition line** includes key metadata, which need to be parsed using the appropriate parsing rule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/vtecftwy/metagentools/blob/main/metagentools/core.py#LNone){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### TextFileBaseReader.parse_text\n",
       "\n",
       ">      TextFileBaseReader.parse_text (txt:str, pattern:str=None,\n",
       ">                                     keys:list[str]=None)\n",
       "\n",
       "Parse text using regex pattern and key. Return a metadata dictionary\n",
       "\n",
       "The passed text is parsed using the regex pattern. The method return a dictionary in the format:\n",
       "    {\n",
       "        'key_1': 'metadata 1',\n",
       "        'key_2': 'metadata 2',\n",
       "        ...\n",
       "    }\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| txt | str |  | text to parse |\n",
       "| pattern | str | None | If None, uses standard regex pattern to extract metadata, otherwise, uses passed regex |\n",
       "| keys | list | None | If None, uses standard regex list of keys, otherwise, uses passed list of keys (str) |\n",
       "| **Returns** | **dict** |  | **parsed metadata in key/value format** |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/vtecftwy/metagentools/blob/main/metagentools/core.py#LNone){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### TextFileBaseReader.parse_text\n",
       "\n",
       ">      TextFileBaseReader.parse_text (txt:str, pattern:str=None,\n",
       ">                                     keys:list[str]=None)\n",
       "\n",
       "Parse text using regex pattern and key. Return a metadata dictionary\n",
       "\n",
       "The passed text is parsed using the regex pattern. The method return a dictionary in the format:\n",
       "    {\n",
       "        'key_1': 'metadata 1',\n",
       "        'key_2': 'metadata 2',\n",
       "        ...\n",
       "    }\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| txt | str |  | text to parse |\n",
       "| pattern | str | None | If None, uses standard regex pattern to extract metadata, otherwise, uses passed regex |\n",
       "| keys | list | None | If None, uses standard regex list of keys, otherwise, uses passed list of keys (str) |\n",
       "| **Returns** | **dict** |  | **parsed metadata in key/value format** |"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(AlnFileReader.parse_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "pattern, keys = it.re_pattern, it.re_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'aln_start_pos': '26865',\n",
       " 'readid': '2591237:ncbi:1-20100',\n",
       " 'readnb': '20100',\n",
       " 'refseq_strand': '-',\n",
       " 'refseqid': '2591237:ncbi:1',\n",
       " 'refseqnb': '1',\n",
       " 'refsource': 'ncbi',\n",
       " 'reftaxonomyid': '2591237'}"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "it.parse_text(dfn_line, pattern, keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/vtecftwy/metagentools/blob/main/metagentools/cnn_virus/data.py#L230){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### AlnFileReader.parse_definition_line_with_position\n",
       "\n",
       ">      AlnFileReader.parse_definition_line_with_position (dfn_line:str)\n",
       "\n",
       "Parse definition line and adds relative position\n",
       "\n",
       "|    | **Type** | **Details** |\n",
       "| -- | -------- | ----------- |\n",
       "| dfn_line | str | fefinition line string to be parsed |\n",
       "| **Returns** | **dict** | **parsed metadata in key/value format + relative position of the read** |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/vtecftwy/metagentools/blob/main/metagentools/cnn_virus/data.py#L230){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### AlnFileReader.parse_definition_line_with_position\n",
       "\n",
       ">      AlnFileReader.parse_definition_line_with_position (dfn_line:str)\n",
       "\n",
       "Parse definition line and adds relative position\n",
       "\n",
       "|    | **Type** | **Details** |\n",
       "| -- | -------- | ----------- |\n",
       "| dfn_line | str | fefinition line string to be parsed |\n",
       "| **Returns** | **dict** | **parsed metadata in key/value format + relative position of the read** |"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(AlnFileReader.parse_definition_line_with_position)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upon instance creation, `AlnFileReader` automatically checks the `default_parsing_rules.json` file for a workable rule among saved rules. Saved rules include the rule for ART Illumina ALN files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'aln_art_illumina'"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "it.re_rule_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is therefore not required to pass a specific `pattern` and `keys` parameter.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'aln_start_pos': '26865',\n",
       " 'readid': '2591237:ncbi:1-20100',\n",
       " 'readnb': '20100',\n",
       " 'refseq_strand': '-',\n",
       " 'refseqid': '2591237:ncbi:1',\n",
       " 'refseqnb': '1',\n",
       " 'refsource': 'ncbi',\n",
       " 'reftaxonomyid': '2591237'}"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "it.parse_text(dfn_line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ART Ilumina ALN files definition lines consist of:\n",
    "\n",
    "- The **read** ID: `readid`, e.g. `2591237:ncbi:1-20100`\n",
    "- the **read** number (order in the file): `readnb`, e.g. `20100`\n",
    "- The **read** start position in the reference sequence: `aln_start_pos`, e.g. `23878`\n",
    "- The **reference sequence** ID: `readid`, e.g. `2591237:ncbi:1-20100`\n",
    "- The **reference sequence** number: `refseqnb`, e.g. `1`\n",
    "- The **reference sequence** source: `refsource`, e.g. `ncbi`\n",
    "- The **reference sequence** taxonomy: `reftaxonomyid`, e.g. `2591237`\n",
    "- The **reference sequence** strand:  `refseq_strand` wich is either `+` or  `-`,\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/vtecftwy/metagentools/blob/main/metagentools/cnn_virus/data.py#L239){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### AlnFileReader.parse_file\n",
       "\n",
       ">      AlnFileReader.parse_file (add_ref_seq_aligned:bool=False,\n",
       ">                                add_read_seq_aligned:bool=False)\n",
       "\n",
       "Read ALN file, return a dict w/ alignment info for each read and optionaly aligned reference sequence & read\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| add_ref_seq_aligned | bool | False | Add the reference sequence aligned to the parsed dictionary when True |\n",
       "| add_read_seq_aligned | bool | False | Add the read sequence aligned to the parsed dictionary when True |\n",
       "| **Returns** | **dict[str]** |  |  |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/vtecftwy/metagentools/blob/main/metagentools/cnn_virus/data.py#L239){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### AlnFileReader.parse_file\n",
       "\n",
       ">      AlnFileReader.parse_file (add_ref_seq_aligned:bool=False,\n",
       ">                                add_read_seq_aligned:bool=False)\n",
       "\n",
       "Read ALN file, return a dict w/ alignment info for each read and optionaly aligned reference sequence & read\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| add_ref_seq_aligned | bool | False | Add the reference sequence aligned to the parsed dictionary when True |\n",
       "| add_read_seq_aligned | bool | False | Add the read sequence aligned to the parsed dictionary when True |\n",
       "| **Returns** | **dict[str]** |  |  |"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(AlnFileReader.parse_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2591237:ncbi:1-20100\n",
      "{'aln_start_pos': '26865',\n",
      " 'readid': '2591237:ncbi:1-20100',\n",
      " 'readnb': '20100',\n",
      " 'refseq_strand': '-',\n",
      " 'refseqid': '2591237:ncbi:1',\n",
      " 'refseqnb': '1',\n",
      " 'refsource': 'ncbi',\n",
      " 'reftaxonomyid': '2591237'}\n",
      "2591237:ncbi:1-20099\n",
      "{'aln_start_pos': '7219',\n",
      " 'readid': '2591237:ncbi:1-20099',\n",
      " 'readnb': '20099',\n",
      " 'refseq_strand': '-',\n",
      " 'refseqid': '2591237:ncbi:1',\n",
      " 'refseqnb': '1',\n",
      " 'refsource': 'ncbi',\n",
      " 'reftaxonomyid': '2591237'}\n",
      "2591237:ncbi:1-20098\n",
      "{'aln_start_pos': '25514',\n",
      " 'readid': '2591237:ncbi:1-20098',\n",
      " 'readnb': '20098',\n",
      " 'refseq_strand': '-',\n",
      " 'refseqid': '2591237:ncbi:1',\n",
      " 'refseqnb': '1',\n",
      " 'refsource': 'ncbi',\n",
      " 'reftaxonomyid': '2591237'}\n",
      "2591237:ncbi:1-20097\n",
      "{'aln_start_pos': '17747',\n",
      " 'readid': '2591237:ncbi:1-20097',\n",
      " 'readnb': '20097',\n",
      " 'refseq_strand': '-',\n",
      " 'refseqid': '2591237:ncbi:1',\n",
      " 'refseqnb': '1',\n",
      " 'refsource': 'ncbi',\n",
      " 'reftaxonomyid': '2591237'}\n",
      "2591237:ncbi:1-20096\n",
      "{'aln_start_pos': '19819',\n",
      " 'readid': '2591237:ncbi:1-20096',\n",
      " 'readnb': '20096',\n",
      " 'refseq_strand': '+',\n",
      " 'refseqid': '2591237:ncbi:1',\n",
      " 'refseqnb': '1',\n",
      " 'refsource': 'ncbi',\n",
      " 'reftaxonomyid': '2591237'}\n"
     ]
    }
   ],
   "source": [
    "parsed = it.parse_file()\n",
    "\n",
    "for i, (k, v) in enumerate(parsed.items()):\n",
    "    print(k)\n",
    "    pprint(v)\n",
    "    if i > 3: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/vtecftwy/metagentools/blob/main/metagentools/cnn_virus/data.py#L259){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### AlnFileReader.parse_header_reference_sequences\n",
       "\n",
       ">      AlnFileReader.parse_header_reference_sequences (pattern:str|None=None,\n",
       ">                                                      keys:list[str]|None=None)\n",
       "\n",
       "Extract metadata from all header reference sequences\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| pattern | str \\| None | None | regex pattern to apply to parse the reference sequence info |\n",
       "| keys | list[str] \\| None | None | list of keys: keys are both regex match group names and corresponding output dict keys |\n",
       "| **Returns** | **dict[str]** |  | **parsed metadata in key/value format** |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/vtecftwy/metagentools/blob/main/metagentools/cnn_virus/data.py#L259){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### AlnFileReader.parse_header_reference_sequences\n",
       "\n",
       ">      AlnFileReader.parse_header_reference_sequences (pattern:str|None=None,\n",
       ">                                                      keys:list[str]|None=None)\n",
       "\n",
       "Extract metadata from all header reference sequences\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| pattern | str \\| None | None | regex pattern to apply to parse the reference sequence info |\n",
       "| keys | list[str] \\| None | None | list of keys: keys are both regex match group names and corresponding output dict keys |\n",
       "| **Returns** | **dict[str]** |  | **parsed metadata in key/value format** |"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(AlnFileReader.parse_header_reference_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'2591237:ncbi:1': {'refseq_accession': 'MK211378',\n",
      "                    'refseq_length': '30213',\n",
      "                    'refseqid': '2591237:ncbi:1',\n",
      "                    'refseqnb': '1',\n",
      "                    'refsource': 'ncbi',\n",
      "                    'reftaxonomyid': '2591237',\n",
      "                    'species': 'Coronavirus BtRs-BetaCoV/YN2018D  scientific '\n",
      "                               'name'}}\n"
     ]
    }
   ],
   "source": [
    "pprint(it.parse_header_reference_sequences())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/vtecftwy/metagentools/blob/main/metagentools/cnn_virus/data.py#L274){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### AlnFileReader.set_header_parsing_rules\n",
       "\n",
       ">      AlnFileReader.set_header_parsing_rules (pattern:str|bool=None,\n",
       ">                                              keys:list[str]=None,\n",
       ">                                              verbose:bool=False)\n",
       "\n",
       "Set the regex parsing rule for reference sequence in ALN header.\n",
       "\n",
       "Updates 3 class attributes: `re_header_rule_name`, `re_header_pattern`, `re_header_keys`\n",
       "\n",
       "TODO: refactor this and the method in Core: to use a single function for the common part and a parameter for the text_to_parse\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| pattern | str \\| bool | None | regex pattern to apply to parse the text, search in parsing rules json if None |\n",
       "| keys | list[str] | None | list of keys/group for regex, search in parsing rules json if None |\n",
       "| verbose | bool | False | when True, provides information on each rule |\n",
       "| **Returns** | **None** |  |  |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/vtecftwy/metagentools/blob/main/metagentools/cnn_virus/data.py#L274){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### AlnFileReader.set_header_parsing_rules\n",
       "\n",
       ">      AlnFileReader.set_header_parsing_rules (pattern:str|bool=None,\n",
       ">                                              keys:list[str]=None,\n",
       ">                                              verbose:bool=False)\n",
       "\n",
       "Set the regex parsing rule for reference sequence in ALN header.\n",
       "\n",
       "Updates 3 class attributes: `re_header_rule_name`, `re_header_pattern`, `re_header_keys`\n",
       "\n",
       "TODO: refactor this and the method in Core: to use a single function for the common part and a parameter for the text_to_parse\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| pattern | str \\| bool | None | regex pattern to apply to parse the text, search in parsing rules json if None |\n",
       "| keys | list[str] | None | list of keys/group for regex, search in parsing rules json if None |\n",
       "| verbose | bool | False | when True, provides information on each rule |\n",
       "| **Returns** | **None** |  |  |"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(AlnFileReader.set_header_parsing_rules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "Rule <fasta_cov_ncbi> generated an error\n",
      "No match on this line\n",
      "--------------------------------------------------------------------------------\n",
      "Rule <fasta_rhinolophus_ferrumequinum> generated an error\n",
      "No match on this line\n",
      "--------------------------------------------------------------------------------\n",
      "Rule <fastq_art_illumina> generated an error\n",
      "No match on this line\n",
      "--------------------------------------------------------------------------------\n",
      "Rule <aln_art_illumina> generated an error\n",
      "No match on this line\n",
      "--------------------------------------------------------------------------------\n",
      "Rule <aln_art_illumina-refseq> generated 7 matches\n",
      "--------------------------------------------------------------------------------\n",
      "^@SQ[\\t\\s]*(?P<refseqid>(?P<reftaxonomyid>\\d*):(?P<refsource>\\w*):(?P<refseqnb>\\d*))[\\t\\s]*\\[(?P<refseq_accession>[\\d\\w]*)\\][\\t\\s]*(?P=reftaxonomyid)[\\s\\t]*(?P=refsource)[\\s\\t]*(?P=refseqnb)[\\s\\t]*\\[(?P=refseq_accession)\\][\\s\\t]*(?P=reftaxonomyid)[\\s\\t]*(?P<species>\\w[\\w\\d\\/\\s\\-\\.]*)[\\s\\t](?P<refseq_length>\\d*)$\n",
      "['refseqid', 'reftaxonomyid', 'refsource', 'refseqnb', 'refseq_accession', 'species', 'refseq_length']\n",
      "--------------------------------------------------------------------------------\n",
      "Selected rule with most matches: aln_art_illumina-refseq\n"
     ]
    }
   ],
   "source": [
    "it.set_header_parsing_rules(verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aln_art_illumina-refseq\n",
      "^@SQ[\\t\\s]*(?P<refseqid>(?P<reftaxonomyid>\\d*):(?P<refsource>\\w*):(?P<refseqnb>\\d*))[\\t\\s]*\\[(?P<refseq_accession>[\\d\\w]*)\\][\\t\\s]*(?P=reftaxonomyid)[\\s\\t]*(?P=refsource)[\\s\\t]*(?P=refseqnb)[\\s\\t]*\\[(?P=refseq_accession)\\][\\s\\t]*(?P=reftaxonomyid)[\\s\\t]*(?P<species>\\w[\\w\\d\\/\\s\\-\\.]*)[\\s\\t](?P<refseq_length>\\d*)$\n",
      "['refseqid', 'reftaxonomyid', 'refsource', 'refseqnb', 'refseq_accession', 'species', 'refseq_length']\n"
     ]
    }
   ],
   "source": [
    "print(it.re_header_rule_name)\n",
    "print(it.re_header_pattern)\n",
    "print(it.re_header_keys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sequence and reads are provided in various formats (text for original data, fastq + aln for simulated reads) and the model expects a specific format for training, validation and testing datasets.\n",
    "\n",
    "The following functions allow to build the datasets in the format expected by the model from the raw data available.\n",
    "\n",
    "In addition, text based dataset are not efficient, especially for training. Additional functions allow to save and parse dataset in TFRecord format.\n",
    "\n",
    "There are two pipelines for building inference datasets:\n",
    "- via a text inference dataset, in the same format as the original paper's data\n",
    "- via a TFRecord inference dataset for faster operations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text based inference datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this pipeline, the steps are:\n",
    "\n",
    "1. Create a text inference file and a metadata file from FASTQ and ALN with `create_infer_ds_from_fastq`\n",
    "2. Create a `tf.data.TextLineDataset` from the text inference dataset\n",
    "3. Transform it into an inference/training dataset with `.map` and `strings_to_tensors)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def create_infer_ds_from_fastq(\n",
    "    p2fastq: str|Path,             # Path to the fastq file (aln file path is inferred)\n",
    "    output_dir:str|Path|None=None, # Path to directory where ds file will be saved\n",
    "    overwrite_ds:bool=False,       # If True, overwrite existing ds file. If False, error is raised if ds file exists\n",
    "    nsamples:int|None=None         # Used to limit the number of reads to use for inference, use all if None\n",
    ")-> (Path, Path, pd.DataFrame):    # Paths to dataset file, path to metadata file, dataframe with metadata\n",
    "    \"\"\"Build an inference dataset file as required by the CNN Virus model from a simreads fastq (ART format).\n",
    "    \n",
    "    Also extract the fastq read sequence metadata, saves it in a metadata file and returns them as a DataFrame\n",
    "    \"\"\"\n",
    "    fastq = FastqFileReader(p2fastq)\n",
    "    aln = AlnFileReader(p2fastq.parent / f\"{p2fastq.stem}.aln\")\n",
    "    \n",
    "    if output_dir is None:\n",
    "        p2outdir = Path()\n",
    "    else:\n",
    "        validate_path(output_dir, path_type='dir', raise_error=True)\n",
    "        p2outdir = output_dir if isinstance(output_dir, Path) else Path(output_dir)\n",
    "    \n",
    "    p2dataset = p2outdir / f\"{p2fastq.stem}_ds\"\n",
    "    p2metadata = p2outdir / f\"{p2fastq.stem}_metadata.csv\"\n",
    "    \n",
    "    if p2dataset.is_file():\n",
    "        if overwrite_ds: \n",
    "            p2dataset.unlink()\n",
    "            if p2metadata.is_file(): p2metadata.unlink()\n",
    "        else:\n",
    "            raise ValueError(f\"{p2dataset.name} already exists in {p2dataset.absolute()}\")\n",
    "    p2dataset.touch()\n",
    "    p2metadata.touch()\n",
    "    \n",
    "    read_ids = []\n",
    "    read_refseqs = []\n",
    "    read_start_pos = []\n",
    "    read_strand = []\n",
    "    \n",
    "    with open(p2dataset, 'a') as fp:\n",
    "        i = 1\n",
    "        for fastq_chunk, aln_chunk in tqdm(zip(fastq, aln)):\n",
    "            seq = fastq_chunk['sequence']\n",
    "            \n",
    "            aln_meta = aln.parse_text(aln_chunk['definition line'])\n",
    "            read_ids.append(aln_meta['readid'])\n",
    "            read_refseqs.append(aln_meta['refseqid'])\n",
    "            read_start_pos.append(aln_meta['aln_start_pos'])\n",
    "            read_strand.append(aln_meta['refseq_strand'])\n",
    "\n",
    "            fp.write(f\"{seq}\\t{0}\\t{0}\\n\")\n",
    "\n",
    "            i += 1\n",
    "            if nsamples:\n",
    "                if i > nsamples: break\n",
    "                    \n",
    "    print(f\"Dataset with {i-1:,d} reads\")\n",
    "\n",
    "    metadata = np.array(list(zip(read_ids, read_refseqs, read_start_pos, read_strand)))\n",
    "    metadata = pd.DataFrame(data={\n",
    "                'read_ids': read_ids,\n",
    "                'read_refseqs': read_refseqs,\n",
    "                'read_start_pos': read_start_pos,\n",
    "                'read_strand': read_strand})\n",
    "    metadata.to_csv(p2metadata, index=True)\n",
    "    \n",
    "    return p2dataset, p2metadata, metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/vtecftwy/metagentools/blob/main/metagentools/cnn_virus/data.py#L356){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### create_infer_ds_from_fastq\n",
       "\n",
       ">      create_infer_ds_from_fastq (p2fastq:str|pathlib.Path,\n",
       ">                                  output_dir:str|pathlib.Path|None=None,\n",
       ">                                  overwrite_ds:bool=False,\n",
       ">                                  nsamples:int|None=None)\n",
       "\n",
       "Build an inference dataset file as required by the CNN Virus model from a simreads fastq (ART format).\n",
       "\n",
       "Also extract the fastq read sequence metadata, saves it in a metadata file and returns them as a DataFrame\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| p2fastq | str \\| Path |  | Path to the fastq file (aln file path is inferred) |\n",
       "| output_dir | str \\| Path \\| None | None | Path to directory where ds file will be saved |\n",
       "| overwrite_ds | bool | False | If True, overwrite existing ds file. If False, error is raised if ds file exists |\n",
       "| nsamples | int \\| None | None | Used to limit the number of reads to use for inference, use all if None |\n",
       "| **Returns** | **(Path, Path, pd.DataFrame)** |  | **Paths to dataset file, path to metadata file, dataframe with metadata** |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/vtecftwy/metagentools/blob/main/metagentools/cnn_virus/data.py#L356){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### create_infer_ds_from_fastq\n",
       "\n",
       ">      create_infer_ds_from_fastq (p2fastq:str|pathlib.Path,\n",
       ">                                  output_dir:str|pathlib.Path|None=None,\n",
       ">                                  overwrite_ds:bool=False,\n",
       ">                                  nsamples:int|None=None)\n",
       "\n",
       "Build an inference dataset file as required by the CNN Virus model from a simreads fastq (ART format).\n",
       "\n",
       "Also extract the fastq read sequence metadata, saves it in a metadata file and returns them as a DataFrame\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| p2fastq | str \\| Path |  | Path to the fastq file (aln file path is inferred) |\n",
       "| output_dir | str \\| Path \\| None | None | Path to directory where ds file will be saved |\n",
       "| overwrite_ds | bool | False | If True, overwrite existing ds file. If False, error is raised if ds file exists |\n",
       "| nsamples | int \\| None | None | Used to limit the number of reads to use for inference, use all if None |\n",
       "| **Returns** | **(Path, Path, pd.DataFrame)** |  | **Paths to dataset file, path to metadata file, dataframe with metadata** |"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(create_infer_ds_from_fastq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f56ed29c1259470ab47b38c8c6e2af49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset with 100 reads\n"
     ]
    }
   ],
   "source": [
    "path2ds, path2meta, meta = create_infer_ds_from_fastq(\n",
    "    p2fastq=p2fastq, \n",
    "    output_dir=Path('data_dev'),\n",
    "    overwrite_ds=True, \n",
    "    nsamples=100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FASTAQ file name: /home/vtec/projects/bio/metagentools/nbs-dev/data_dev/single_1seq_150bp/single_1seq_150bp.fq\n",
      "Path to dataset:  /home/vtec/projects/bio/metagentools/nbs-dev/data_dev/single_1seq_150bp_ds \n",
      "Path to metadata: /home/vtec/projects/bio/metagentools/nbs-dev/data_dev/single_1seq_150bp_metadata.csv\n"
     ]
    }
   ],
   "source": [
    "print(f\"FASTAQ file name: {p2fastq.absolute()}\")\n",
    "print(f\"Path to dataset:  {path2ds.absolute()} \\nPath to metadata: {path2meta.absolute()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5-line chunk 1\n",
      "TTGCGCCTCCTTCACGATGTCCACACACTTAATGGCAACATTGTCAGTAAGTTTTAAATAACCAGTAAACTGGTTAACTGGTTCCTCAGGTGTTGGTTCTGATTCTAGTTCATGCTCCGATAATTCGGTAGCATCACCAAGCCAGTCCTC\t0\t0\n",
      "TAATTTAGGTCCACAAACTGTAGCAGGTGCATTTAGAAGTTCAAATGAAAGCACAACAACCCTAGTAGCCTGATATTCAATAGGCACATTAGGATAGAAGTCATAAGTACTAAGAGTACGTACACCATTTTCGTCAGAAGTTAGATCCCT\t0\t0\n",
      "CCACAAAGTGCTCCTCAGATGTCTTTGATGACGAAGTGAGGTATCCATTATATGTAGTAACAGCAGCTGGTGATGATACTGACACTACGGCAGGAGCTTTAAGAGAACGCATACAGCGCGCAGCCTCTTCAAGATTAAAACCATGTGTCA\t0\t0\n",
      "ATGTAAAAGTGTTACCATCACAAGTGTTCTTGTAGGTACCATAATCAGGGACAACAACCATAAGTTTGGCTGCTGTAGTCAATGGTATGATGTTGAGTGGAACACAACCATCACGCGCATTGTTGATAATGTTGTTAAGTGCATCATTAT\t0\t0\n",
      "AGAAGCACCAGCACATATGTCAACAATAGGTGTCTGCACAATGACTGACATTGCTAAGAAACCTACTGAGAGTGCTTGTTCCTCGCTTACTGTCTTATTTGATGGTAGAGTGGAAGGACAGGTAGACCTTTTTAGAAATGCCCGTAATGG\t0\t0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "TextFileBaseReader(path2ds, nlines=5).print_first_chunks(nchunks=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>read_ids</th>\n",
       "      <th>read_refseqs</th>\n",
       "      <th>read_start_pos</th>\n",
       "      <th>read_strand</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2591237:ncbi:1-20100</td>\n",
       "      <td>2591237:ncbi:1</td>\n",
       "      <td>26865</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2591237:ncbi:1-20099</td>\n",
       "      <td>2591237:ncbi:1</td>\n",
       "      <td>7219</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2591237:ncbi:1-20098</td>\n",
       "      <td>2591237:ncbi:1</td>\n",
       "      <td>25514</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2591237:ncbi:1-20097</td>\n",
       "      <td>2591237:ncbi:1</td>\n",
       "      <td>17747</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2591237:ncbi:1-20096</td>\n",
       "      <td>2591237:ncbi:1</td>\n",
       "      <td>19819</td>\n",
       "      <td>+</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               read_ids    read_refseqs read_start_pos read_strand\n",
       "0  2591237:ncbi:1-20100  2591237:ncbi:1          26865           -\n",
       "1  2591237:ncbi:1-20099  2591237:ncbi:1           7219           -\n",
       "2  2591237:ncbi:1-20098  2591237:ncbi:1          25514           -\n",
       "3  2591237:ncbi:1-20097  2591237:ncbi:1          17747           -\n",
       "4  2591237:ncbi:1-20096  2591237:ncbi:1          19819           +"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def strings_to_tensors(\n",
    "    b: tf.Tensor        # batch of strings \n",
    "    ):\n",
    "    \"\"\"Function converting a batch of bp strings into three tensors: (x_seqs, (y_labels, y_pos))\"\"\"\n",
    "    \n",
    "    # Split the string in three : returns a ragged tensor which needs to be converted into a normal tensor using .to_tensor()\n",
    "    t = tf.strings.split(b, '\\t').to_tensor(default_value = '', shape=[None, 3])\n",
    "\n",
    "    # Split each sequence string into a list of single base strings:\n",
    "    # 'TCAAAATAATCA' -> ['T','C','A','A','A','A','T','A','A','T','C','A']\n",
    "    seqs = tf.strings.bytes_split(t[:, 0]).to_tensor(shape=(None, 50))\n",
    "\n",
    "\n",
    "    # BHE sequences\n",
    "    # Each base letter (A, C, G, T, N) is replaced by a OHE vector\n",
    "    #     \"A\" converted into [1,0,0,0,0]\n",
    "    #     \"C\" converted into [0,1,0,0,0]\n",
    "    #     \"G\" converted into [0,0,1,0,0]\n",
    "    #     \"T\" converted into [0,0,0,1,0]\n",
    "    #     \"N\" converted into [0,0,0,0,1]\n",
    "    # \n",
    "    # Technical Notes:\n",
    "    # a. The batch of sequence `seqs` has a shape (batch_size, 50) after splitting each byte. \n",
    "    #    Must flatten it first, then apply the transform on each base, then reshape to original shape\n",
    "    # b. We need to map each letter to one vector/tensor. \n",
    "    #    1. Cast bytes seqs into integer sequence (uint8 to work byte by byte)\n",
    "    #    2. For each base letter (A, C, G, T, N) create one tensor (batch_size, 50) (seqs_A, _C, _G, _T, _N)\n",
    "    #    3. Value is 1 if it is the base in the sequence, otherwise 0\n",
    "    #    4. Concatenate these 5 tensors into a tensor of shape (batch_size, 50, 5)\n",
    " \n",
    "    seqs_uint8 = tf.io.decode_raw(seqs, out_type=tf.uint8)\n",
    "    # note: tf.io.decode_raw adds one dimension at the end in the process\n",
    "    #       [b'C', b'A', b'T'] will return [[67], [65], [84]] and not [67, 65, 84]\n",
    "    #       this is actually what we want to contatenate the values for each base letter\n",
    "\n",
    "    A, C, G, T, N = 65, 67, 71, 84, 78\n",
    "\n",
    "    seqs_A = tf.cast(seqs_uint8 == A, tf.float32)\n",
    "    seqs_C = tf.cast(seqs_uint8 == C, tf.float32)\n",
    "    seqs_G = tf.cast(seqs_uint8 == G, tf.float32)\n",
    "    seqs_T = tf.cast(seqs_uint8 == T, tf.float32)\n",
    "    seqs_N = tf.cast(seqs_uint8 == N , tf.float32)\n",
    "\n",
    "    x_seqs = tf.concat([seqs_A, seqs_C, seqs_G, seqs_T, seqs_N], axis=2)\n",
    "\n",
    "    # OHE labels\n",
    "    n_labels = 187\n",
    "    y_labels = tf.strings.to_number(t[:, 1], out_type=tf.int32)\n",
    "    y_labels = tf.gather(tf.eye(n_labels), y_labels)\n",
    "\n",
    "    # OHE positions\n",
    "    n_pos = 10\n",
    "    y_pos = tf.strings.to_number(t[:, 2], out_type=tf.int32)\n",
    "    y_pos= tf.gather(tf.eye(n_pos), y_pos)\n",
    "\n",
    "    return (x_seqs, (y_labels, y_pos))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TFRecord based inference datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this pipeline, the steps are:\n",
    "\n",
    "1. Go from FASTQ and ALN to a RFRecord file and a metadata file with `tfrecord_from_fastq` or `tfrecord_from_text`\n",
    "2. Create a `tf.data.TFRecordDataset` from the saved TFRecord file\n",
    "3. Transform it into an inference/training dataset with `.map` and `tfr_to_tensors`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def _bytes_feature(value):\n",
    "    \"\"\"Returns a bytes_list from a string / byte.\"\"\"\n",
    "    if isinstance(value, type(tf.constant(0))): # if value ist tensor\n",
    "        value = value.numpy() # get value of tensor\n",
    "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
    "\n",
    "def _float_feature(value):\n",
    "  \"\"\"Returns a floast_list from a float / double.\"\"\"\n",
    "  return tf.train.Feature(float_list=tf.train.FloatList(value=[value]))\n",
    "\n",
    "def _int64_feature(value):\n",
    "  \"\"\"Returns an int64_list from a bool / enum / int / uint.\"\"\"\n",
    "  return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\n",
    "\n",
    "def _serialize_array(array):\n",
    "  array = serialize_tensor(array)\n",
    "  return array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def _base_hot_encode(\n",
    "    line: str        # one string (one line in text dataset)\n",
    "    ):\n",
    "    \"\"\"Convert a line from text dataset into three tensors: read sequence (BHE), virus label and position\"\"\"\n",
    "    \n",
    "    # Split the line (string) in three : read, label, position\n",
    "    t = tf.strings.split(line.replace('\\n', ''), '\\t')\n",
    "\n",
    "    # Split the sequence string into a list of single base strings:\n",
    "    # 'TCAAAATAATCA' -> ['T','C','A','A','A','A','T','A','A','T','C','A']\n",
    "    read = tf.strings.bytes_split(t[0])\n",
    "\n",
    "    # Base Hot Encode sequences (BHE)\n",
    "    # Each base letter (A, C, G, T, N) is replaced by a OHE vector\n",
    "    #     \"A\" converted into [1,0,0,0,0]\n",
    "    #     \"C\" converted into [0,1,0,0,0]\n",
    "    #     \"G\" converted into [0,0,1,0,0]\n",
    "    #     \"T\" converted into [0,0,0,1,0]\n",
    "    #     \"N\" converted into [0,0,0,0,1]\n",
    "    \n",
    "    # Decode the base letters A, C, ... into their ASCII code for easy conversion into BHE\n",
    "    # ASCII code for A, C, G, T and N:\n",
    "    A, C, G, T, N = 65, 67, 71, 84, 78\n",
    "    read_uint8 = tf.io.decode_raw(read, out_type=tf.uint8)\n",
    "\n",
    "    # Technical Notes: \n",
    "    #   tf.io.decode_raw adds one dimension at the end in the process\n",
    "    #   [b'C', b'A', b'T'] will return [[67], [65], [84]] and not [67, 65, 84]\n",
    "    #   this is actually what we want to contatenate the values for each base letter\n",
    "    read_A = tf.cast(read_uint8 == A, tf.float32)\n",
    "    read_C = tf.cast(read_uint8 == C, tf.float32)\n",
    "    read_G = tf.cast(read_uint8 == G, tf.float32)\n",
    "    read_T = tf.cast(read_uint8 == T, tf.float32)\n",
    "    read_N = tf.cast(read_uint8 == N , tf.float32)\n",
    "    x_reads = tf.concat([read_A, read_C, read_G, read_T, read_N], axis=1)\n",
    "\n",
    "    # OHE labels\n",
    "    n_labels = 187\n",
    "    y_labels = tf.strings.to_number(t[1], out_type=tf.int32) # int32 so it can be used an index in gather\n",
    "    y_labels = tf.gather(tf.eye(n_labels, dtype=tf.float32), y_labels)\n",
    "\n",
    "    # OHE positions\n",
    "    n_pos = 10\n",
    "    y_pos = tf.strings.to_number(t[2], out_type=tf.int32) # int32 so it can be used an index in gather\n",
    "    y_pos= tf.gather(tf.eye(n_pos, dtype=tf.float32), y_pos)\n",
    "\n",
    "    return x_reads, y_labels, y_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def tfrecord_from_fastq(\n",
    "    p2fastq:Path,              # Path to the fastaq file (should be associated with a aln file)\n",
    "    p2tfrds:Path|None=None,    # Path to the TFRecord file, default creates a file in savec directory\n",
    "    overwrite:bool=False       # When True, overides any existing file, When False, raises an error\n",
    "    ) -> (Path, Path):         # Paths to the saved TFRecord file and the metadata csv file\n",
    "    \"\"\"Creates a TFRecord dataset for inference from fastq and aln files, as well as a csv metadata file\n",
    "\n",
    "    The TFRecord dataset can be used for training or prediction, using the original CNN Virus model.\n",
    "    The metadata file is a Pandas DataFrame converted into csv\n",
    "    \"\"\"\n",
    "    # Setup paths\n",
    "    if p2tfrds is None:\n",
    "        p2tfrds = ProjectFileSystem().data / 'saved/cnn_virus_datasets' / f\"{p2fastq.stem}.tfrecords\"\n",
    "    p2metadata = p2tfrds.parent / f\"{p2tfrds.stem}.metadata\"\n",
    "\n",
    "    if p2tfrds.exists():\n",
    "        if overwrite:\n",
    "            p2tfrds.unlink()\n",
    "            if p2metadata.exists(): p2metadata.unlink()\n",
    "        else: \n",
    "            raise ValueError(f\"{p2tfrds.name} already exists. To overwrite, set parameter `overwrite` to True\")\n",
    "\n",
    "    p2aln = p2fastq.parent / f\"{p2fastq.stem}.aln\"\n",
    "    assert p2aln.is_file(), f\"No ALN file associated with {fastq.name}\"\n",
    "    \n",
    "    fastq = FastqFileReader(p2fastq)\n",
    "    aln = AlnFileReader(p2aln)\n",
    "    read_ids, read_refseqs, read_start_pos, read_strand = [], [], [], []\n",
    "    writer = tf.io.TFRecordWriter(str(p2tfrds.absolute())) \n",
    "\n",
    "    for i, (fastq_element, aln_element) in tqdm(enumerate(zip(fastq, aln))):\n",
    "        # Extract read text sequence from fastq and metadata from aln files\n",
    "        seq = fastq_element['sequence']           \n",
    "        aln_meta = aln.parse_text(aln_element['definition line'])\n",
    "        read_ids.append(aln_meta['readid'])\n",
    "        read_refseqs.append(aln_meta['refseqid'])\n",
    "        read_start_pos.append(aln_meta['aln_start_pos'])\n",
    "        read_strand.append(aln_meta['refseq_strand'])\n",
    "\n",
    "        # Create and write one Example, including BHE sequence, the label and the position\n",
    "        bhe_seq, label, pos = _base_hot_encode(f\"{seq}\\t{0}\\t{0}\\n\")\n",
    "        data = {\n",
    "            'read' : _bytes_feature(_serialize_array(bhe_seq)),\n",
    "            'label' : _bytes_feature(_serialize_array(label)),\n",
    "            'pos' : _bytes_feature(_serialize_array(pos))\n",
    "        }\n",
    "        out = tf.train.Example(features=tf.train.Features(feature=data))\n",
    "        writer.write(out.SerializeToString())\n",
    "\n",
    "    writer.close()\n",
    "    print(f\"Wrote {i+1} reads to TFRecord file {p2tfrds.name}\")\n",
    "\n",
    "    metadata = np.array(list(zip(read_ids, read_refseqs, read_start_pos, read_strand)))\n",
    "    metadata = pd.DataFrame(data={\n",
    "                'read_ids': read_ids,\n",
    "                'read_refseqs': read_refseqs,\n",
    "                'read_start_pos': read_start_pos,\n",
    "                'read_strand': read_strand})\n",
    "    metadata.to_csv(p2metadata, index=True)\n",
    "    \n",
    "    return p2tfrds, p2metadata, metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def tfrecord_from_text(\n",
    "    p2ds,                      # Path to the text dataset, in the format of original CNN Virus data\n",
    "    p2tfrds:Path|None=None,    # Path to the TFRecord file, default creates a file in savec directory\n",
    "    overwrite:bool=False       # When True, overides any existing file, When False, raises an error\n",
    "    ) -> Path:                 # Path to the saved TFRecord file\n",
    "    # Setup paths\n",
    "    if p2tfrds is None:\n",
    "        p2tfrds = ProjectFileSystem().data / 'saved/cnn_virus_datasets' / f\"{p2ds.stem}.tfrecords\"\n",
    "    # p2metadata = p2tfrds.parent / f\"{p2tfrds.stem}.metadata\"\n",
    "\n",
    "    if p2tfrds.exists():\n",
    "        if overwrite:\n",
    "            p2tfrds.unlink()\n",
    "            # if p2metadata.exists(): p2metadata.unlink()\n",
    "        else: \n",
    "            raise ValueError(f\"{p2tfrds.name} already exists. To overwrite, set parameter `overwrite` to True\")\n",
    "\n",
    "    reads = TextFileBaseReader(p2ds, nlines=1)\n",
    "    writer = tf.io.TFRecordWriter(str(p2tfrds.absolute())) \n",
    "\n",
    "    for i, line in enumerate(reads):\n",
    "        # Create and write one Example, including BHE sequence, the label and the position\n",
    "        bhe_seq, label, pos = _base_hot_encode(line)\n",
    "        data = {\n",
    "            'read' : _bytes_feature(_serialize_array(bhe_seq)),\n",
    "            'label' : _bytes_feature(_serialize_array(label)),\n",
    "            'pos' : _bytes_feature(_serialize_array(pos))\n",
    "            }\n",
    "        out = tf.train.Example(features=tf.train.Features(feature=data))\n",
    "        writer.write(out.SerializeToString())\n",
    "\n",
    "    writer.close()\n",
    "    print(f\"Wrote {i+1} reads to TFRecord\")\n",
    "    return p2tfrds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def _parse_tfr_element(element):\n",
    "    # Define the underlying structure of the data (mirror the dta structure above)\n",
    "    data = {    \n",
    "        'read' : FixedLenFeature([], tf.string),\n",
    "        'label' : FixedLenFeature([], tf.string),\n",
    "        'pos' : FixedLenFeature([], tf.string) \n",
    "    }\n",
    "\n",
    "    content = tf.io.parse_single_example(element, data)\n",
    "  \n",
    "    read_bytes = content['read']\n",
    "    label_bytes = content['label']\n",
    "    pos_bytes = content['pos']\n",
    "    \n",
    "    # Parse the string tensor into a real tensors, with proper types\n",
    "    read = tf.io.parse_tensor(read_bytes, out_type=tf.float32)\n",
    "    label = tf.io.parse_tensor(label_bytes, out_type=tf.float32)\n",
    "    pos = tf.io.parse_tensor(pos_bytes, out_type=tf.float32)\n",
    "    \n",
    "    return (read, (label, pos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def get_dataset_from_tfr(\n",
    "    p2tfrds:Path   # Path to the TFRecord dataset\n",
    "    ) -> tf.data.Dataset: # dataset\n",
    "    # Create a dataset from the TFRecord file\n",
    "    dataset = tf.data.TFRecordDataset(p2tfrds)\n",
    "    # Convert the strings into the proper format using the parsing function\n",
    "    dataset = dataset.map(_parse_tfr_element)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a dataset from an existing TFRecord file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorSpec(shape=<unknown>, dtype=tf.float32, name=None),\n",
       " (TensorSpec(shape=<unknown>, dtype=tf.float32, name=None),\n",
       "  TensorSpec(shape=<unknown>, dtype=tf.float32, name=None)))"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Check how to define shape of the elements\n",
    "p2ds = Path('data_dev/single_1seq_50bp-10reads.tfrecords')\n",
    "ds = get_dataset_from_tfr(p2ds)\n",
    "ds.element_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50, 5) (187,) (10,)\n",
      "(50, 5) (187,) (10,)\n",
      "(50, 5) (187,) (10,)\n",
      "(50, 5) (187,) (10,)\n",
      "(50, 5) (187,) (10,)\n",
      "(50, 5) (187,) (10,)\n",
      "(50, 5) (187,) (10,)\n",
      "(50, 5) (187,) (10,)\n"
     ]
    }
   ],
   "source": [
    "for r, (l, p) in ds.take(8):\n",
    "    print(r.shape, l.shape, p.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can convert this dataset into a batch dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 50, 5) (2, 187) (2, 10)\n",
      "(2, 50, 5) (2, 187) (2, 10)\n",
      "(2, 50, 5) (2, 187) (2, 10)\n",
      "(2, 50, 5) (2, 187) (2, 10)\n",
      "(2, 50, 5) (2, 187) (2, 10)\n"
     ]
    }
   ],
   "source": [
    "ds_batched = ds.batch(2)\n",
    "for r, (l, p) in ds_batched.take(8):\n",
    "    print(r.shape, l.shape, p.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Original Code (Refactored)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selected classes and functions refactored from the original code, coming with the paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class DataGenerator_from_50mer(Sequence):\n",
    "    \"\"\"data generator for generating batches of data from 50-mers\"\"\"\n",
    "\n",
    "    d_nucl = {\"A\": 0,\"C\": 1,\"G\": 2,\"T\": 3,\"N\":4}\n",
    "\n",
    "    def __init__(self, f_matrix, f_labels, f_pos, batch_size=1024,n_classes=187, shuffle=True):\n",
    "        self.batch_size = batch_size\n",
    "        self.labels = f_labels\n",
    "        self.matrix = f_matrix\n",
    "        self.pos = f_pos\n",
    "        self.n_classes = n_classes\n",
    "        self.shuffle = shuffle\n",
    "        self.on_epoch_end()\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.labels) / self.batch_size))\n",
    "    def __getitem__(self, index):\n",
    "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "        X, y= self.__data_generation(indexes)\n",
    "        return X,y\n",
    "    def on_epoch_end(self):\n",
    "        self.indexes = np.arange(len(self.labels))\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)\n",
    "    def __data_generation(self, index):\n",
    "        x_train=[]\n",
    "        for i in index:\n",
    "            seq=self.matrix[i]\n",
    "            seq_list=[j for j in seq]\n",
    "            x_train.append(seq_list)\n",
    "        x_train=np.array(x_train)\n",
    "        x_tensor=np.zeros(list(x_train.shape)+[5])\n",
    "        for row in range(len(x_train)):\n",
    "            for col in range(50):\n",
    "                x_tensor[row,col,self.d_nucl[x_train[row,col]]]=1\n",
    "        y_pos=[]\n",
    "        y_label=[self.labels[i] for i in index]\n",
    "        y_label=np.array(y_label)\n",
    "        y_label=to_categorical(y_label, num_classes=self.n_classes)\n",
    "        y_pos=[self.pos[i] for i in index]\n",
    "        y_pos=np.array(y_pos)\n",
    "        y_pos=to_categorical(y_pos, num_classes=10)\n",
    "        return x_tensor,{'labels': y_label, 'pos': y_pos}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/vtecftwy/metagentools/blob/main/metagentools/cnn_virus/data.py#L470){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### DataGenerator_from_50mer\n",
       "\n",
       ">      DataGenerator_from_50mer (f_matrix, f_labels, f_pos, batch_size=1024,\n",
       ">                                n_classes=187, shuffle=True)\n",
       "\n",
       "data generator for generating batches of data from 50-mers"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/vtecftwy/metagentools/blob/main/metagentools/cnn_virus/data.py#L470){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### DataGenerator_from_50mer\n",
       "\n",
       ">      DataGenerator_from_50mer (f_matrix, f_labels, f_pos, batch_size=1024,\n",
       ">                                n_classes=187, shuffle=True)\n",
       "\n",
       "data generator for generating batches of data from 50-mers"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(DataGenerator_from_50mer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def get_learning_weights(filepath):\n",
    "    \"\"\"get different learning weights for different classes, from file\"\"\"\n",
    "    f = open(filepath,\"r\").readlines()\n",
    "    d_weights = {}\n",
    "    for i in f:\n",
    "        i = i.strip().split(\"\\t\")\n",
    "        d_weights[float(i[0])]=float(i[1])\n",
    "    return d_weights\n",
    "\n",
    "def get_params_50mer():\n",
    "    \"\"\"set default params for generating batches of 50-mer\"\"\"\n",
    "    params = {'batch_size': 1024,\n",
    "    'n_classes': 187,\n",
    "    'shuffle': True}\n",
    "    return params\n",
    "\n",
    "def get_params_150mer():\n",
    "    \"\"\" set default params for generating batches of 150-mer\"\"\"\n",
    "    params = {'batch_size': 101,\n",
    "    'n_classes': 187,\n",
    "    'shuffle': False}\n",
    "    return params\n",
    "\n",
    "def get_kmer_from_50mer(filepath, max_seqs=None):\n",
    "    \"\"\"Load data from sequence file and returns three tensors, with max nbr sequences\"\"\"\n",
    "    f_matrix=[]\n",
    "    f_labels=[]\n",
    "    f_pos=[]\n",
    "    with open(filepath, 'r') as fp:\n",
    "        i = 0\n",
    "        while True:\n",
    "            line = fp.readline()\n",
    "            i += 1\n",
    "            # EOF\n",
    "            if line == '':\n",
    "                break\n",
    "            # Reached max number of k-mers to load from file\n",
    "            elif max_seqs is not None and i > max_seqs:\n",
    "                break\n",
    "            else:\n",
    "                seq, label, pos = line.strip().split('\\t')\n",
    "                f_matrix.append(seq)\n",
    "                f_labels.append(label)\n",
    "                f_pos.append(pos)\n",
    "    return f_matrix,f_labels,f_pos\n",
    "\n",
    "def get_kmer_from_150mer(filepath, max_seqs=None):\n",
    "    \"\"\"Load data from sequence file and returns three tensors, with max nbr sequences\"\"\"\n",
    "    f_matrix=[]\n",
    "    f_labels=[]\n",
    "    f_pos=[]\n",
    "    with open(filepath,\"r\") as fp:\n",
    "        i = 0\n",
    "        while True:\n",
    "            line = fp.readline()\n",
    "            i += 1\n",
    "            # EOF\n",
    "            if line == '':\n",
    "                break\n",
    "            # Reached max number of k-mers to load from file\n",
    "            elif max_seqs is not None and i > max_seqs:\n",
    "                break\n",
    "            else:\n",
    "                seq, label, pos = line.strip().split('\\t')\n",
    "                # Split 150-mer into 101 50-mers, shifted by one nucleotide\n",
    "                for i in range(len(seq)-49):\n",
    "                    kmer=seq[i:i+50]\n",
    "                    f_matrix.append(kmer)\n",
    "                    f_labels.append(label)\n",
    "                    f_pos.append(pos)\n",
    "    return f_matrix,f_labels,f_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/vtecftwy/metagentools/blob/main/metagentools/cnn_virus/data.py#L537){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### get_kmer_from_50mer\n",
       "\n",
       ">      get_kmer_from_50mer (filepath, max_seqs=None)\n",
       "\n",
       "Load data from sequence file and returns three tensors, with max nbr sequences"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/vtecftwy/metagentools/blob/main/metagentools/cnn_virus/data.py#L537){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### get_kmer_from_50mer\n",
       "\n",
       ">      get_kmer_from_50mer (filepath, max_seqs=None)\n",
       "\n",
       "Load data from sequence file and returns three tensors, with max nbr sequences"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(get_kmer_from_50mer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/vtecftwy/metagentools/blob/main/metagentools/cnn_virus/data.py#L560){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### get_kmer_from_150mer\n",
       "\n",
       ">      get_kmer_from_150mer (filepath, max_seqs=None)\n",
       "\n",
       "Load data from sequence file and returns three tensors, with max nbr sequences"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/vtecftwy/metagentools/blob/main/metagentools/cnn_virus/data.py#L560){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### get_kmer_from_150mer\n",
       "\n",
       ">      get_kmer_from_150mer (filepath, max_seqs=None)\n",
       "\n",
       "Load data from sequence file and returns three tensors, with max nbr sequences"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(get_kmer_from_150mer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path for the training file\n",
    "filepath_50mer= Path('data_dev/50mer_ds_100_seq')\n",
    "filepath_150mer= Path('data_dev/150mer_ds_100_seq')\n",
    "assert filepath_50mer.is_file(), filepath_50mer\n",
    "assert filepath_150mer.is_file(), filepath_150mer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['AAAAAGATTTTGAGAGAGGTCGACCTGTCCTCCTAAAACGTTTACAAAAG',\n",
       " 'CATGTAACGCAGCTTAGTCCGATCGTGGCTATAATCCGTCTTTCGATTTG',\n",
       " 'AACAACATCTTGTTGATGATAACCGTCAAAGTGTTTTGGGTCTGGAGGGA',\n",
       " 'AGTACCTGGAGAGCGTTAAGAAACACAAACGGCTGGATGTAGTGCCGCGC',\n",
       " 'CCACGTCGATGAAGCTCCGACGAGAGTCGGCGCTGAGCCCGCGCACCTCC']"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f_matrix,f_labels,f_pos = get_kmer_from_50mer(filepath_50mer, max_seqs=5)\n",
    "f_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['71', '1', '158', '6', '71'], ['0', '7', '6', '7', '6'])"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f_labels, f_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CTACATGACCCTGACACTCAGCTACGAGATGTCAAATTTTGGGGGCAATG',\n",
       " 'TACATGACCCTGACACTCAGCTACGAGATGTCAAATTTTGGGGGCAATGA',\n",
       " 'ACATGACCCTGACACTCAGCTACGAGATGTCAAATTTTGGGGGCAATGAA',\n",
       " 'CATGACCCTGACACTCAGCTACGAGATGTCAAATTTTGGGGGCAATGAAA',\n",
       " 'ATGACCCTGACACTCAGCTACGAGATGTCAAATTTTGGGGGCAATGAAAG']"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f_matrix,f_labels,f_pos = get_kmer_from_150mer(filepath_150mer, max_seqs=1)\n",
    "f_matrix[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['120', '120', '120', '120', '120'], ['3', '3', '3', '3', '3'])"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f_labels[:5], f_pos[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "26b205197a934fdeabb71e65ac11acba": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "514ad0bfcabf4df580a9a872af814af9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "55646397fc9349d3af9e98b1f2b26f5d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "70f6be4247664b708b662c34e7abe3ee": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7179c6cc207941648c348b1bf10cb87f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "LabelModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "LabelModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "LabelView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_70f6be4247664b708b662c34e7abe3ee",
      "placeholder": "​",
      "style": "IPY_MODEL_bdde467d943148ce9bb6355fd7582a5c",
      "value": "0.078 MB of 0.078 MB uploaded (0.020 MB deduped)\r"
     }
    },
    "7849f255e99b4853bdba7f8badf1054a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "VBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "VBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "VBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_7179c6cc207941648c348b1bf10cb87f",
       "IPY_MODEL_e0819a1ddcc64c08a748a2fd88350f09"
      ],
      "layout": "IPY_MODEL_26b205197a934fdeabb71e65ac11acba"
     }
    },
    "bdde467d943148ce9bb6355fd7582a5c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "e0819a1ddcc64c08a748a2fd88350f09": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_55646397fc9349d3af9e98b1f2b26f5d",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_514ad0bfcabf4df580a9a872af814af9",
      "value": 1
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
