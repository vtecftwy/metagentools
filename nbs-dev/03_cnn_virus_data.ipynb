{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|default_exp cnn_virus.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from __future__ import annotations\n",
    "from ecutilities.ipython import nb_setup\n",
    "from ecutilities.core import files_in_tree\n",
    "from fastcore.test import test_fail\n",
    "from nbdev import show_doc, nbdev_export\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "Set autoreload mode\n"
     ]
    }
   ],
   "source": [
    "#| hide\n",
    "nb_setup()\n",
    "\n",
    "# ON_COLAB, p2dataroot, p2data = setup_nb(_dev=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "import collections\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import tensorflow as tf\n",
    "import warnings\n",
    "\n",
    "from ecutilities.core import validate_path\n",
    "from functools import partial, partialmethod\n",
    "from metagentools.bio import q_score2prob_error\n",
    "from metagentools.core import TextFileBaseReader, ProjectFileSystem\n",
    "from pathlib import Path\n",
    "from tqdm.notebook import tqdm, trange\n",
    "from typing import Any, Optional\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' # or any {'0', '1', '2'}\n",
    "import tensorflow as tf\n",
    "from tensorflow.io import serialize_tensor, FixedLenFeature\n",
    "from tensorflow.keras.utils import Sequence, to_categorical\n",
    "from tensorflow.data import TextLineDataset, TFRecordDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "# Retrieve the package root\n",
    "from metagentools import __file__\n",
    "CODE_ROOT = Path(__file__).parents[0]\n",
    "PACKAGE_ROOT = Path(__file__).parents[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow version: 2.8.2 - Expected 2.8.2\n",
      "metagentools package location: /home/vtec/projects/bio/metagentools/metagentools/__init__.py\n"
     ]
    }
   ],
   "source": [
    "#|hide\n",
    "print(f\"Tensorflow version: {tf.__version__} - Expected 2.8.2\")\n",
    "print(f\"metagentools package location: {__file__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data\n",
    "\n",
    "> Data preprocessing and transform functions, data reader classes, datasets for CNN Virus data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data structure for CNN Virus project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many different types of files and datasets for this project. All data are located in directory `data`, under the project root. The following is an overview of the main types of data and in which directory they sit in the tree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A description of the content of each directory can be stored in a file `readme.md` or another `*.md` file. \n",
    "\n",
    "These `readme.md` files can be conveniently accessed using the `.readme(path)` method on `ProjectFileSystem`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# Set configuration file to be the one in nbs-dev folder.\n",
    "# As ProjectFileSystem is a singleton class, this only needs to be done once per notebook\n",
    "p2dev_cfg = PACKAGE_ROOT / 'nbs-dev/metagentools-dev.cfg'\n",
    "pfs = ProjectFileSystem(config_fname=p2dev_cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running linux on local computer\n",
      "Device's home directory: /home/vtec\n",
      "Project file structure:\n",
      " - Root ........ /home/vtec/projects/bio/metagentools \n",
      " - Data Dir .... /home/vtec/projects/bio/metagentools/nbs-dev/data_dev \n",
      " - Notebooks ... /home/vtec/projects/bio/metagentools/nbs\n"
     ]
    }
   ],
   "source": [
    "pfs = ProjectFileSystem()\n",
    "pfs.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we have setup this notebook to use the development data directory `metagentools/nbs-dev/data_dev` and not the standard `metagentools/data`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "assert pfs.data.name == 'data_dev' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<hr>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "ReadMe file for directory `nbs-dev/data_dev`:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<hr>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### Data directory for `metagentools` development \n",
       "This directory includes all the data required to test and validate `metagentools` code.\n",
       "\n",
       "```text\n",
       "data_dev\n",
       " |--- CNN_Virus_data\n",
       " |     |--- 50mer_ds_100_seq\n",
       " |     |--- 150mer_ds_100_seq\n",
       " |     |--- train_short\n",
       " |     |--- val_short\n",
       " |     |--- weight_of_classes\n",
       " |--- ncbi\n",
       " |     |--- infer_results\n",
       " |     |     |--- cnn_virus\n",
       " |     |     |--- csv\n",
       " |     |     |--- xlsx\n",
       " |     |--- refsequences\n",
       " |     |     |--- cov\n",
       " |     |     |     |--- another_sequence.fa\n",
       " |     |     |     |--- cov_virus_sequences_one.fa\n",
       " |     |     |     |--- cov_virus_sequences_two.fa\n",
       " |     |     |     |--- sequences_two_no_matching_rule.fa\n",
       " |     |--- simreads\n",
       " |     |     |--- cov\n",
       " |     |     |     |--- paired_1seq_50bp\n",
       " |     |     |     |      |--- paired_1seq_50bp_1.aln\n",
       " |     |     |     |      |--- paired_1seq_50bp_1.fq\n",
       " |     |     |     |--- single_1seq_50bp\n",
       " |     |     |     |      |--- single_1seq_50bp_1.aln\n",
       " |     |     |     |      |--- single_1seq_50bp_1.fq\n",
       " |--- ....           \n",
       " |--- readme.md               \n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<hr>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pfs.readme()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Original datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<hr>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "ReadMe file for directory `nbs-dev/data_dev/CNN_Virus_data`:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<hr>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### CNN Virus data\n",
       "\n",
       "This directory includes data used to train and validate the initial CNN Virus model, as well as a few smaller datasets for experimenting. \n",
       "\n",
       "\n",
       "#### File list and description:\n",
       "##### 50-mer \n",
       "50-mer reads and their labels, in *text format* with one line per sample. Each line consists of three components, separated by tabs: the 50-mer read or sequence, the virus species label and the position label:\n",
       "```text\n",
       "'TTACNAGCTCCAGTCTAAGATTGTAACTGGCCTTTTTAAAGATTGCTCTA    94    5\\n'\n",
       "``` \n",
       "Files:\n",
       "- `50mer_training`: dataset with 50,903,296 reads for training\n",
       "- `50mer_validating`: dataset with 1,000,000 reads for validation\n",
       "- `50mer_ds_100_reads`: small subset of 100 reads from the validating dataset for experiments\n",
       "\n",
       "##### 150-mer\n",
       "150-mer reads and their labels in *text format* in a similar format as above:\n",
       "```text\n",
       "'TTCTTTCACCACCACAACCAGTCGGCCGTGGAGAGGCGTCGCCGCGTCTCGTTCGTCGAGGCCGATCGACTGCCGCATGAGAGCGGGTGGTATTCTTCCGAAGACGACGGAGACCGGGACGGTGATGAGGAAACTGGAGAGAGCCACAAC    6    0\\n'\n",
       "```\n",
       "Files:\n",
       "- `ICTV_150mer_benchmarking`: dataset with 10,0000 read\n",
       "- `150mer_ds_100_reads`: small subset of 100 reads from `ICTV_150mer_benchmarking`\n",
       "\n",
       "##### Longer reads\n",
       "Reads of various length with no labels, in simple *fasta format*. Each read sequence is preceded by a definition line: `> Sequence n`, where `n` is the sequence number.\n",
       "\n",
       "Files:\n",
       "- `training_sequences_300bp.fasta`: dataset with 9,000 300-mer reads\n",
       "- `training_sequences_500bp.fasta`: dataset with 9,000 500-mer reads\n",
       "- `validation_sequences.fasta`: dataset with 564 reads of mixed lengths ranging from 163-mer to 497-mer\n",
       "\n",
       "##### Other files:\n",
       "- `virus_name_mapping`: mapping between virus species and their numerical label\n",
       "- `weight_of_classes`:  weights for each virus species class in the training dataset\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<hr>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pfs.readme(pfs.data/'CNN_Virus_data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Original Data Virus Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class OriginalLabels:\n",
    "    \"\"\"Converts labels to species name for original data\"\"\"\n",
    "    def __init__(self, p2mapping=None):\n",
    "        if p2mapping is None:\n",
    "            p2mapping = ProjectFileSystem().data / 'CNN_Virus_data/virus_name_mapping'\n",
    "        assert p2mapping.is_file()\n",
    "        df = pd.read_csv(p2mapping, sep='\\t', header=None, names=['species', 'label'])\n",
    "        self._label2species = df['species'].to_list()\n",
    "        self._label2species.append('Unknown Virus Species')\n",
    "        self._species2label = {specie:label for specie, label in zip(df['species'], df['label'])}\n",
    "        self._species2label['Unknown Virus Species'] = len(self._label2species)\n",
    "\n",
    "    def search(self, s:str  # string to search through all original virus species\n",
    "                       ):\n",
    "        \"\"\"Prints all species whose name contains the passed string `s`\"\"\"\n",
    "        print('\\n'.join([f\"{k}. Label: {v}\" for k,v in self._species2label.items() if s in k.lower()]))\n",
    "\n",
    "    def label2species(self, n:int # label to convert to species name\n",
    "                      ):\n",
    "        return self._label2species[n]\n",
    "\n",
    "    def species2label(self, s:str  # string to convert to label\n",
    "                      ):\n",
    "        return self._species2label[s]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Original data include 187 viruses, with label from 0 to 186. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0: Variola_virus\n",
      " 94: Middle_East_respiratory_syndrome-related_coronavirus\n",
      "117: Severe_acute_respiratory_syndrome-related_coronavirus\n",
      "118: Yellow_fever_virus\n"
     ]
    }
   ],
   "source": [
    "lbls = OriginalLabels()\n",
    "for n in [0, 94, 117, 118]:\n",
    "    print(f\"{n:3d}: {lbls.label2species(n)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "### OriginalLabels.search\n",
       "\n",
       ">      OriginalLabels.search (s:str)\n",
       "\n",
       "Prints all species whose name contains the passed string `s`\n",
       "\n",
       "|    | **Type** | **Details** |\n",
       "| -- | -------- | ----------- |\n",
       "| s | str | string to search through all original virus species |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "### OriginalLabels.search\n",
       "\n",
       ">      OriginalLabels.search (s:str)\n",
       "\n",
       "Prints all species whose name contains the passed string `s`\n",
       "\n",
       "|    | **Type** | **Details** |\n",
       "| -- | -------- | ----------- |\n",
       "| s | str | string to search through all original virus species |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(OriginalLabels.search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sandfly_fever_Naples_phlebovirus. Label: 35\n",
      "Crimean-Congo_hemorrhagic_fever_orthonairovirus. Label: 76\n",
      "Yellow_fever_virus. Label: 118\n",
      "Rift_Valley_fever_phlebovirus. Label: 156\n"
     ]
    }
   ],
   "source": [
    "OriginalLabels().search('fever')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NCBI reference sequences for simulated reads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pfs = ProjectFileSystem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<hr>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "ReadMe file for directory `nbs-dev/data_dev/ncbi`:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<hr>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### NCBI Data\n",
       "\n",
       "This directory includes all data related to the work done with reference sequences from NCBI. \n",
       "\n",
       "The data is organized in the following subfolders:\n",
       "\n",
       "- `refsequences`: reference CoV sequences downloaded from NCBI, and related metadata\n",
       "- `simreads`: all data from simulated reads, using ART Illumina simulator and the reference sequences\n",
       "- `infer_results`: results from the inference using models with the simulated reads\n",
       "- `ds`: datasets in proper format for training or inference/prediction using the CNN Virus model\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<hr>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pfs.readme(pfs.data / 'ncbi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<hr>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "ReadMe file for directory `nbs-dev/data_dev/ncbi/refsequences`:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No markdown file in this folder\n"
     ]
    }
   ],
   "source": [
    "pfs.readme(pfs.data / 'ncbi/refsequences')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<hr>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "ReadMe file for directory `nbs-dev/data_dev/ncbi/simreads`:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<hr>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### NCBI simulated reads\n",
       "This directory includes all sets of simulated read sequence files generated from NCBI viral sequences using  ARC Illumina. \n",
       "\n",
       "```ascii\n",
       "this-directory\n",
       "    |--cov\n",
       "    |    |\n",
       "    |    |--single_10seq_50bp\n",
       "    |    |    |--single_10seq_50bp.fq\n",
       "    |    |    |--single_10seq_50bp.alnEnd\n",
       "    |    |-- ...\n",
       "    |    |--single_100seq_150bp\n",
       "    |    |    |--single_100seq_150bp.fq\n",
       "    |    |    |--single_100seq_150bp.aln\n",
       "    |    |--paired_100seq_50bp\n",
       "    |    |    |--paired_100seq_50bp2.aln\n",
       "    |    |    |--paired_100seq_50bp1.aln\n",
       "    |    |    |--paired_100seq_50bp2.fq\n",
       "    |    |    |--paired_100seq_50bp1.fq\n",
       "    |    |-- ...\n",
       "    |    |\n",
       "    |---yf\n",
       "    |    |\n",
       "    |    |--yf_AY968064-single-150bp\n",
       "    |    |    |--yf_AY968064-single-1seq-150bp.fq\n",
       "    |    |    |--yf_AY968064-single-1seq-150bp.aln\n",
       "    |    |\n",
       "    |--mRhiFer1\n",
       "    |    |--mRhiFer1_v1.p.dna_rm.primary_assembly.1\n",
       "    |    |    |--mRhiFer1_v1.p.dna_rm.primary_assembly.1.fq\n",
       "    |    |    |--mRhiFer1_v1.p.dna_rm.primary_assembly.1.aln\n",
       "    |    |\n",
       "\n",
       "```\n",
       "\n",
       "This directory includes several subdirectories, each for one virus, e.g. `cov` for corona, `yf` for yellow fever.\n",
       "\n",
       "In each virus subdirectory, several simreads directory includes simulated reads with various parameters, named as `<method>_<nb-seq>_<nb-bp>` where\"\n",
       "- `<method>` is either `single` or `paired` depending on the simulation method\n",
       "- `<nb-seq>` is the number of reference sequences used for simulation, and refers to the `fa` file used\n",
       "- `<nb-bp>` is the number of base pairs used to simulate reads\n",
       "\n",
       "\n",
       "Each sub-directory includes simreads files made using a simulation method and a specific number of reference sequences.\n",
       "- `xxx.fq` and `xxx.aln` files when method is `single`\n",
       "- `xxx1.fq`, `xxx2.fq`, `xxx1.aln` and `xxx2.aln` files when method is `paired`.\n",
       "\n",
       "Example:\n",
       "- `paired_10seq_50bp` means that the simreads were generated by using the `paired` method to simulate 50-bp reads, and using the `fa` file `cov_virus_sequences_010-seqs.fa`.\n",
       "- `single_100seq_50bp` means that the simreads were generated by using the `single` method to simulate 50-bp reads, and using the `fa` file `cov_virus_sequences_100-seqs.fa`. Note that this generated 20,660,104 reads !\n",
       "\n",
       "#### Simread file formats\n",
       "\n",
       "Simulated reads information is split between two files:\n",
       "- **FASTQ** (`.fq`) files providing the read sequences and their ASCII quality scores\n",
       "- **ALN** (`.aln`) files with alignment information\n",
       "\n",
       "##### FASTQ (`.fq`)\n",
       "FASTQ files generated by ART Illumina have the following structure (showing 5 reads), with 4 lines for each read:\n",
       "\n",
       "```ascii\n",
       "@2591237:ncbi:1-60400\n",
       "ACAACTCCTATTCGTAGTTGAAGTTGTTGACAAATACTTTGATTGTTACG\n",
       "+\n",
       "CCCBCGFGBGGGGGGGBGGGGGGGGG>GGG1G=/GGGGGGGGGGGGGGGG\n",
       "@2591237:ncbi:1-60399\n",
       "GATCAATGTGGCATCTACAATACAGACAGCATGAAGCACCACCAAAGGAC\n",
       "+\n",
       "BCBCCFGGGGGGGG1CGGGG<GGBGGGGGFGCGGGGGGDGGG/GG1GGGG\n",
       "@2591237:ncbi:1-60398\n",
       "ATCTACCAGTGGTAGATGGGTTCTTAATAATGAACATTATAGAGCTCTAC\n",
       "+\n",
       "CCCCCGGGEGG1GGF1G/GGEGGGGGGGGGGGGFFGGGGGGGGGGDGGDG\n",
       "@2591237:ncbi:1-60397\n",
       "CGTAAAGTAGAGGCTGTATGGTAGCTAGCACAAATGCCAGCACCAATAGG\n",
       "+\n",
       "BCCCCGGGFGGGGGGFGGGGFGG1GGGGGGG>GG1GGGGGGGGGGE<GGG\n",
       "@2591237:ncbi:1-60396\n",
       "GGTATCGGGTATCTCCTGCATCAATGCAAGGTCTTACAAAGATAAATACT\n",
       "+\n",
       "CBCCCGGG@CGGGGGGGGGGGG=GFGGGGDGGGFG1GGGGGGGG@GGGGG\n",
       "```\n",
       "The following information can be parsed from the each read sequence in the FASTQ file:\n",
       "\n",
       "- Line 1: `readid`, a unique ID for the read, under for format `@readid` \n",
       "- Line 2: `readseq`, the sequence of the read\n",
       "- Line 3: a separator `+`\n",
       "- Line 4: `read_qscores`, the base quality scores encoded in ASCII \n",
       "\n",
       "Example:\n",
       "```\n",
       "@2591237:ncbi:1-60400\n",
       "ACAACTCCTATTCGTAGTTGAAGTTGTTGACAAATACTTTGATTGTTACG\n",
       "+\n",
       "CCCBCGFGBGGGGGGGBGGGGGGGGG>GGG1G=/GGGGGGGGGGGGGGGG\n",
       "```\n",
       "- `readid` = `2591237:ncbi:1-60400`\n",
       "- `readseq` = `ACAACTCCTATTCGTAGTTGAAGTTGTTGACAAATACTTTGATTGTTACG`, a 50 bp read\n",
       "- `read_qscores` = `CCCBCGFGBGGGGGGGBGGGGGGGGG>GGG1G=/GGGGGGGGGGGGGGGG`\n",
       "\n",
       "\n",
       "#### ALN (`.aln`) \n",
       "ALN files generated by ART Illumina consist of :\n",
       "- a header with the ART-Ilumina command used for the simulation (`@CM`) and info on each of the reference sequences used for the simulations (`@SQ`). Header always starts with `##ART_Illumina` and ends with `##Header End` :\n",
       "- the body with 3 lines for each read:\n",
       "    1. definition line with `readid`, \n",
       "        - reference sequence identification number `refseqid`, \n",
       "        - the position in the read in the reference sequence `aln_start_pos` \n",
       "        - the strand the read was taken from `ref_seq_strand`. `+` for coding strand and `-` for template strand\n",
       "    2. aligned reference sequence, that is the sequence segment in the original reference corresponding to the read\n",
       "    3. aligned read sequence, that is the simmulated read sequence, where each bp corresponds to the reference sequence bp in the same position.\n",
       "\n",
       "Example of a ALN file generated by ART Illumina (showing 5 reads):\n",
       "\n",
       "```ascii\n",
       "##ART_Illumina    read_length    50\n",
       "@CM    /bin/art_illumina -i /home/vtec/projects/bio/metagentools/data/cov_data/cov_virus_sequences_ten.fa -ss HS25 -l 50 -f 100 -o /home/vtec/projects/bio/metagentools/data/cov_simreads/single_10seq_50bp/single_10seq_50bp -rs 1674660835\n",
       "@SQ    2591237:ncbi:1 1   MK211378    2591237    ncbi    1     Coronavirus BtRs-BetaCoV/YN2018D    30213\n",
       "@SQ    11128:ncbi:2   2   LC494191    11128    ncbi    2     Bovine coronavirus    30942\n",
       "@SQ    31631:ncbi:3   3   KY967361    31631    ncbi    3     Human coronavirus OC43        30661\n",
       "@SQ    277944:ncbi:4  4   LC654455    277944    ncbi    4     Human coronavirus NL63    27516\n",
       "@SQ    11120:ncbi:5   5   MN987231    11120    ncbi    5     Infectious bronchitis virus    27617\n",
       "@SQ    28295:ncbi:6   6   KU893866    28295    ncbi    6     Porcine epidemic diarrhea virus    28043\n",
       "@SQ    28295:ncbi:7   7   KJ645638    28295    ncbi    7     Porcine epidemic diarrhea virus    27998\n",
       "@SQ    28295:ncbi:8   8   KJ645678    28295    ncbi    8     Porcine epidemic diarrhea virus    27998\n",
       "@SQ    28295:ncbi:9   9   KR873434    28295    ncbi    9     Porcine epidemic diarrhea virus    28038\n",
       "@SQ    1699095:ncbi:10 10  KT368904    1699095    ncbi    10     Camel alphacoronavirus    27395\n",
       "##Header End\n",
       ">2591237:ncbi:1    2591237:ncbi:1-60400    14770    +\n",
       "ACAACTCCTATTCGTAGTTGAAGTTGTTGACAAATACTTTGATTGTTACG\n",
       "ACAACTCCTATTCGTAGTTGAAGTTGTTGACAAATACTTTGATTGTTACG\n",
       ">2591237:ncbi:1    2591237:ncbi:1-60399    17012    -\n",
       "GATCAATGTGGCATCTACAATACAGACAGCATGAAGCACCACCAAAGGAC\n",
       "GATCAATGTGGCATCTACAATACAGACAGCATGAAGCACCACCAAAGGAC\n",
       ">2591237:ncbi:1    2591237:ncbi:1-60398    9188    +\n",
       "ATCTACCAGTGGTAGATGGGTTCTTAATAATGAACATTATAGAGCTCTAC\n",
       "ATCTACCAGTGGTAGATGGGTTCTTAATAATGAACATTATAGAGCTCTAC\n",
       ".....\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<hr>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pfs.readme(pfs.data / 'ncbi/simreads')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model related data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pfs = ProjectFileSystem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<hr>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "ReadMe file for directory `nbs-dev/data_dev/saved`:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<hr>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### Saved data related to models\n",
       "\n",
       "This directory includes all data related to models and saved:\n",
       "- saved model parameters\n",
       "- saved datasets\n",
       "\n",
       "For example:\n",
       "- `cnn_virus_original/pretrained_model.h5` is the saved model parameters for the CNN Virus model\n",
       "- `cnn_virus_datasets/*.tfrecords` are the preprocessed datasets used for inference or training, saved in TFRecord format for performance\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<hr>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pfs.readme(pfs.data / 'saved')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parsing sequence files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following classes make it easier to read and parse files of different formats into their underlying components to generated the training, validation, testing and inference datasets for the model.\n",
    "\n",
    "Each class inherits from `TextFileBaseReader` and adds:\n",
    "\n",
    "- One or several text parsing method(s) to parse metadata according to a specific format\n",
    "- A file parsing method to extract metadata from all elements in the file, returning it as a key:value dictionary and optionally save the metadata as a json file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FASTA file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extension of `TextFileBaseReader` class for fasta sequence files.\n",
    "\n",
    "Structure of a FASTA sequence file:\n",
    "```ascii\n",
    ">definition line - format varies from dataset to dataset\n",
    "sequence line: sequence of bases\n",
    "```\n",
    "Example for the NCBI datasets:\n",
    "```ascii\n",
    ">seqid accession taxonomyid source seqnb organism\n",
    "TATTAGGTTTTCTACCTACCCAGGAAAAGCCAACCAACCTCGATCTCTTGTAGATCTGTTCTCTAAACGAACTTTAAAAT ...\n",
    ">2591237:ncbi:1 MK211378\t2591237\tncbi\t1 Coronavirus BtRs-BetaCoV/YN2018D\n",
    "TATTAGGTTTTCTACCTACCCAGGAAAAGCCAACCAACCTCGATCTCTTGTAGATCTGTTCTCTAAACGAACTTTAAAAT ...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">2591237:ncbi:1\t1\tMK211378\t2591237\tncbi\tCoronavirus BtRs-BetaCoV/YN2018D ...\n",
      "TATTAGGTTTTCTACCTACCCAGGAAAAGCCAACCAACCTCGATCTCTTGTAGATCTGTTCTCTAAACGAACTTTAAAAT ...\n",
      ">11128:ncbi:2\t2\tLC494191\t11128\tncbi\tBovine coronavirus ...\n",
      "CATCCCGCTTCACTGATCTCTTGTTAGATCTTTTCATAATCTAAACTTTATAAAAACATCCACTCCCTGTAGTCTATGCC ...\n"
     ]
    }
   ],
   "source": [
    "p2fasta = pfs.data / 'ncbi/refsequences/cov/cov_virus_sequences_two.fa'\n",
    "\n",
    "fasta = TextFileBaseReader(p2fasta, nlines=1)\n",
    "for i, t in enumerate(fasta):\n",
    "    txt = t.replace('\\n', '')[:80] + ' ...'\n",
    "    print(f\"{txt}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class FastaFileReader(TextFileBaseReader):\n",
    "    \"\"\"Wrap a FASTA file and retrieve its content in raw format and parsed format\"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        path: str|Path,  # path to the Fasta file\n",
    "    ):\n",
    "        super().__init__(path, nlines=1)\n",
    "        self.text_to_parse_key = 'definition line'\n",
    "        self.set_parsing_rules(verbose=False)\n",
    "        \n",
    "    def __next__(self)-> dict[str, str]:   # `{'definition line': text in dfn line, 'sequence': full sequence as str}` \n",
    "        \"\"\"Return one definition line and the corresponding sequence\"\"\"\n",
    "        lines = []\n",
    "        for i in range(2):\n",
    "            lines.append(self._safe_readline())\n",
    "        dfn_line = lines[0].replace('\\n', '')   #remove the next line symbol at the end of the line\n",
    "        sequence = lines[1].replace('\\n', '')   #remove the next line symbol at the end of the line\n",
    "        self._chunk_nb = self._chunk_nb + 1\n",
    "        return {'definition line':dfn_line, 'sequence':f\"{sequence}\"}\n",
    "\n",
    "    @property\n",
    "    def read_nb(self)-> int:\n",
    "        return self._chunk_nb\n",
    "    \n",
    "    def print_first_chunks(\n",
    "        self, \n",
    "        nchunks:int=3,  # number of chunks to print out\n",
    "    ):\n",
    "        \"\"\"Print the first `nchunks` chunks of text from the file\"\"\"\n",
    "        self.reset_iterator()\n",
    "        for i, seq_dict in enumerate(self.__iter__()):\n",
    "            print(f\"\\nSequence {i+1}:\")\n",
    "            print(seq_dict['definition line'])\n",
    "            print(f\"{seq_dict['sequence'][:80]} ...\")\n",
    "            if i >= nchunks-1: break\n",
    "        self.reset_iterator()\n",
    "            \n",
    "    def parse_file(\n",
    "        self,\n",
    "        add_seq :bool=False,     # When True, add the full sequence to the parsed metadata dictionary\n",
    "        save_json: bool=False    # When True, save the file metadata as a json file of same stem name\n",
    "    )-> dict[str]:               # Metadata as Key/Values pairs\n",
    "        \"\"\"Read fasta file and return a dictionary with definition line metadata and optionally sequences\"\"\"\n",
    "    \n",
    "        self.reset_iterator()\n",
    "        parsed = {}\n",
    "        for d in self:\n",
    "            dfn_line = d['definition line']\n",
    "            seq = d['sequence']\n",
    "            metadata = self._parse_text_fn(dfn_line, self.re_pattern, self.re_keys)\n",
    "            if add_seq: metadata['sequence'] = seq         \n",
    "            parsed[metadata['seqid']] = metadata\n",
    "                        \n",
    "        if save_json:\n",
    "            p2json = self.path.parent / f\"{self.path.stem}_metadata.json\"\n",
    "            with open(p2json, 'w') as fp:\n",
    "                json.dump(parsed, fp, indent=4)\n",
    "                print(f\"Metadata for '{self.path.name}'> saved as <{p2json.name}> in  \\n{p2json.parent.absolute()}\\n\")\n",
    "\n",
    "        return parsed\n",
    "\n",
    "    def review(self):\n",
    "        \"\"\"Prints the first and last sequences and metadata in the fasta file and returns the nb or sequences\"\"\"\n",
    "\n",
    "        self.reset_iterator()\n",
    "        for i, seq in enumerate(self):\n",
    "            if i == 0:\n",
    "                first_dfn = seq['definition line']\n",
    "                first_sequence = seq['sequence'][:80] + ' ...'\n",
    "                first_meta = self.parse_text(seq['definition line'])\n",
    "        print(f\"There {'is' if i == 0 else 'are'} {i+1} sequences in this file\")\n",
    "        print('\\nFirst Sequence:')\n",
    "        print(first_dfn)\n",
    "        print(first_sequence)\n",
    "        print(first_meta)\n",
    "        if i != 0:\n",
    "            print('\\nLast Sequence:')\n",
    "            print(seq['definition line'])\n",
    "            print(seq['sequence'][:80] + ' ...')\n",
    "            print(self.parse_text(seq['definition line']))\n",
    "        return i+1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/vtecftwy/metagentools/blob/main/metagentools/cnn_virus/data.py#L61){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### FastaFileReader\n",
       "\n",
       ">      FastaFileReader (path:str|pathlib.Path)\n",
       "\n",
       "Wrap a FASTA file and retrieve its content in raw format and parsed format\n",
       "\n",
       "|    | **Type** | **Details** |\n",
       "| -- | -------- | ----------- |\n",
       "| path | str \\| Path | path to the Fasta file |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/vtecftwy/metagentools/blob/main/metagentools/cnn_virus/data.py#L61){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### FastaFileReader\n",
       "\n",
       ">      FastaFileReader (path:str|pathlib.Path)\n",
       "\n",
       "Wrap a FASTA file and retrieve its content in raw format and parsed format\n",
       "\n",
       "|    | **Type** | **Details** |\n",
       "| -- | -------- | ----------- |\n",
       "| path | str \\| Path | path to the Fasta file |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(FastaFileReader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an iterator, `FastaFileReader` returns a `dict` at each step, as follows:\n",
    "```python\n",
    "{\n",
    "    'definition line': 'string in file as the definition line for the sequence',\n",
    "    'sequence': 'the full sequence'\n",
    "}\n",
    "```\n",
    "\n",
    "Illustration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">2591237:ncbi:1\t1\tMK211378\t2591237\tncbi\tCoronavirus BtRs-BetaCoV/YN2018D ...\n",
      "TATTAGGTTTTCTACCTACCCAGGAAAAGCCAACCAACCTCGATCTCTTGTAGATCTGTTCTCTAAACGAACTTTAAAAT ...\n"
     ]
    }
   ],
   "source": [
    "p2fasta = pfs.data / 'ncbi/refsequences/cov/cov_virus_sequences_two.fa'\n",
    "fasta = FastaFileReader(p2fasta)\n",
    "iteration_output = next(fasta)\n",
    "\n",
    "print(iteration_output['definition line'][:80], '...')\n",
    "print(iteration_output['sequence'][:80], '...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output type :     <class 'dict'>\n",
      "keys :            dict_keys(['definition line', 'sequence'])\n",
      "definition line : >2591237:ncbi:1\t1\tMK211378\t2591237\tncbi\tCoronavirus BtRs-BetaCoV/YN2018D ...'\n",
      "sequence :       'TATTAGGTTTTCTACCTACCCAGGAAAAGCCAACCAACCTCGATCTCTTGTAGATCTGTTCTCTAAACGAACTTTAAAATCTGTGTAGCTGTCGCTCGGC ...'\n"
     ]
    }
   ],
   "source": [
    "print(f\"output type :     {type(iteration_output)}\")\n",
    "print(f\"keys :            {iteration_output.keys()}\")\n",
    "print(f\"definition line : {iteration_output['definition line'][:80]} ...'\")\n",
    "print(f\"sequence :       '{iteration_output['sequence'][:100]} ...'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `definition line` is a string, with tab separated values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'>2591237:ncbi:1\\t1\\tMK211378\\t2591237\\tncbi\\tCoronavirus BtRs-BetaCoV/YN2018D'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(iteration_output['definition line'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/vtecftwy/metagentools/blob/main/metagentools/cnn_virus/data.py#L122){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### FastaFileReader.review\n",
       "\n",
       ">      FastaFileReader.review ()\n",
       "\n",
       "Prints the first and last sequences and metadata in the fasta file and returns the nb or sequences"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/vtecftwy/metagentools/blob/main/metagentools/cnn_virus/data.py#L122){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### FastaFileReader.review\n",
       "\n",
       ">      FastaFileReader.review ()\n",
       "\n",
       "Prints the first and last sequences and metadata in the fasta file and returns the nb or sequences"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(FastaFileReader.review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 2 sequences in this file\n",
      "\n",
      "First Sequence:\n",
      ">2591237:ncbi:1\t1\tMK211378\t2591237\tncbi\tCoronavirus BtRs-BetaCoV/YN2018D\n",
      "TATTAGGTTTTCTACCTACCCAGGAAAAGCCAACCAACCTCGATCTCTTGTAGATCTGTTCTCTAAACGAACTTTAAAAT ...\n",
      "{'accession': 'MK211378', 'organism': 'Coronavirus BtRs-BetaCoV/YN2018D', 'seqid': '2591237:ncbi:1', 'seqnb': '1', 'source': 'ncbi', 'taxonomyid': '2591237'}\n",
      "\n",
      "Last Sequence:\n",
      ">11128:ncbi:2\t2\tLC494191\t11128\tncbi\tBovine coronavirus\n",
      "CATCCCGCTTCACTGATCTCTTGTTAGATCTTTTCATAATCTAAACTTTATAAAAACATCCACTCCCTGTAGTCTATGCC ...\n",
      "{'accession': 'LC494191', 'organism': 'Bovine coronavirus', 'seqid': '11128:ncbi:2', 'seqnb': '2', 'source': 'ncbi', 'taxonomyid': '11128'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_seqs = fasta.review()\n",
    "nb_seqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/vtecftwy/metagentools/blob/main/metagentools/cnn_virus/data.py#L85){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### FastaFileReader.print_first_chunks\n",
       "\n",
       ">      FastaFileReader.print_first_chunks (nchunks:int=3)\n",
       "\n",
       "Print the first `nchunks` chunks of text from the file\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| nchunks | int | 3 | number of chunks to print out |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/vtecftwy/metagentools/blob/main/metagentools/cnn_virus/data.py#L85){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### FastaFileReader.print_first_chunks\n",
       "\n",
       ">      FastaFileReader.print_first_chunks (nchunks:int=3)\n",
       "\n",
       "Print the first `nchunks` chunks of text from the file\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| nchunks | int | 3 | number of chunks to print out |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(FastaFileReader.print_first_chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is convenient to quickly discover and explore new fasta files in raw text format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sequence 1:\n",
      ">2591237:ncbi:1\t1\tMK211378\t2591237\tncbi\tCoronavirus BtRs-BetaCoV/YN2018D\n",
      "TATTAGGTTTTCTACCTACCCAGGAAAAGCCAACCAACCTCGATCTCTTGTAGATCTGTTCTCTAAACGAACTTTAAAAT ...\n",
      "\n",
      "Sequence 2:\n",
      ">11128:ncbi:2\t2\tLC494191\t11128\tncbi\tBovine coronavirus\n",
      "CATCCCGCTTCACTGATCTCTTGTTAGATCTTTTCATAATCTAAACTTTATAAAAACATCCACTCCCTGTAGTCTATGCC ...\n"
     ]
    }
   ],
   "source": [
    "fasta = FastaFileReader(p2fasta)\n",
    "fasta.print_first_chunks(nchunks=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parsing metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The class also provides methods to parse metadata from the file content (definition line, headers, ...)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A regex pattern is used for parsing metadata fom the definition lines in the reference sequence fasta file.\n",
    "\n",
    "Below, we parse the data from the definition line of our Corona virus NCBI dataset (rule `fasta_ncbi_std`):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sequence 1:\n",
    "\n",
    "- Definition Line:\n",
    "```ascii\n",
    ">2591237:ncbi:1 [MK211378]\t2591237\tncbi\t1 [MK211378] 2591237\tCoronavirus YN2018D\t\tscientific name\n",
    "```\n",
    "- Metadata:\n",
    "    - `seqid` = `2591237:ncbi:1`\n",
    "    - `taxonomyid` = `2591237`\n",
    "    - `source` = `ncbi`\n",
    "    - `seqnb` = `1`\n",
    "    - `accession` = `MK211378`\n",
    "    - `species` = `Coronavirus BtRs-BetaCoV/YN2018D`\n",
    "\n",
    "Sequence 2:\n",
    "\n",
    "- Definition Line\n",
    "```ascii\n",
    "    >11128:ncbi:2 [LC494191]\n",
    "```\n",
    "\n",
    "- Metadata:\n",
    "    - `seqid` = `11128:ncbi:2`\n",
    "    - `taxonomyid` = `11128`\n",
    "    - `source` = `ncbi`\n",
    "    - `seqnb` = `2`\n",
    "    - `accession` = `LC494191`\n",
    "    - `species` = `''`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`FastaFileReader` offers:\n",
    "- `parse_text` a method to parse the metadata\n",
    "- an option to set a default \"parsing rule\" for one instance with `set_parsing_rules`.\n",
    "- `parse_file` a method to parse the metadata from all sequences in the file and save it as a json file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/vtecftwy/metagentools/blob/main/metagentools/core.py#LNone){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### TextFileBaseReader.parse_text\n",
       "\n",
       ">      TextFileBaseReader.parse_text (txt:str, pattern:str=None,\n",
       ">                                     keys:list[str]=None)\n",
       "\n",
       "Parse text using regex pattern and keys. Return a metadata dictionary.\n",
       "\n",
       "The passed text is parsed using the regex pattern. The method return a dictionary in the format:\n",
       "\n",
       "    {\n",
       "        'key_1': 'metadata 1',\n",
       "        'key_2': 'metadata 2',\n",
       "        ...\n",
       "    }\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| txt | str |  | text to parse |\n",
       "| pattern | str | None | If None, uses standard regex pattern to extract metadata, otherwise, uses passed regex |\n",
       "| keys | list | None | If None, uses standard regex list of keys, otherwise, uses passed list of keys (str) |\n",
       "| **Returns** | **dict** |  | **parsed metadata in key/value format** |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/vtecftwy/metagentools/blob/main/metagentools/core.py#LNone){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### TextFileBaseReader.parse_text\n",
       "\n",
       ">      TextFileBaseReader.parse_text (txt:str, pattern:str=None,\n",
       ">                                     keys:list[str]=None)\n",
       "\n",
       "Parse text using regex pattern and keys. Return a metadata dictionary.\n",
       "\n",
       "The passed text is parsed using the regex pattern. The method return a dictionary in the format:\n",
       "\n",
       "    {\n",
       "        'key_1': 'metadata 1',\n",
       "        'key_2': 'metadata 2',\n",
       "        ...\n",
       "    }\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| txt | str |  | text to parse |\n",
       "| pattern | str | None | If None, uses standard regex pattern to extract metadata, otherwise, uses passed regex |\n",
       "| keys | list | None | If None, uses standard regex list of keys, otherwise, uses passed list of keys (str) |\n",
       "| **Returns** | **dict** |  | **parsed metadata in key/value format** |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(FastaFileReader.parse_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the parser function with specifically defined `pattern` and `keys`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">2591237:ncbi:1\t1\tMK211378\t2591237\tncbi\tCoronavirus BtRs-BetaCoV/YN2018D\n"
     ]
    }
   ],
   "source": [
    "fasta = FastaFileReader(p2fasta)\n",
    "dfn_line, sequence = next(fasta).values()\n",
    "print(dfn_line.replace('\\n', ''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pattern = r\"^>(?P<seqid>(?P<taxonomyid>\\d+):(?P<source>ncbi):(?P<seqnb>\\d*))[\\s\\t]*\\[(?P<accession>[\\w\\d]*)\\]([\\s\\t]*(?P=taxonomyid)[\\s\\t]*(?P=source)[\\s\\t]*(?P=seqnb)[\\s\\t]*\\[(?P=accession)\\][\\s\\t]*(?P=taxonomyid)[\\s\\t]*(?P<species>[\\w\\s\\-\\_\\/]*))?\"\n",
    "pattern = r\"^>(?P<seqid>(?P<taxonomyid>\\d+):(?P<source>ncbi):(?P<seqnb>\\d*))[\\s\\t]*(?P=seqnb)[\\s\\t](?P<accession>[\\w\\d]*)([\\s\\t]*(?P=taxonomyid)[\\s\\t]*(?P=source)[\\s\\t][\\s\\t]*(?P<organism>[\\w\\s\\-\\_\\/]*))?\"\n",
    "\n",
    "keys = 'seqid taxonomyid accession source seqnb organism'.split(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accession': 'MK211378',\n",
       " 'organism': 'Coronavirus BtRs-BetaCoV/YN2018D',\n",
       " 'seqid': '2591237:ncbi:1',\n",
       " 'seqnb': '1',\n",
       " 'source': 'ncbi',\n",
       " 'taxonomyid': '2591237'}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fasta.parse_text(dfn_line, pattern=pattern, keys=keys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When a `FastaFileReader` instance is created, all existing rules in the file `default_parsing_rules.json` are tested on the first definition line of the fasta file and the one rule that parses the most matches will be selected automatically and saved in instance attributes `re_rule_name`, `re_pattern` and `re_keys`. \n",
    "\n",
    "`parse_file` extract metadata from each definition line in the fasta file and return a dictionary with all metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fasta_ncbi_std\n",
      "^>(?P<seqid>(?P<taxonomyid>\\d+):(?P<source>ncbi):(?P<seqnb>\\d*))[\\s\\t]*(?P=seqnb)[\\s\\t](?P<accession>[\\w\\d]*)([\\s\\t]*(?P=taxonomyid)[\\s\\t]*(?P=source)[\\s\\t][\\s\\t]*(?P<organism>[\\w\\s\\-\\_/]*))?\n",
      "['seqid', 'taxonomyid', 'source', 'accession', 'seqnb', 'organism']\n"
     ]
    }
   ],
   "source": [
    "print(fasta.re_rule_name)\n",
    "print(fasta.re_pattern)\n",
    "print(fasta.re_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accession': 'MK211378',\n",
       " 'organism': 'Coronavirus BtRs-BetaCoV/YN2018D',\n",
       " 'seqid': '2591237:ncbi:1',\n",
       " 'seqnb': '1',\n",
       " 'source': 'ncbi',\n",
       " 'taxonomyid': '2591237'}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fasta.parse_text(dfn_line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When another fasta file, which has another definition line structure, is used, another parsing rule is selected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">1 dna_rm:primary_assembly primary_assembly:mRhiFer1_v1.p:1:1:124933378:1 REF\n"
     ]
    }
   ],
   "source": [
    "p2other = pfs.data / 'ncbi/refsequences/cov/another_sequence.fa'\n",
    "assert p2other.is_file()\n",
    "\n",
    "it2 = FastaFileReader(path=p2other)\n",
    "\n",
    "dfn_line, sequence = next(it2).values()\n",
    "print(dfn_line.replace('\\n', ''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fasta_rhinolophus_ferrumequinum\n",
      "^>\\d[\\s\\t](?P<seq_type>dna_rm):(?P<id_type>[\\w\\_]*)[\\s\\w](?P=id_type):(?P<assy>[\\w\\d\\_]*)\\.(?P<seq_level>[\\w]*):\\d*:\\d*:(?P<taxonomy>\\d*):(?P<id>\\d*)[\\s\t]REF$\n",
      "['seq_type', 'id_type', 'assy', 'seq_level', 'taxonomy', 'id']\n"
     ]
    }
   ],
   "source": [
    "print(it2.re_rule_name)\n",
    "print(it2.re_pattern)\n",
    "print(it2.re_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'assy': 'mRhiFer1_v1',\n",
      " 'id': '1',\n",
      " 'id_type': 'primary_assembly',\n",
      " 'seq_level': 'p',\n",
      " 'seq_type': 'dna_rm',\n",
      " 'taxonomy': '124933378'}\n"
     ]
    }
   ],
   "source": [
    "pprint(it2.parse_text(dfn_line))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This rule selection is performed by the class method `set_parsing_rule`. The method can also be called with specific `pattern` and `keys` to force parsing rule not yet saved in the json file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/vtecftwy/metagentools/blob/main/metagentools/core.py#LNone){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### TextFileBaseReader.set_parsing_rules\n",
       "\n",
       ">      TextFileBaseReader.set_parsing_rules (pattern:str|bool=None,\n",
       ">                                            keys:list[str]=None,\n",
       ">                                            verbose:bool=False)\n",
       "\n",
       "Set the standard regex parsing rule for the file.\n",
       "\n",
       "Rules can be set:\n",
       "\n",
       "1. manually by passing specific custom values for `pattern` and `keys`\n",
       "2. automatically, by testing all parsing rules saved in `parsing_rule.json` \n",
       "\n",
       "Automatic selection of parsing rules works by testing each rule saved in `parsing_rule.json` on the first \n",
       "definition line of the fasta file, and selecting the one rule that generates the most metadata matches.\n",
       "\n",
       "Rules consists of two parameters:\n",
       "\n",
       "- The regex pattern including one `group` for each metadata item, e.g `(?P<group_name>regex_code)`\n",
       "- The list of keys, i.e. the list with the name of each regex groups, used as key in the metadata dictionary\n",
       "\n",
       "This method updates the three following class attributes: `re_rule_name`, `re_pattern`, `re_keys`\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| pattern | str \\| bool | None | regex pattern to apply to parse the text, search in parsing rules json if None |\n",
       "| keys | list | None | list of keys/group for regex, search in parsing rules json if None |\n",
       "| verbose | bool | False | when True, provides information on each rule |\n",
       "| **Returns** | **None** |  |  |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/vtecftwy/metagentools/blob/main/metagentools/core.py#LNone){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### TextFileBaseReader.set_parsing_rules\n",
       "\n",
       ">      TextFileBaseReader.set_parsing_rules (pattern:str|bool=None,\n",
       ">                                            keys:list[str]=None,\n",
       ">                                            verbose:bool=False)\n",
       "\n",
       "Set the standard regex parsing rule for the file.\n",
       "\n",
       "Rules can be set:\n",
       "\n",
       "1. manually by passing specific custom values for `pattern` and `keys`\n",
       "2. automatically, by testing all parsing rules saved in `parsing_rule.json` \n",
       "\n",
       "Automatic selection of parsing rules works by testing each rule saved in `parsing_rule.json` on the first \n",
       "definition line of the fasta file, and selecting the one rule that generates the most metadata matches.\n",
       "\n",
       "Rules consists of two parameters:\n",
       "\n",
       "- The regex pattern including one `group` for each metadata item, e.g `(?P<group_name>regex_code)`\n",
       "- The list of keys, i.e. the list with the name of each regex groups, used as key in the metadata dictionary\n",
       "\n",
       "This method updates the three following class attributes: `re_rule_name`, `re_pattern`, `re_keys`\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| pattern | str \\| bool | None | regex pattern to apply to parse the text, search in parsing rules json if None |\n",
       "| keys | list | None | list of keys/group for regex, search in parsing rules json if None |\n",
       "| verbose | bool | False | when True, provides information on each rule |\n",
       "| **Returns** | **None** |  |  |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(FastaFileReader.set_parsing_rules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "definition line: '>2591237:ncbi:1\t1\tMK211378\t2591237\tncbi\tCoronavirus BtRs-BetaCoV/YN2018'\n"
     ]
    }
   ],
   "source": [
    "fasta = FastaFileReader(p2fasta)\n",
    "dfn_line, sequence = next(fasta).values()\n",
    "print(f\"definition line: '{dfn_line[:-1]}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Automatic parsing works by testing each saved rule for the value of `definition line` in the first sequence in the fasta file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "key for text to parse: definition line\n",
      "\n",
      "Text to parse for testing (extracted from first iteration):\n",
      ">2591237:ncbi:1\t1\tMK211378\t2591237\tncbi\tCoronavirus BtRs-BetaCoV/YN2018D\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Rule <fasta_ncbi_std> generated 6 matches\n",
      "--------------------------------------------------------------------------------\n",
      "^>(?P<seqid>(?P<taxonomyid>\\d+):(?P<source>ncbi):(?P<seqnb>\\d*))[\\s\\t]*(?P=seqnb)[\\s\\t](?P<accession>[\\w\\d]*)([\\s\\t]*(?P=taxonomyid)[\\s\\t]*(?P=source)[\\s\\t][\\s\\t]*(?P<organism>[\\w\\s\\-\\_/]*))?\n",
      "['seqid', 'taxonomyid', 'source', 'accession', 'seqnb', 'organism']\n",
      "--------------------------------------------------------------------------------\n",
      "Rule <fastq_art_illumina_ncbi_std> generated an error\n",
      "No match on this line\n",
      "--------------------------------------------------------------------------------\n",
      "Rule <aln_art_illumina_ncbi_std> generated an error\n",
      "No match on this line\n",
      "--------------------------------------------------------------------------------\n",
      "Rule <aln_art_illumina-refseq-ncbi-std> generated an error\n",
      "No match on this line\n",
      "--------------------------------------------------------------------------------\n",
      "Rule <fasta_ncbi_cov> generated an error\n",
      "No match on this line\n",
      "--------------------------------------------------------------------------------\n",
      "Rule <fasta_rhinolophus_ferrumequinum> generated an error\n",
      "No match on this line\n",
      "--------------------------------------------------------------------------------\n",
      "Selected rule with most matches: fasta_ncbi_std\n"
     ]
    }
   ],
   "source": [
    "print(f\"key for text to parse: {fasta.text_to_parse_key}\\n\")\n",
    "fasta.reset_iterator()\n",
    "print('Text to parse for testing (extracted from first iteration):')\n",
    "print(next(fasta)[fasta.text_to_parse_key])\n",
    "print()\n",
    "fasta.set_parsing_rules(verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If no saved rule generates a match, `re_rule_name`, `re_pattern` and `re_keys` remain `None` and a warning message is issued to ask user to add a parsing rule manually. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vtec/projects/bio/metagentools/metagentools/core.py:523: UserWarning: \n",
      "        None of the saved parsing rules were able to extract metadata from the first line in this file.\n",
      "        You must set a custom rule (pattern + keys) before parsing text, by using:\n",
      "            `self.set_parsing_rules(custom_pattern, custom_list_of_keys)`\n",
      "                \n",
      "  warnings.warn(msg, category=UserWarning)\n"
     ]
    }
   ],
   "source": [
    "p2nomatch = pfs.data / 'ncbi/refsequences/cov/sequences_two_no_matching_rule.fa'\n",
    "fasta2 = FastaFileReader(p2nomatch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fasta2.re_rule_name is None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But we still can set a standard rule manually, by passing a re pattern and the corresponding list of keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom Rule\n",
      "^>(?P<seqid>(?P<taxonomyid>\\d+):(?P<source>ncbi):(?P<seqnb>\\d*))\\s*(?P<text>[\\w\\s]*)$\n",
      "['seqid', 'taxonomyid', 'source', 'seqnb', 'text']\n"
     ]
    }
   ],
   "source": [
    "pat = r\"^>(?P<seqid>(?P<taxonomyid>\\d+):(?P<source>ncbi):(?P<seqnb>\\d*))\\s*(?P<text>[\\w\\s]*)$\"\n",
    "keys = \"seqid taxonomyid source seqnb text\".split()\n",
    "fasta2.set_parsing_rules(pattern=pat, keys=keys)\n",
    "\n",
    "print(fasta2.re_rule_name)\n",
    "print(fasta2.re_pattern)\n",
    "print(fasta2.re_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "definition line: '>2591237:ncbi:1 this sequence does not match any saved parsing rul'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'seqid': '2591237:ncbi:1',\n",
       " 'seqnb': '1',\n",
       " 'source': 'ncbi',\n",
       " 'taxonomyid': '2591237',\n",
       " 'text': 'this sequence does not match any saved parsing rule'}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fasta2.reset_iterator()\n",
    "dfn_line, sequence = next(fasta2).values()\n",
    "print(f\"definition line: '{dfn_line[:-1]}'\")\n",
    "fasta2.parse_text(dfn_line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/vtecftwy/metagentools/blob/main/metagentools/cnn_virus/data.py#L98){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### FastaFileReader.parse_file\n",
       "\n",
       ">      FastaFileReader.parse_file (add_seq:bool=False, save_json:bool=False)\n",
       "\n",
       "Read fasta file and return a dictionary with definition line metadata and optionally sequences\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| add_seq | bool | False | When True, add the full sequence to the parsed metadata dictionary |\n",
       "| save_json | bool | False | When True, save the file metadata as a json file of same stem name |\n",
       "| **Returns** | **dict[str]** |  | **Metadata as Key/Values pairs** |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/vtecftwy/metagentools/blob/main/metagentools/cnn_virus/data.py#L98){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### FastaFileReader.parse_file\n",
       "\n",
       ">      FastaFileReader.parse_file (add_seq:bool=False, save_json:bool=False)\n",
       "\n",
       "Read fasta file and return a dictionary with definition line metadata and optionally sequences\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| add_seq | bool | False | When True, add the full sequence to the parsed metadata dictionary |\n",
       "| save_json | bool | False | When True, save the file metadata as a json file of same stem name |\n",
       "| **Returns** | **dict[str]** |  | **Metadata as Key/Values pairs** |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(FastaFileReader.parse_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'11128:ncbi:2': {'accession': 'LC494191',\n",
      "                  'organism': 'Bovine coronavirus',\n",
      "                  'seqid': '11128:ncbi:2',\n",
      "                  'seqnb': '2',\n",
      "                  'source': 'ncbi',\n",
      "                  'taxonomyid': '11128'},\n",
      " '2591237:ncbi:1': {'accession': 'MK211378',\n",
      "                    'organism': 'Coronavirus BtRs-BetaCoV/YN2018D',\n",
      "                    'seqid': '2591237:ncbi:1',\n",
      "                    'seqnb': '1',\n",
      "                    'source': 'ncbi',\n",
      "                    'taxonomyid': '2591237'}}\n"
     ]
    }
   ],
   "source": [
    "fasta = FastaFileReader(p2fasta)\n",
    "pprint(fasta.parse_file())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metadata for 'cov_virus_sequences_two.fa'> saved as <cov_virus_sequences_two_metadata.json> in  \n",
      "/home/vtec/projects/bio/metagentools/nbs-dev/data_dev/ncbi/refsequences/cov\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fasta.parse_file(save_json=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'aln_art_illumina-refseq-ncbi-std': {'keys': 'refseqid '\n",
      "                                              'reftaxonomyid '\n",
      "                                              'refsource '\n",
      "                                              'refseqnb '\n",
      "                                              'refseq_accession '\n",
      "                                              'organism '\n",
      "                                              'refseq_length',\n",
      "                                      'pattern': '^@SQ[\\\\t\\\\s]*(?P<refseqid>(?P<reftaxonomyid>\\\\d*):(?P<refsource>\\\\w*):(?P<refseqnb>\\\\d*))[\\\\t\\\\s]*(?P=refseqnb)[\\\\t\\\\s]*(?P<refseq_accession>[\\\\w\\\\d]*)[\\\\t\\\\s]*(?P=reftaxonomyid)[\\\\t\\\\s]*(?P=refsource)[\\\\t\\\\s](?P<organism>.*)[\\\\t\\\\s](?P<refseq_length>\\\\d*)$'},\n",
      " 'aln_art_illumina_ncbi_std': {'keys': 'refseqid '\n",
      "                                       'reftaxonomyid '\n",
      "                                       'refsource '\n",
      "                                       'refseqnb '\n",
      "                                       'readid '\n",
      "                                       'readnb '\n",
      "                                       'aln_start_pos '\n",
      "                                       'refseq_strand',\n",
      "                               'pattern': '^>(?P<refseqid>(?P<reftaxonomyid>\\\\d*):(?P<refsource>\\\\w*):(?P<refseqnb>\\\\d*))(\\\\s|\\t'\n",
      "                                          ')*(?P<readid>(?P=reftaxonomyid):(?P=refsource):(?P=refseqnb)-(?P<readnb>\\\\d*(\\\\/\\\\d(-\\\\d)?)?))(\\\\s|\\\\t)(?P<aln_start_pos>\\\\d*)(\\\\s|\\\\t)(?P<refseq_strand>(-|\\\\+))$'},\n",
      " 'fasta_ncbi_cov': {'keys': 'seqid '\n",
      "                            'taxonomyid '\n",
      "                            'source '\n",
      "                            'accession '\n",
      "                            'seqnb '\n",
      "                            'organism',\n",
      "                    'pattern': '^>(?P<seqid>(?P<taxonomyid>\\\\d+):(?P<source>ncbi):(?P<seqnb>\\\\d*))[\\\\s\\\\t]*\\\\[(?P<accession>[\\\\w\\\\d]*)\\\\]([\\\\s\\\\t]*(?P=taxonomyid)[\\\\s\\\\t]*(?P=source)[\\\\s\\\\t]*(?P=seqnb)[\\\\s\\\\t]*\\\\[(?P=accession)\\\\][\\\\s\\\\t]*(?P=taxonomyid)[\\\\s\\\\t]*(?P<organism>[\\\\w\\\\s\\\\-\\\\_\\\\/]*))?'},\n",
      " 'fasta_ncbi_std': {'keys': 'seqid '\n",
      "                            'taxonomyid '\n",
      "                            'source '\n",
      "                            'accession '\n",
      "                            'seqnb '\n",
      "                            'organism',\n",
      "                    'pattern': '^>(?P<seqid>(?P<taxonomyid>\\\\d+):(?P<source>ncbi):(?P<seqnb>\\\\d*))[\\\\s\\\\t]*(?P=seqnb)[\\\\s\\\\t](?P<accession>[\\\\w\\\\d]*)([\\\\s\\\\t]*(?P=taxonomyid)[\\\\s\\\\t]*(?P=source)[\\\\s\\\\t][\\\\s\\\\t]*(?P<organism>[\\\\w\\\\s\\\\-\\\\_/]*))?'},\n",
      " 'fasta_rhinolophus_ferrumequinum': {'keys': 'seq_type '\n",
      "                                             'id_type '\n",
      "                                             'assy '\n",
      "                                             'seq_level '\n",
      "                                             'taxonomy '\n",
      "                                             'id',\n",
      "                                     'pattern': '^>\\\\d[\\\\s\\\\t](?P<seq_type>dna_rm):(?P<id_type>[\\\\w\\\\_]*)[\\\\s\\\\w](?P=id_type):(?P<assy>[\\\\w\\\\d\\\\_]*)\\\\.(?P<seq_level>[\\\\w]*):\\\\d*:\\\\d*:(?P<taxonomy>\\\\d*):(?P<id>\\\\d*)[\\\\s\\t'\n",
      "                                                ']REF$'},\n",
      " 'fastq_art_illumina_ncbi_std': {'keys': 'readid '\n",
      "                                         'reftaxonomyid '\n",
      "                                         'refsource '\n",
      "                                         'refseqnb '\n",
      "                                         'readnb',\n",
      "                                 'pattern': '^@(?P<readid>(?P<reftaxonomyid>\\\\d*):(?P<refsource>\\\\w*):(?P<refseqnb>\\\\d*)-(?P<readnb>\\\\d*(\\\\/\\\\d)?))$'}}\n"
     ]
    }
   ],
   "source": [
    "with open('../default_parsing_rules.json', 'r') as fp:\n",
    "    pprint(json.load(fp), width=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metadata for 'cov_virus_sequence_one.fa'> saved as <cov_virus_sequence_one_metadata.json> in  \n",
      "/home/vtec/projects/bio/metagentools/nbs-dev/data_dev/ncbi/refsequences/cov\n",
      "\n",
      "{'2591237:ncbi:1': {'accession': 'MK211378',\n",
      "                    'organism': 'Coronavirus BtRs-BetaCoV/YN2018D',\n",
      "                    'seqid': '2591237:ncbi:1',\n",
      "                    'seqnb': '1',\n",
      "                    'source': 'ncbi',\n",
      "                    'taxonomyid': '2591237'}}\n"
     ]
    }
   ],
   "source": [
    "p2fasta = pfs.data / 'ncbi/refsequences/cov/cov_virus_sequence_one.fa'\n",
    "fasta = FastaFileReader(p2fasta)\n",
    "fasta_meta = fasta.parse_file(save_json=True)\n",
    "pprint(fasta_meta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FASTQ file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extension of `TextFileBaseReader` class for fastq sequence files.\n",
    "\n",
    "Structure of a FASTQ sequence file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@2591237:ncbi:1-40200\n",
      "TTGTAGATGGTGTTCCTTTTGTTGTTTCAACTGGATACCATTTTCGTGAGTTAGGAGTTGTACATAATCAGGATGTAAAC\n",
      "+\n",
      "CCCGGGCGGGGGCJGJJJGJJGJJJGJGGJGJJJGJGGGGGGGGCJGJGGGGGJJJJGCCGGGGGJCGCGJGJCG=GGGG\n",
      "@2591237:ncbi:1-40199\n",
      "TCATAGTACTACAGATAGAGACACCAGCTACGGTGCGAGCTCTATTCTTTGCACTAATGGCGTACTTAAGAGTCATTTGA\n",
      "+\n",
      "=CCGGGGCGGGGGJJGJJGJGJG=GJJGJCGJJJCJ=JJJJGGJJCJGJGG=JGC1JJGG8GCJCGGGCGG(GCGGCGC=\n",
      "@2591237:ncbi:1-40198\n",
      "TAACATAGTGGTTCGTTTATCAAGGATAATCTATCTCCATAGGTTCTTCATCATCTAACTCTGAATATTTATTCTTAGTT\n",
      "+\n",
      "C=CGGGGGGGGGGCJJJJ=JJJJJJJJJJJGGJJJJ1GJJ8GJJGGGJGGJJC=JJGGGCCGG88GG=GGGGGGCJGGGG\n"
     ]
    }
   ],
   "source": [
    "p2fastq = pfs.data / 'ncbi/simreads/cov/single_1seq_150bp/single_1seq_150bp.fq'\n",
    "\n",
    "fastq = TextFileBaseReader(p2fastq, nlines=1)\n",
    "for i, t in enumerate(fastq):\n",
    "    txt = t.replace('\\n', '')[:80]\n",
    "    print(f\"{txt}\")\n",
    "    if i >= 11: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class FastqFileReader(TextFileBaseReader):\n",
    "    \"\"\"Iterator going through a fastq file's sequences and return each section + prob error as a dict\"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        path:str|Path,   # path to the fastq file\n",
    "    )-> dict:           # key/value with keys: definition line; sequence; q score; prob error\n",
    "        self.nlines = 4\n",
    "        super().__init__(path, nlines=self.nlines)\n",
    "        self.text_to_parse_key = 'definition line'\n",
    "        self.set_parsing_rules(verbose=False)        \n",
    "    \n",
    "    def __next__(self):\n",
    "        \"\"\"Return definition line, sequence and quality scores\"\"\"\n",
    "        lines = []\n",
    "        for i in range(self.nlines):\n",
    "            lines.append(self._safe_readline().replace('\\n', ''))\n",
    "        \n",
    "        output = {\n",
    "            'definition line':lines[0], \n",
    "            'sequence':f\"{lines[1]}\", \n",
    "            'read_qscores': f\"{lines[3]}\",\n",
    "        }\n",
    "        output['probs error'] = np.array([q_score2prob_error(q) for q in output['read_qscores']])\n",
    "        self._chunk_nb = self._chunk_nb + 1\n",
    "        return output\n",
    "\n",
    "    @property\n",
    "    def read_nb(self)-> int:\n",
    "        return self._chunk_nb\n",
    "    \n",
    "    def print_first_chunks(\n",
    "        self, \n",
    "        nchunks:int=3,  # number of chunks to print out\n",
    "    ):\n",
    "        \"\"\"Print the first `nchunks` chunks of text from the file\"\"\"\n",
    "        for i, seq_dict in enumerate(self.__iter__()):\n",
    "            print(f\"\\nSequence {i+1}:\")\n",
    "            print(seq_dict['definition line'])\n",
    "            print(f\"{seq_dict['sequence'][:80]} ...\")\n",
    "            if i >= nchunks: break\n",
    "            \n",
    "    def parse_file(\n",
    "        self,\n",
    "        add_readseq :bool=False,    # When True, add the full sequence to the parsed metadata dictionary\n",
    "        add_qscores:bool=False,     # Add the read ASCII Q Scores to the parsed dictionary when True\n",
    "        add_probs_error:bool=False, # Add the read probability of error to the parsed dictionary when True\n",
    "        save_json: bool=False       # When True, save the file metadata as a json file of same stem name\n",
    "    )-> dict[str]:                  # Metadata as Key/Values pairs\n",
    "        \"\"\"Read fastq file, return a dict with definition line metadata and optionally read sequence and q scores, ...\"\"\"\n",
    "    \n",
    "        self.reset_iterator()\n",
    "        parsed = {}\n",
    "        for d in self:\n",
    "            dfn_line = d['definition line']\n",
    "            seq, q_scores, prob_e = d['sequence'], d['read_qscores'], d['probs error']\n",
    "            metadata = self._parse_text_fn(dfn_line, self.re_pattern, self.re_keys)\n",
    "            if add_readseq: metadata['readseq'] = seq         \n",
    "            if add_qscores: metadata['read_qscores'] = q_scores\n",
    "            if add_probs_error: metadata['probs error'] = prob_e\n",
    "            parsed[metadata['readid']] = metadata \n",
    "                        \n",
    "        if save_json:\n",
    "            p2json = self.path.parent / f\"{self.path.stem}_metadata.json\"\n",
    "            with open(p2json, 'w') as fp:\n",
    "                json.dump(parsed, fp, indent=4)\n",
    "                print(f\"Metadata for '{self.path.name}'> saved as <{p2json.name}> in  \\n{p2json.parent.absolute()}\\n\")\n",
    "\n",
    "        return parsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/vtecftwy/metagentools/blob/main/metagentools/cnn_virus/data.py#L145){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### FastqFileReader\n",
       "\n",
       ">      FastqFileReader (path:str|pathlib.Path)\n",
       "\n",
       "Iterator going through a fastq file's sequences and return each section + prob error as a dict\n",
       "\n",
       "|    | **Type** | **Details** |\n",
       "| -- | -------- | ----------- |\n",
       "| path | str \\| Path | path to the fastq file |\n",
       "| **Returns** | **dict** | **key/value with keys: definition line; sequence; q score; prob error** |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/vtecftwy/metagentools/blob/main/metagentools/cnn_virus/data.py#L145){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### FastqFileReader\n",
       "\n",
       ">      FastqFileReader (path:str|pathlib.Path)\n",
       "\n",
       "Iterator going through a fastq file's sequences and return each section + prob error as a dict\n",
       "\n",
       "|    | **Type** | **Details** |\n",
       "| -- | -------- | ----------- |\n",
       "| path | str \\| Path | path to the fastq file |\n",
       "| **Returns** | **dict** | **key/value with keys: definition line; sequence; q score; prob error** |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(FastqFileReader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n",
      "dict_keys(['definition line', 'sequence', 'read_qscores', 'probs error'])\n",
      "Definition line:  @2591237:ncbi:1-40200\n",
      "Read sequence:    TTGTAGATGGTGTTCCTTTTGTTGTTTCAACTGGATACCATTTTCGTGAGTTAGGAGTTGTACATAATCAGGATGTAAACTTACATAGCTCGCGTCTCAGTTTCAAGGAACTTTTAGTGTATGCTGCTGATCCAGCCATGCATGCAGCTT\n",
      "Q scores (ASCII): CCCGGGCGGGGGCJGJJJGJJGJJJGJGGJGJJJGJGGGGGGGGCJGJGGGGGJJJJGCCGGGGGJCGCGJGJCG=GGGG=CGGGGGG1GCGCGGGGCCGJC8GGGGGGGGGGGCGGGGGGGGGGGC8GGGGGGCGGC1GGGCGGGGGCC\n",
      "Prob error:       0.0004,0.0004,0.0004,0.0002,0.0002,0.0002,0.0004,0.0002,0.0002,0.0002,0.0002,0.0002,0.0004,0.0001,0.0002,0.0001,0.0001,0.0001,0.0002,0.0001,0.0001,0.0002,0.0001,0.0001,0.0001,0.0002,0.0001,0.0002,0.0002,0.0001,0.0002,0.0001,0.0001,0.0001,0.0002,0.0001,0.0002,0.0002,0.0002,0.0002,0.0002,0.0002,0.0002,0.0002,0.0004,0.0001,0.0002,0.0001,0.0002,0.0002,0.0002,0.0002,0.0002,0.0001,0.0001,0.0001,0.0001,0.0002,0.0004,0.0004,0.0002,0.0002,0.0002,0.0002,0.0002,0.0001,0.0004,0.0002,0.0004,0.0002,0.0001,0.0002,0.0001,0.0004,0.0002,0.0016,0.0002,0.0002,0.0002,0.0002,0.0016,0.0004,0.0002,0.0002,0.0002,0.0002,0.0002,0.0002,0.0251,0.0002,0.0004,0.0002,0.0004,0.0002,0.0002,0.0002,0.0002,0.0004,0.0004,0.0002,0.0001,0.0004,0.0050,0.0002,0.0002,0.0002,0.0002,0.0002,0.0002,0.0002,0.0002,0.0002,0.0002,0.0002,0.0004,0.0002,0.0002,0.0002,0.0002,0.0002,0.0002,0.0002,0.0002,0.0002,0.0002,0.0002,0.0004,0.0050,0.0002,0.0002,0.0002,0.0002,0.0002,0.0002,0.0004,0.0002,0.0002,0.0004,0.0251,0.0002,0.0002,0.0002,0.0004,0.0002,0.0002,0.0002,0.0002,0.0002,0.0004,0.0004\n"
     ]
    }
   ],
   "source": [
    "fastq = FastqFileReader(p2fastq)\n",
    "iteration_output = next(fastq)\n",
    "\n",
    "print(type(iteration_output))\n",
    "print(iteration_output.keys())\n",
    "print(f\"Definition line:  {iteration_output['definition line']}\")\n",
    "print(f\"Read sequence:    {iteration_output['sequence']}\")\n",
    "print(f\"Q scores (ASCII): {iteration_output['read_qscores']}\")\n",
    "print(f\"Prob error:       {','.join([f'{p:.4f}' for p in iteration_output['probs error']])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Five largest probabilities of error:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00158489, 0.00501187, 0.00501187, 0.02511886, 0.02511886])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sort(iteration_output['probs error'])[-5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 80, 127, 102, 138,  88])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argsort(iteration_output['probs error'])[-5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'readid': '2591237:ncbi:1-40200',\n",
       " 'readnb': '40200',\n",
       " 'refseqnb': '1',\n",
       " 'refsource': 'ncbi',\n",
       " 'reftaxonomyid': '2591237'}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfn_line = iteration_output['definition line']\n",
    "meta = fastq.parse_text(dfn_line)\n",
    "meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['definition line', 'sequence', 'read_qscores', 'probs error'])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fastq = FastqFileReader(p2fastq)\n",
    "next(fastq).keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/vtecftwy/metagentools/blob/main/metagentools/cnn_virus/data.py#L186){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### FastqFileReader.parse_file\n",
       "\n",
       ">      FastqFileReader.parse_file (add_readseq:bool=False,\n",
       ">                                  add_qscores:bool=False,\n",
       ">                                  add_probs_error:bool=False,\n",
       ">                                  save_json:bool=False)\n",
       "\n",
       "Read fastq file, return a dict with definition line metadata and optionally read sequence and q scores, ...\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| add_readseq | bool | False | When True, add the full sequence to the parsed metadata dictionary |\n",
       "| add_qscores | bool | False | Add the read ASCII Q Scores to the parsed dictionary when True |\n",
       "| add_probs_error | bool | False | Add the read probability of error to the parsed dictionary when True |\n",
       "| save_json | bool | False | When True, save the file metadata as a json file of same stem name |\n",
       "| **Returns** | **dict[str]** |  | **Metadata as Key/Values pairs** |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/vtecftwy/metagentools/blob/main/metagentools/cnn_virus/data.py#L186){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### FastqFileReader.parse_file\n",
       "\n",
       ">      FastqFileReader.parse_file (add_readseq:bool=False,\n",
       ">                                  add_qscores:bool=False,\n",
       ">                                  add_probs_error:bool=False,\n",
       ">                                  save_json:bool=False)\n",
       "\n",
       "Read fastq file, return a dict with definition line metadata and optionally read sequence and q scores, ...\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| add_readseq | bool | False | When True, add the full sequence to the parsed metadata dictionary |\n",
       "| add_qscores | bool | False | Add the read ASCII Q Scores to the parsed dictionary when True |\n",
       "| add_probs_error | bool | False | Add the read probability of error to the parsed dictionary when True |\n",
       "| save_json | bool | False | When True, save the file metadata as a json file of same stem name |\n",
       "| **Returns** | **dict[str]** |  | **Metadata as Key/Values pairs** |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(FastqFileReader.parse_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2591237:ncbi:1-40200\n",
      "{'readid': '2591237:ncbi:1-40200',\n",
      " 'readnb': '40200',\n",
      " 'refseqnb': '1',\n",
      " 'refsource': 'ncbi',\n",
      " 'reftaxonomyid': '2591237'}\n",
      "2591237:ncbi:1-40199\n",
      "{'readid': '2591237:ncbi:1-40199',\n",
      " 'readnb': '40199',\n",
      " 'refseqnb': '1',\n",
      " 'refsource': 'ncbi',\n",
      " 'reftaxonomyid': '2591237'}\n",
      "2591237:ncbi:1-40198\n",
      "{'readid': '2591237:ncbi:1-40198',\n",
      " 'readnb': '40198',\n",
      " 'refseqnb': '1',\n",
      " 'refsource': 'ncbi',\n",
      " 'reftaxonomyid': '2591237'}\n",
      "2591237:ncbi:1-40197\n",
      "{'readid': '2591237:ncbi:1-40197',\n",
      " 'readnb': '40197',\n",
      " 'refseqnb': '1',\n",
      " 'refsource': 'ncbi',\n",
      " 'reftaxonomyid': '2591237'}\n"
     ]
    }
   ],
   "source": [
    "parsed = fastq.parse_file(add_readseq=False, add_qscores=False, add_probs_error=False)\n",
    "for i, (k, v) in enumerate(parsed.items()):\n",
    "    print(k)\n",
    "    pprint(v)\n",
    "    if i >=3: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>readid</th>\n",
       "      <th>readnb</th>\n",
       "      <th>refseqnb</th>\n",
       "      <th>refsource</th>\n",
       "      <th>reftaxonomyid</th>\n",
       "      <th>readseq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2591237:ncbi:1-40200</th>\n",
       "      <td>2591237:ncbi:1-40200</td>\n",
       "      <td>40200</td>\n",
       "      <td>1</td>\n",
       "      <td>ncbi</td>\n",
       "      <td>2591237</td>\n",
       "      <td>TTGTAGATGGTGTTCCTTTTGTTGTTTCAACTGGATACCATTTTCG...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2591237:ncbi:1-40199</th>\n",
       "      <td>2591237:ncbi:1-40199</td>\n",
       "      <td>40199</td>\n",
       "      <td>1</td>\n",
       "      <td>ncbi</td>\n",
       "      <td>2591237</td>\n",
       "      <td>TCATAGTACTACAGATAGAGACACCAGCTACGGTGCGAGCTCTATT...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2591237:ncbi:1-40198</th>\n",
       "      <td>2591237:ncbi:1-40198</td>\n",
       "      <td>40198</td>\n",
       "      <td>1</td>\n",
       "      <td>ncbi</td>\n",
       "      <td>2591237</td>\n",
       "      <td>TAACATAGTGGTTCGTTTATCAAGGATAATCTATCTCCATAGGTTC...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2591237:ncbi:1-40197</th>\n",
       "      <td>2591237:ncbi:1-40197</td>\n",
       "      <td>40197</td>\n",
       "      <td>1</td>\n",
       "      <td>ncbi</td>\n",
       "      <td>2591237</td>\n",
       "      <td>TAATCACTGATAGCAGCATTGCCATCCTGAGCAAAGAAGAAGTGTT...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2591237:ncbi:1-40196</th>\n",
       "      <td>2591237:ncbi:1-40196</td>\n",
       "      <td>40196</td>\n",
       "      <td>1</td>\n",
       "      <td>ncbi</td>\n",
       "      <td>2591237</td>\n",
       "      <td>CTAATGTCAGTACGCCTACAATGCCTGCATCACGCATAGCATCGCA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2591237:ncbi:1-40195</th>\n",
       "      <td>2591237:ncbi:1-40195</td>\n",
       "      <td>40195</td>\n",
       "      <td>1</td>\n",
       "      <td>ncbi</td>\n",
       "      <td>2591237</td>\n",
       "      <td>AAGCTGAAGCATACATAACACAGTCCTTAAGCCGATAACCAGACAA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2591237:ncbi:1-40194</th>\n",
       "      <td>2591237:ncbi:1-40194</td>\n",
       "      <td>40194</td>\n",
       "      <td>1</td>\n",
       "      <td>ncbi</td>\n",
       "      <td>2591237</td>\n",
       "      <td>AGTGGAAGAACTTCACCGTCAAGATGAAACTCGACGGGGCTCTCCA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2591237:ncbi:1-40193</th>\n",
       "      <td>2591237:ncbi:1-40193</td>\n",
       "      <td>40193</td>\n",
       "      <td>1</td>\n",
       "      <td>ncbi</td>\n",
       "      <td>2591237</td>\n",
       "      <td>GCGTCTCGAGTGCTTCGAGTTCACCGTTCTTGAGAACAACCTCCTC...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2591237:ncbi:1-40192</th>\n",
       "      <td>2591237:ncbi:1-40192</td>\n",
       "      <td>40192</td>\n",
       "      <td>1</td>\n",
       "      <td>ncbi</td>\n",
       "      <td>2591237</td>\n",
       "      <td>CTGGTAGTATCTAAGGCTCCACTGAAATACTTGTACTTGTTATATA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2591237:ncbi:1-40191</th>\n",
       "      <td>2591237:ncbi:1-40191</td>\n",
       "      <td>40191</td>\n",
       "      <td>1</td>\n",
       "      <td>ncbi</td>\n",
       "      <td>2591237</td>\n",
       "      <td>GTCTCTATCTGTAGTACTATGACAAATAGACAGTTTCATCAGAAAT...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    readid readnb refseqnb refsource  \\\n",
       "2591237:ncbi:1-40200  2591237:ncbi:1-40200  40200        1      ncbi   \n",
       "2591237:ncbi:1-40199  2591237:ncbi:1-40199  40199        1      ncbi   \n",
       "2591237:ncbi:1-40198  2591237:ncbi:1-40198  40198        1      ncbi   \n",
       "2591237:ncbi:1-40197  2591237:ncbi:1-40197  40197        1      ncbi   \n",
       "2591237:ncbi:1-40196  2591237:ncbi:1-40196  40196        1      ncbi   \n",
       "2591237:ncbi:1-40195  2591237:ncbi:1-40195  40195        1      ncbi   \n",
       "2591237:ncbi:1-40194  2591237:ncbi:1-40194  40194        1      ncbi   \n",
       "2591237:ncbi:1-40193  2591237:ncbi:1-40193  40193        1      ncbi   \n",
       "2591237:ncbi:1-40192  2591237:ncbi:1-40192  40192        1      ncbi   \n",
       "2591237:ncbi:1-40191  2591237:ncbi:1-40191  40191        1      ncbi   \n",
       "\n",
       "                     reftaxonomyid  \\\n",
       "2591237:ncbi:1-40200       2591237   \n",
       "2591237:ncbi:1-40199       2591237   \n",
       "2591237:ncbi:1-40198       2591237   \n",
       "2591237:ncbi:1-40197       2591237   \n",
       "2591237:ncbi:1-40196       2591237   \n",
       "2591237:ncbi:1-40195       2591237   \n",
       "2591237:ncbi:1-40194       2591237   \n",
       "2591237:ncbi:1-40193       2591237   \n",
       "2591237:ncbi:1-40192       2591237   \n",
       "2591237:ncbi:1-40191       2591237   \n",
       "\n",
       "                                                                readseq  \n",
       "2591237:ncbi:1-40200  TTGTAGATGGTGTTCCTTTTGTTGTTTCAACTGGATACCATTTTCG...  \n",
       "2591237:ncbi:1-40199  TCATAGTACTACAGATAGAGACACCAGCTACGGTGCGAGCTCTATT...  \n",
       "2591237:ncbi:1-40198  TAACATAGTGGTTCGTTTATCAAGGATAATCTATCTCCATAGGTTC...  \n",
       "2591237:ncbi:1-40197  TAATCACTGATAGCAGCATTGCCATCCTGAGCAAAGAAGAAGTGTT...  \n",
       "2591237:ncbi:1-40196  CTAATGTCAGTACGCCTACAATGCCTGCATCACGCATAGCATCGCA...  \n",
       "2591237:ncbi:1-40195  AAGCTGAAGCATACATAACACAGTCCTTAAGCCGATAACCAGACAA...  \n",
       "2591237:ncbi:1-40194  AGTGGAAGAACTTCACCGTCAAGATGAAACTCGACGGGGCTCTCCA...  \n",
       "2591237:ncbi:1-40193  GCGTCTCGAGTGCTTCGAGTTCACCGTTCTTGAGAACAACCTCCTC...  \n",
       "2591237:ncbi:1-40192  CTGGTAGTATCTAAGGCTCCACTGAAATACTTGTACTTGTTATATA...  \n",
       "2591237:ncbi:1-40191  GTCTCTATCTGTAGTACTATGACAAATAGACAGTTTCATCAGAAAT...  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata = fastq.parse_file(add_readseq=True)\n",
    "df = pd.DataFrame(metadata).T\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "Rule <fasta_ncbi_std> generated an error\n",
      "No match on this line\n",
      "--------------------------------------------------------------------------------\n",
      "Rule <fastq_art_illumina_ncbi_std> generated 5 matches\n",
      "--------------------------------------------------------------------------------\n",
      "^@(?P<readid>(?P<reftaxonomyid>\\d*):(?P<refsource>\\w*):(?P<refseqnb>\\d*)-(?P<readnb>\\d*(\\/\\d)?))$\n",
      "['readid', 'reftaxonomyid', 'refsource', 'refseqnb', 'readnb']\n",
      "--------------------------------------------------------------------------------\n",
      "Rule <aln_art_illumina_ncbi_std> generated an error\n",
      "No match on this line\n",
      "--------------------------------------------------------------------------------\n",
      "Rule <aln_art_illumina-refseq-ncbi-std> generated an error\n",
      "No match on this line\n",
      "--------------------------------------------------------------------------------\n",
      "Rule <fasta_ncbi_cov> generated an error\n",
      "No match on this line\n",
      "--------------------------------------------------------------------------------\n",
      "Rule <fasta_rhinolophus_ferrumequinum> generated an error\n",
      "No match on this line\n",
      "--------------------------------------------------------------------------------\n",
      "Selected rule with most matches: fastq_art_illumina_ncbi_std\n"
     ]
    }
   ],
   "source": [
    "fastq.set_parsing_rules(verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ALN Alignment Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extension of `TextFileBaseReader` class for ALN read/sequence alignment files.\n",
    "\n",
    "Structure of a ALN sequence file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##ART_Illumina\tread_length\t150\n",
      "@CM\t/usr/bin/art_illumina -i /home/vtec/projects/bio/metagentools/data/ncbi/refs\n",
      "@SQ\t2591237:ncbi:1\t1\tMK211378\t2591237\tncbi\tCoronavirus BtRs-BetaCoV/YN2018D\t3021\n",
      "##Header End\n",
      ">2591237:ncbi:1\t2591237:ncbi:1-40200\t14370\t+\n",
      "TTGTAGATGGTGTTCCTTTTGTTGTTTCAACTGGATACCATTTTCGTGAGTTAGGAGTTGTACATAATCAGGATGTAAAC\n",
      "TTGTAGATGGTGTTCCTTTTGTTGTTTCAACTGGATACCATTTTCGTGAGTTAGGAGTTGTACATAATCAGGATGTAAAC\n",
      ">2591237:ncbi:1\t2591237:ncbi:1-40199\t15144\t-\n",
      "TCATAGTACTACAGATAGAGACACCAGCTACGGTGCGAGCTCTATTCTTTGCACTAATGGCGTACTTAAGATTCATTTGA\n",
      "TCATAGTACTACAGATAGAGACACCAGCTACGGTGCGAGCTCTATTCTTTGCACTAATGGCGTACTTAAGAGTCATTTGA\n",
      ">2591237:ncbi:1\t2591237:ncbi:1-40198\t2971\t-\n",
      "TAACATAGTGGTTCGTTTATCAAGGATAATCTATCTCCATAGGTTCTTCATCATCTAACTCTGAATATTTATTCTTAGTT\n",
      "TAACATAGTGGTTCGTTTATCAAGGATAATCTATCTCCATAGGTTCTTCATCATCTAACTCTGAATATTTATTCTTAGTT\n"
     ]
    }
   ],
   "source": [
    "p2aln = pfs.data / 'ncbi/simreads/cov/single_1seq_150bp/single_1seq_150bp.aln'\n",
    "assert p2aln.is_file()\n",
    "\n",
    "aln = TextFileBaseReader(p2aln, nlines=1)\n",
    "for i, t in enumerate(aln):\n",
    "    txt = t.replace('\\n', '')[:80]\n",
    "    print(f\"{txt}\")\n",
    "    if i >= 12: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class AlnFileReader(TextFileBaseReader):\n",
    "    \"\"\"Iterator going through an ALN file\"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        path:str|Path,   # path to the aln file\n",
    "    )-> dict:            # key/value with keys: \n",
    "        \"\"\"Set TextFileBaseReader attributes and specific class attributes\"\"\"\n",
    "        self.nlines = 1\n",
    "        super().__init__(path, nlines=self.nlines)\n",
    "        self.header = self.read_header()\n",
    "        self.nlines = 3\n",
    "        self.text_to_parse_key = 'definition line'\n",
    "        self.set_parsing_rules(verbose=False)\n",
    "        self.set_header_parsing_rules(verbose=False)\n",
    "        self.ref_sequences = self.parse_header_reference_sequences()\n",
    "\n",
    "    def __next__(self):\n",
    "        \"\"\"Return definition line, sequence and quality scores\"\"\"\n",
    "        lines = []\n",
    "        for i in range(self.nlines):\n",
    "            lines.append(self._safe_readline().replace('\\n', ''))\n",
    "\n",
    "        output = {\n",
    "            'definition line':lines[0], \n",
    "            'ref_seq_aligned':f\"{lines[1]}\", \n",
    "            'read_seq_aligned': f\"{lines[2]}\",\n",
    "        }   \n",
    "        return output\n",
    "    \n",
    "    def read_header(self):\n",
    "        \"\"\"Read ALN file Header and return each section parsed in a dictionary\"\"\"\n",
    "        \n",
    "        header = {}\n",
    "        if self.fp is not None:\n",
    "            self.fp.close()\n",
    "        self.fp = open(self.path, 'r')\n",
    "        \n",
    "        line = self._safe_readline().replace('\\n', '')\n",
    "        if not line.startswith('##ART_Illumina'): \n",
    "            raise ValueError(f\"Header of this file does not start with ##ART_Illumina\")\n",
    "        line = self._safe_readline().replace('\\n', '')\n",
    "        if not line.startswith('@CM'): \n",
    "            raise ValueError(f\"First header line should start with @CM\")\n",
    "        else: \n",
    "            header['command'] = line[3:].replace('\\t', '').strip()\n",
    "\n",
    "        refseqs = []\n",
    "        while True:\n",
    "            line = self._safe_readline().replace('\\n', '')\n",
    "            if line.startswith('##Header End'): break\n",
    "            else:\n",
    "                refseqs.append(line)\n",
    "        header['reference sequences'] = refseqs\n",
    "        \n",
    "        return header\n",
    "    \n",
    "    def reset_iterator(self):\n",
    "        \"\"\"Reset the iterator to point to the first line in the file, by recreating a new file handle.\n",
    "        \n",
    "        `AlnFileReader` requires a specific `reset_iterator` method, in order to skip the header every time it is reset\n",
    "        \"\"\"\n",
    "        if self.fp is not None:\n",
    "            self.fp.close()\n",
    "        self.fp = open(self.path, 'r')\n",
    "        while True:\n",
    "            line = self._safe_readline().replace('\\n', '')\n",
    "            if line.startswith('##Header End'): break\n",
    "\n",
    "    def parse_definition_line_with_position(\n",
    "        self, \n",
    "        dfn_line:str    # fefinition line string to be parsed\n",
    "        )-> dict:       # parsed metadata in key/value format + relative position of the read\n",
    "        \"\"\"Parse definition line and adds relative position\"\"\"\n",
    "        read_meta = self.parse_text(dfn_line)\n",
    "        read_refseqid = read_meta['refseqid']\n",
    "        read_start_pos = int(read_meta['aln_start_pos'])\n",
    "        read_refseq_lentgh = int(self.ref_sequences[read_refseqid]['refseq_length'])\n",
    "        read_meta['read_pos'] = (read_start_pos *10)// read_refseq_lentgh + 1\n",
    "        return read_meta\n",
    "    \n",
    "    def parse_file(\n",
    "        self, \n",
    "        add_ref_seq_aligned:bool=False,   # Add the reference sequence aligned to the parsed dictionary when True\n",
    "        add_read_seq_aligned:bool=False,  # Add the read sequence aligned to the parsed dictionary when True\n",
    "    )-> dict[str]: \n",
    "        # Key/Values. Keys: \n",
    "        # `readid`,`seqid`,`seq_nbr`,`read_nbr`,`aln_start_pos`,`ref_seq_strand`\n",
    "        # optionaly `ref_seq_aligned`,`read_seq_aligned`\n",
    "        \"\"\"Read ALN file, return a dict w/ alignment info for each read and optionaly aligned reference sequence & read\"\"\"\n",
    "        self.reset_iterator()\n",
    "        parsed = {}\n",
    "        for d in self:\n",
    "            dfn_line = d['definition line']\n",
    "            ref_seq_aligned, read_seq_aligned = d['ref_seq_aligned'], d['read_seq_aligned']\n",
    "            metadata = self.parse_text(dfn_line)\n",
    "            if add_ref_seq_aligned: metadata['ref_seq_aligned'] = ref_seq_aligned         \n",
    "            if add_read_seq_aligned: metadata['read_seq_aligned'] = read_seq_aligned\n",
    "            parsed[metadata['readid']] = metadata \n",
    "        return parsed\n",
    "\n",
    "    def parse_header_reference_sequences(\n",
    "        self,\n",
    "        pattern:str|None=None,     # regex pattern to apply to parse the reference sequence info\n",
    "        keys:list[str]|None=None,  # list of keys: keys are both regex match group names and corresponding output dict keys \n",
    "        )->dict[str]:                  # parsed metadata in key/value format\n",
    "        \"\"\"Extract metadata from all header reference sequences\"\"\"\n",
    "        if pattern is None and keys is None:\n",
    "            pattern, keys = self.re_header_pattern, self.re_header_keys\n",
    "        parsed = {}\n",
    "        for seq_dfn_line in self.header['reference sequences']:\n",
    "            metadata = self.parse_text(seq_dfn_line, pattern, keys)\n",
    "            parsed[metadata['refseqid']] = metadata\n",
    "            \n",
    "        return parsed       \n",
    "        \n",
    "    def set_header_parsing_rules(\n",
    "        self,\n",
    "        pattern: str|bool=None,   # regex pattern to apply to parse the text, search in parsing rules json if None\n",
    "        keys: list[str]=None,     # list of keys/group for regex, search in parsing rules json if None\n",
    "        verbose: bool=False       # when True, provides information on each rule\n",
    "    )-> None:\n",
    "        \"\"\"Set the regex parsing rule for reference sequence in ALN header.\n",
    "               \n",
    "        Updates 3 class attributes: `re_header_rule_name`, `re_header_pattern`, `re_header_keys`\n",
    "        \n",
    "        TODO: refactor this and the method in Core: to use a single function for the common part and a parameter for the text_to_parse \n",
    "        \"\"\"\n",
    "        \n",
    "        P2JSON = Path(f\"{PACKAGE_ROOT}/default_parsing_rules.json\")\n",
    "        \n",
    "        self.re_header_rule_name = None\n",
    "        self.re_header_pattern = None\n",
    "        self.re_header_keys = None\n",
    "        \n",
    "        # get the first reference sequence definition line in header\n",
    "        text_to_parse = self.header['reference sequences'][0]\n",
    "        divider_line = f\"{'-'*80}\"\n",
    "\n",
    "        if pattern is not None and keys is not None:  # When specific pattern and keys are passed\n",
    "            try:\n",
    "                metadata_dict = self.parse_text(text_to_parse, pattern, keys)\n",
    "                self.re_header_rule_name = 'Custom Rule'\n",
    "                self.re_header_pattern = pattern\n",
    "                self.re_header_keys = keys\n",
    "                if verbose:\n",
    "                    print(divider_line)\n",
    "                    print(f\"Custom rule was set for header in this instance.\")\n",
    "            except Exception as err: \n",
    "                raise ValueError(f\"The pattern generates the following error:\\n{err}\")\n",
    "                \n",
    "        else:  # automatic rule selection among rules saved in json file\n",
    "            # Load all existing rules from json file\n",
    "            with open(P2JSON, 'r') as fp:\n",
    "                parsing_rules = json.load(fp)\n",
    "                \n",
    "            # test all existing rules and keep the one with highest number of matches\n",
    "            max_nbr_matches = 0\n",
    "            for k, v in parsing_rules.items():\n",
    "                re_header_pattern = v['pattern']\n",
    "                re_header_keys = v['keys'].split(' ')\n",
    "                try:\n",
    "                    metadata_dict = self.parse_text(text_to_parse, re_header_pattern, re_header_keys)\n",
    "                    nbr_matches = len(metadata_dict)\n",
    "                    if verbose:\n",
    "                        print(divider_line)\n",
    "                        print(f\"Rule <{k}> generated {nbr_matches:,d} matches\")\n",
    "                        print(divider_line)\n",
    "                        print(re_header_pattern)\n",
    "                        print(re_header_keys)\n",
    "\n",
    "                    if len(metadata_dict) > max_nbr_matches:\n",
    "                        self.re_header_pattern = re_header_pattern\n",
    "                        self.re_header_keys = re_header_keys\n",
    "                        self.re_header_rule_name = k    \n",
    "                except Exception as err:\n",
    "                    if verbose:\n",
    "                        print(divider_line)\n",
    "                        print(f\"Rule <{k}> generated an error\")\n",
    "                        print(err)\n",
    "                    else:\n",
    "                        pass\n",
    "            if self.re_header_rule_name is None:\n",
    "                msg = \"\"\"\n",
    "        None of the saved parsing rules were able to extract metadata from the first line in this file.\n",
    "        You must set a custom rule (pattern + keys) before parsing text, by using:\n",
    "            `self.set_parsing_rules(custom_pattern, custom_list_of_keys)`\n",
    "                \"\"\"\n",
    "                warnings.warn(msg, category=UserWarning)\n",
    "            \n",
    "            if verbose:\n",
    "                print(divider_line)\n",
    "                print(f\"Selected rule with most matches: {self.re_header_rule_name}\")\n",
    "\n",
    "            # We used the iterator, now we need to reset it to make all lines available\n",
    "            self.reset_iterator()\n",
    "\n",
    "    def cnn_virus_input_generator(\n",
    "        self,\n",
    "        label:int = 118, # label for this batch (assuming all reads are from the same species)\n",
    "        bs:int = 32      # batch size \n",
    "        ) -> tuple[dict[list], tf.Tensor]: # dict of metadata list and tensor of strings\n",
    "        \"\"\"Create a generator yielding a metadata batch (dict) and a read batch (tensor of strings)\n",
    "\n",
    "        The metadata dictionary contains lists of metadata for each read in the batch. For example:\n",
    "        ``` \n",
    "        {\n",
    "            'readid': ['2591237:ncbi:1-40200','2591237:ncbi:1-40199','2591237:ncbi:1-40198', ...],\n",
    "            'refseqid': ['2591237:ncbi:1','2591237:ncbi:1','2591237:ncbi:1', ...],\n",
    "            'read_pos': [5, 6, 1, ...],\n",
    "            'refsource': ['ncbi', 'ncbi', 'ncbi', ...],\n",
    "            ...\n",
    "        }\n",
    "        ```\n",
    "        \"\"\"\n",
    "        list_metadata = []\n",
    "        list_strings = []\n",
    "        for i, chunk in enumerate(self):\n",
    "            read_seq = chunk['read_seq_aligned']\n",
    "            read_label = label\n",
    "            metadata = self.parse_definition_line_with_position(chunk['definition line'])\n",
    "            list_metadata.append(metadata)\n",
    "            list_strings.append(f\"{read_seq}\\t{read_label}\\t{metadata['read_pos']}\")\n",
    "            if i%bs == bs-1:\n",
    "                batch_metadata = dict(zip(list_metadata[0].keys(), list(map(list, zip(*[list(d.values()) for d in list_metadata])))))\n",
    "                batch_strings = tf.convert_to_tensor(list_strings, dtype=tf.string)\n",
    "                list_metadata = []\n",
    "                list_strings = []\n",
    "                yield batch_metadata, batch_strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/vtecftwy/metagentools/blob/main/metagentools/cnn_virus/data.py#L215){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### AlnFileReader\n",
       "\n",
       ">      AlnFileReader (path:str|pathlib.Path)\n",
       "\n",
       "Iterator going through an ALN file\n",
       "\n",
       "|    | **Type** | **Details** |\n",
       "| -- | -------- | ----------- |\n",
       "| path | str \\| Path | path to the aln file |\n",
       "| **Returns** | **dict** | **key/value with keys:** |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/vtecftwy/metagentools/blob/main/metagentools/cnn_virus/data.py#L215){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### AlnFileReader\n",
       "\n",
       ">      AlnFileReader (path:str|pathlib.Path)\n",
       "\n",
       "Iterator going through an ALN file\n",
       "\n",
       "|    | **Type** | **Details** |\n",
       "| -- | -------- | ----------- |\n",
       "| path | str \\| Path | path to the aln file |\n",
       "| **Returns** | **dict** | **key/value with keys:** |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(AlnFileReader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aln = AlnFileReader(p2aln)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`AlnFileReader` iterator returns elements one by one, as dictionaries with each data line related to the read, accessible through the following keys: \n",
    "\n",
    "- key `'definition line'`: **read definition line**, including read metadata \n",
    "- key `'ref_seq_aligned'`: **aligned reference sequence**, that is the sequence segment in the original reference corresponding to the read\n",
    "- key `'read_seq_aligned'`: **aligned read**, that is the simmulated read sequence, where each bp corresponds to the reference sequence bp in the same position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['definition line', 'ref_seq_aligned', 'read_seq_aligned'])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_iteration = next(aln)\n",
    "one_iteration.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'definition line': '>2591237:ncbi:1\\t2591237:ncbi:1-40200\\t14370\\t+',\n",
      " 'read_seq_aligned': 'TTGTAGATGGTGTTCCTTTTGTTGTTTCAACTGGATACCATTTTCGTGAGTTAGGAGTTGTACATAATCAGGATGTAAACTTACATAGCTCGCGTCTCAGTTTCAAGGAACTTTTAGTGTATGCTGCTGATCCAGCCATGCATGCAGCTT',\n",
      " 'ref_seq_aligned': 'TTGTAGATGGTGTTCCTTTTGTTGTTTCAACTGGATACCATTTTCGTGAGTTAGGAGTTGTACATAATCAGGATGTAAACTTACATAGCTCGCGTCTCAGTTTCAAGGAACTTTTAGTGTATGCTGCTGATCCAGCCATGCATGCAGCTT'}\n"
     ]
    }
   ],
   "source": [
    "pprint(one_iteration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfn_line, ref_seq_aligned, read_seq_aligned = one_iteration.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'>2591237:ncbi:1\\t2591237:ncbi:1-40200\\t14370\\t+'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfn_line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'TTGTAGATGGTGTTCCTTTTGTTGTTTCAACTGGATACCATTTTCGTGAGTTAGGAGTTGTACATAATCAGGATGTAAACTTACATAGCTCGCGTCTCAG'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ref_seq_aligned[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'TTGTAGATGGTGTTCCTTTTGTTGTTTCAACTGGATACCATTTTCGTGAGTTAGGAGTTGTACATAATCAGGATGTAAACTTACATAGCTCGCGTCTCAG'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "read_seq_aligned[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'definition line': '>2591237:ncbi:1\\t2591237:ncbi:1-40199\\t15144\\t-',\n",
      " 'read_seq_aligned': 'TCATAGTACTACAGATAGAGACACCAGCTACGGTGCGAGCTCTATTCTTTGCACTAATGGCGTACTTAAGAGTCATTTGAGTTATAGTAGGGATGACATTACGCTTAGTATACGCGAAAAGTGCATCTTGATCCTCATAACTCATTGAGT',\n",
      " 'ref_seq_aligned': 'TCATAGTACTACAGATAGAGACACCAGCTACGGTGCGAGCTCTATTCTTTGCACTAATGGCGTACTTAAGATTCATTTGAGTTATAGTAGGGATGACATTACGCTTAGTATACGCGAAAAGTGCATCTTGATCCTCATAACTCATTGAGT'}\n"
     ]
    }
   ],
   "source": [
    "another_iteration = next(aln)\n",
    "pprint(another_iteration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">2591237:ncbi:1\t2591237:ncbi:1-40200\t14370\t+\n",
      "TTGTAGATGGTGTTCCTTTTGTTGTTTCAACTGGATACCATTTTCGTGAGTTAGGAGTTGTACATAATCAGGATGTAAAC ...\n",
      "TTGTAGATGGTGTTCCTTTTGTTGTTTCAACTGGATACCATTTTCGTGAGTTAGGAGTTGTACATAATCAGGATGTAAAC ...\n",
      "\n",
      ">2591237:ncbi:1\t2591237:ncbi:1-40199\t15144\t-\n",
      "TCATAGTACTACAGATAGAGACACCAGCTACGGTGCGAGCTCTATTCTTTGCACTAATGGCGTACTTAAGATTCATTTGA ...\n",
      "TCATAGTACTACAGATAGAGACACCAGCTACGGTGCGAGCTCTATTCTTTGCACTAATGGCGTACTTAAGAGTCATTTGA ...\n",
      "\n",
      ">2591237:ncbi:1\t2591237:ncbi:1-40198\t2971\t-\n",
      "TAACATAGTGGTTCGTTTATCAAGGATAATCTATCTCCATAGGTTCTTCATCATCTAACTCTGAATATTTATTCTTAGTT ...\n",
      "TAACATAGTGGTTCGTTTATCAAGGATAATCTATCTCCATAGGTTCTTCATCATCTAACTCTGAATATTTATTCTTAGTT ...\n",
      "\n",
      ">2591237:ncbi:1\t2591237:ncbi:1-40197\t15485\t-\n",
      "TAATCACTGATAGCAGCATTGCCATCCTGAGCAAAGAAGAAGTGTTTTAGTTCAACAGAACTTCCTTCCTTAAAGAAACC ...\n",
      "TAATCACTGATAGCAGCATTGCCATCCTGAGCAAAGAAGAAGTGTTTTAGTTCAACAGAACTTCCTTCCTTAAAGAAACC ...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "aln.reset_iterator()\n",
    "for i, d in enumerate(aln):\n",
    "    print(d['definition line'])\n",
    "    print(d['ref_seq_aligned'][:80], '...')\n",
    "    print(d['read_seq_aligned'][:80], '...\\n')\n",
    "    if i >= 3: break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once instantiated, the `AlnFileReader` iterator gives access to the file's header information through `header` instance attribute. It is a dictionary with two keys: `'command'` and `'reference sequences'`:\n",
    "\n",
    "```\n",
    "    {'command':             'art-illumina command used to create the reads',\n",
    "     'reference sequences': ['@SQ metadata on reference sequence 1 used for the reads',\n",
    "                             '@SQ metadata on reference sequence 2 used for the reads', \n",
    "                             ...\n",
    "                            ]\n",
    "    }\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/bin/art_illumina -i /home/vtec/projects/bio/metagentools/data/ncbi/refsequences/cov/cov_refseq_001-seq1.fa -ss HS25 -l 150 -f 200 -o /home/vtec/projects/bio/metagentools/data/ncbi/simreads/cov/single_1seq_150bp/single_1seq_150bp -rs 1723893089\n"
     ]
    }
   ],
   "source": [
    "print(aln.header['command'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@SQ\t2591237:ncbi:1\t1\tMK211378\t2591237\tncbi\tCoronavirus BtRs-BetaCoV/YN2018D\t30213\n"
     ]
    }
   ],
   "source": [
    "for seq_info in aln.header['reference sequences']:\n",
    "    print(seq_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **read definition line** includes key metadata, which need to be parsed using the appropriate parsing rule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/vtecftwy/metagentools/blob/main/metagentools/core.py#LNone){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### TextFileBaseReader.parse_text\n",
       "\n",
       ">      TextFileBaseReader.parse_text (txt:str, pattern:str=None,\n",
       ">                                     keys:list[str]=None)\n",
       "\n",
       "Parse text using regex pattern and keys. Return a metadata dictionary.\n",
       "\n",
       "The passed text is parsed using the regex pattern. The method return a dictionary in the format:\n",
       "\n",
       "    {\n",
       "        'key_1': 'metadata 1',\n",
       "        'key_2': 'metadata 2',\n",
       "        ...\n",
       "    }\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| txt | str |  | text to parse |\n",
       "| pattern | str | None | If None, uses standard regex pattern to extract metadata, otherwise, uses passed regex |\n",
       "| keys | list | None | If None, uses standard regex list of keys, otherwise, uses passed list of keys (str) |\n",
       "| **Returns** | **dict** |  | **parsed metadata in key/value format** |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/vtecftwy/metagentools/blob/main/metagentools/core.py#LNone){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### TextFileBaseReader.parse_text\n",
       "\n",
       ">      TextFileBaseReader.parse_text (txt:str, pattern:str=None,\n",
       ">                                     keys:list[str]=None)\n",
       "\n",
       "Parse text using regex pattern and keys. Return a metadata dictionary.\n",
       "\n",
       "The passed text is parsed using the regex pattern. The method return a dictionary in the format:\n",
       "\n",
       "    {\n",
       "        'key_1': 'metadata 1',\n",
       "        'key_2': 'metadata 2',\n",
       "        ...\n",
       "    }\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| txt | str |  | text to parse |\n",
       "| pattern | str | None | If None, uses standard regex pattern to extract metadata, otherwise, uses passed regex |\n",
       "| keys | list | None | If None, uses standard regex list of keys, otherwise, uses passed list of keys (str) |\n",
       "| **Returns** | **dict** |  | **parsed metadata in key/value format** |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(AlnFileReader.parse_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "pattern, keys = aln.re_pattern, aln.re_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'aln_start_pos': '14370',\n",
       " 'readid': '2591237:ncbi:1-40200',\n",
       " 'readnb': '40200',\n",
       " 'refseq_strand': '+',\n",
       " 'refseqid': '2591237:ncbi:1',\n",
       " 'refseqnb': '1',\n",
       " 'refsource': 'ncbi',\n",
       " 'reftaxonomyid': '2591237'}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aln.parse_text(dfn_line, pattern, keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/vtecftwy/metagentools/blob/main/metagentools/cnn_virus/data.py#L283){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### AlnFileReader.parse_definition_line_with_position\n",
       "\n",
       ">      AlnFileReader.parse_definition_line_with_position (dfn_line:str)\n",
       "\n",
       "Parse definition line and adds relative position\n",
       "\n",
       "|    | **Type** | **Details** |\n",
       "| -- | -------- | ----------- |\n",
       "| dfn_line | str | fefinition line string to be parsed |\n",
       "| **Returns** | **dict** | **parsed metadata in key/value format + relative position of the read** |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/vtecftwy/metagentools/blob/main/metagentools/cnn_virus/data.py#L283){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### AlnFileReader.parse_definition_line_with_position\n",
       "\n",
       ">      AlnFileReader.parse_definition_line_with_position (dfn_line:str)\n",
       "\n",
       "Parse definition line and adds relative position\n",
       "\n",
       "|    | **Type** | **Details** |\n",
       "| -- | -------- | ----------- |\n",
       "| dfn_line | str | fefinition line string to be parsed |\n",
       "| **Returns** | **dict** | **parsed metadata in key/value format + relative position of the read** |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(AlnFileReader.parse_definition_line_with_position)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upon instance creation, `AlnFileReader` automatically checks the `default_parsing_rules.json` file for a workable rule among saved rules. Saved rules include the rule for ART Illumina ALN files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'aln_art_illumina_ncbi_std'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aln.re_rule_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is therefore not required to pass a specific `pattern` and `keys` parameter.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'aln_start_pos': '14370',\n",
       " 'readid': '2591237:ncbi:1-40200',\n",
       " 'readnb': '40200',\n",
       " 'refseq_strand': '+',\n",
       " 'refseqid': '2591237:ncbi:1',\n",
       " 'refseqnb': '1',\n",
       " 'refsource': 'ncbi',\n",
       " 'reftaxonomyid': '2591237'}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aln.parse_text(dfn_line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ART Ilumina ALN files definition lines consist of:\n",
    "\n",
    "- The **read** ID: `readid`, e.g. `2591237:ncbi:1-20100`\n",
    "- the **read** number (order in the file): `readnb`, e.g. `20100`\n",
    "- The **read** start position in the reference sequence: `aln_start_pos`, e.g. `23878`\n",
    "- The **reference sequence** ID: `readid`, e.g. `2591237:ncbi:1-20100`\n",
    "- The **reference sequence** number: `refseqnb`, e.g. `1`\n",
    "- The **reference sequence** source: `refsource`, e.g. `ncbi`\n",
    "- The **reference sequence** taxonomy: `reftaxonomyid`, e.g. `2591237`\n",
    "- The **reference sequence** strand:  `refseq_strand` wich is either `+` or  `-`,\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/vtecftwy/metagentools/blob/main/metagentools/cnn_virus/data.py#L295){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### AlnFileReader.parse_file\n",
       "\n",
       ">      AlnFileReader.parse_file (add_ref_seq_aligned:bool=False,\n",
       ">                                add_read_seq_aligned:bool=False)\n",
       "\n",
       "Read ALN file, return a dict w/ alignment info for each read and optionaly aligned reference sequence & read\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| add_ref_seq_aligned | bool | False | Add the reference sequence aligned to the parsed dictionary when True |\n",
       "| add_read_seq_aligned | bool | False | Add the read sequence aligned to the parsed dictionary when True |\n",
       "| **Returns** | **dict[str]** |  |  |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/vtecftwy/metagentools/blob/main/metagentools/cnn_virus/data.py#L295){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### AlnFileReader.parse_file\n",
       "\n",
       ">      AlnFileReader.parse_file (add_ref_seq_aligned:bool=False,\n",
       ">                                add_read_seq_aligned:bool=False)\n",
       "\n",
       "Read ALN file, return a dict w/ alignment info for each read and optionaly aligned reference sequence & read\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| add_ref_seq_aligned | bool | False | Add the reference sequence aligned to the parsed dictionary when True |\n",
       "| add_read_seq_aligned | bool | False | Add the read sequence aligned to the parsed dictionary when True |\n",
       "| **Returns** | **dict[str]** |  |  |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(AlnFileReader.parse_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2591237:ncbi:1-40200\n",
      "{'aln_start_pos': '14370',\n",
      " 'readid': '2591237:ncbi:1-40200',\n",
      " 'readnb': '40200',\n",
      " 'refseq_strand': '+',\n",
      " 'refseqid': '2591237:ncbi:1',\n",
      " 'refseqnb': '1',\n",
      " 'refsource': 'ncbi',\n",
      " 'reftaxonomyid': '2591237'}\n",
      "2591237:ncbi:1-40199\n",
      "{'aln_start_pos': '15144',\n",
      " 'readid': '2591237:ncbi:1-40199',\n",
      " 'readnb': '40199',\n",
      " 'refseq_strand': '-',\n",
      " 'refseqid': '2591237:ncbi:1',\n",
      " 'refseqnb': '1',\n",
      " 'refsource': 'ncbi',\n",
      " 'reftaxonomyid': '2591237'}\n",
      "2591237:ncbi:1-40198\n",
      "{'aln_start_pos': '2971',\n",
      " 'readid': '2591237:ncbi:1-40198',\n",
      " 'readnb': '40198',\n",
      " 'refseq_strand': '-',\n",
      " 'refseqid': '2591237:ncbi:1',\n",
      " 'refseqnb': '1',\n",
      " 'refsource': 'ncbi',\n",
      " 'reftaxonomyid': '2591237'}\n",
      "2591237:ncbi:1-40197\n",
      "{'aln_start_pos': '15485',\n",
      " 'readid': '2591237:ncbi:1-40197',\n",
      " 'readnb': '40197',\n",
      " 'refseq_strand': '-',\n",
      " 'refseqid': '2591237:ncbi:1',\n",
      " 'refseqnb': '1',\n",
      " 'refsource': 'ncbi',\n",
      " 'reftaxonomyid': '2591237'}\n",
      "2591237:ncbi:1-40196\n",
      "{'aln_start_pos': '16221',\n",
      " 'readid': '2591237:ncbi:1-40196',\n",
      " 'readnb': '40196',\n",
      " 'refseq_strand': '-',\n",
      " 'refseqid': '2591237:ncbi:1',\n",
      " 'refseqnb': '1',\n",
      " 'refsource': 'ncbi',\n",
      " 'reftaxonomyid': '2591237'}\n"
     ]
    }
   ],
   "source": [
    "parsed = aln.parse_file()\n",
    "\n",
    "for i, (k, v) in enumerate(parsed.items()):\n",
    "    print(k)\n",
    "    pprint(v)\n",
    "    if i > 3: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/vtecftwy/metagentools/blob/main/metagentools/cnn_virus/data.py#L315){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### AlnFileReader.parse_header_reference_sequences\n",
       "\n",
       ">      AlnFileReader.parse_header_reference_sequences (pattern:str|None=None,\n",
       ">                                                      keys:list[str]|None=None)\n",
       "\n",
       "Extract metadata from all header reference sequences\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| pattern | str \\| None | None | regex pattern to apply to parse the reference sequence info |\n",
       "| keys | list[str] \\| None | None | list of keys: keys are both regex match group names and corresponding output dict keys |\n",
       "| **Returns** | **dict[str]** |  | **parsed metadata in key/value format** |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/vtecftwy/metagentools/blob/main/metagentools/cnn_virus/data.py#L315){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### AlnFileReader.parse_header_reference_sequences\n",
       "\n",
       ">      AlnFileReader.parse_header_reference_sequences (pattern:str|None=None,\n",
       ">                                                      keys:list[str]|None=None)\n",
       "\n",
       "Extract metadata from all header reference sequences\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| pattern | str \\| None | None | regex pattern to apply to parse the reference sequence info |\n",
       "| keys | list[str] \\| None | None | list of keys: keys are both regex match group names and corresponding output dict keys |\n",
       "| **Returns** | **dict[str]** |  | **parsed metadata in key/value format** |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(AlnFileReader.parse_header_reference_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'2591237:ncbi:1': {'organism': 'Coronavirus BtRs-BetaCoV/YN2018D',\n",
      "                    'refseq_accession': 'MK211378',\n",
      "                    'refseq_length': '30213',\n",
      "                    'refseqid': '2591237:ncbi:1',\n",
      "                    'refseqnb': '1',\n",
      "                    'refsource': 'ncbi',\n",
      "                    'reftaxonomyid': '2591237'}}\n"
     ]
    }
   ],
   "source": [
    "pprint(aln.parse_header_reference_sequences())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/vtecftwy/metagentools/blob/main/metagentools/cnn_virus/data.py#L330){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### AlnFileReader.set_header_parsing_rules\n",
       "\n",
       ">      AlnFileReader.set_header_parsing_rules (pattern:str|bool=None,\n",
       ">                                              keys:list[str]=None,\n",
       ">                                              verbose:bool=False)\n",
       "\n",
       "Set the regex parsing rule for reference sequence in ALN header.\n",
       "\n",
       "Updates 3 class attributes: `re_header_rule_name`, `re_header_pattern`, `re_header_keys`\n",
       "\n",
       "TODO: refactor this and the method in Core: to use a single function for the common part and a parameter for the text_to_parse\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| pattern | str \\| bool | None | regex pattern to apply to parse the text, search in parsing rules json if None |\n",
       "| keys | list[str] | None | list of keys/group for regex, search in parsing rules json if None |\n",
       "| verbose | bool | False | when True, provides information on each rule |\n",
       "| **Returns** | **None** |  |  |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/vtecftwy/metagentools/blob/main/metagentools/cnn_virus/data.py#L330){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### AlnFileReader.set_header_parsing_rules\n",
       "\n",
       ">      AlnFileReader.set_header_parsing_rules (pattern:str|bool=None,\n",
       ">                                              keys:list[str]=None,\n",
       ">                                              verbose:bool=False)\n",
       "\n",
       "Set the regex parsing rule for reference sequence in ALN header.\n",
       "\n",
       "Updates 3 class attributes: `re_header_rule_name`, `re_header_pattern`, `re_header_keys`\n",
       "\n",
       "TODO: refactor this and the method in Core: to use a single function for the common part and a parameter for the text_to_parse\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| pattern | str \\| bool | None | regex pattern to apply to parse the text, search in parsing rules json if None |\n",
       "| keys | list[str] | None | list of keys/group for regex, search in parsing rules json if None |\n",
       "| verbose | bool | False | when True, provides information on each rule |\n",
       "| **Returns** | **None** |  |  |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(AlnFileReader.set_header_parsing_rules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "Rule <fasta_ncbi_std> generated an error\n",
      "No match on this line\n",
      "--------------------------------------------------------------------------------\n",
      "Rule <fastq_art_illumina_ncbi_std> generated an error\n",
      "No match on this line\n",
      "--------------------------------------------------------------------------------\n",
      "Rule <aln_art_illumina_ncbi_std> generated an error\n",
      "No match on this line\n",
      "--------------------------------------------------------------------------------\n",
      "Rule <aln_art_illumina-refseq-ncbi-std> generated 7 matches\n",
      "--------------------------------------------------------------------------------\n",
      "^@SQ[\\t\\s]*(?P<refseqid>(?P<reftaxonomyid>\\d*):(?P<refsource>\\w*):(?P<refseqnb>\\d*))[\\t\\s]*(?P=refseqnb)[\\t\\s]*(?P<refseq_accession>[\\w\\d]*)[\\t\\s]*(?P=reftaxonomyid)[\\t\\s]*(?P=refsource)[\\t\\s](?P<organism>.*)[\\t\\s](?P<refseq_length>\\d*)$\n",
      "['refseqid', 'reftaxonomyid', 'refsource', 'refseqnb', 'refseq_accession', 'organism', 'refseq_length']\n",
      "--------------------------------------------------------------------------------\n",
      "Rule <fasta_ncbi_cov> generated an error\n",
      "No match on this line\n",
      "--------------------------------------------------------------------------------\n",
      "Rule <fasta_rhinolophus_ferrumequinum> generated an error\n",
      "No match on this line\n",
      "--------------------------------------------------------------------------------\n",
      "Selected rule with most matches: aln_art_illumina-refseq-ncbi-std\n"
     ]
    }
   ],
   "source": [
    "aln.set_header_parsing_rules(verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aln_art_illumina-refseq-ncbi-std\n",
      "^@SQ[\\t\\s]*(?P<refseqid>(?P<reftaxonomyid>\\d*):(?P<refsource>\\w*):(?P<refseqnb>\\d*))[\\t\\s]*(?P=refseqnb)[\\t\\s]*(?P<refseq_accession>[\\w\\d]*)[\\t\\s]*(?P=reftaxonomyid)[\\t\\s]*(?P=refsource)[\\t\\s](?P<organism>.*)[\\t\\s](?P<refseq_length>\\d*)$\n",
      "['refseqid', 'reftaxonomyid', 'refsource', 'refseqnb', 'refseq_accession', 'organism', 'refseq_length']\n"
     ]
    }
   ],
   "source": [
    "print(aln.re_header_rule_name)\n",
    "print(aln.re_header_pattern)\n",
    "print(aln.re_header_keys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sequence and reads are provided in various formats (text for original data, fastq + aln for simulated reads) and the model expects a specific format for training, validation and testing datasets.\n",
    "\n",
    "The following functions allow to build the datasets in the format expected by the model from the raw data available.\n",
    "\n",
    "In addition, text based dataset are not efficient, especially for training. Additional functions allow to save and parse dataset in TFRecord format.\n",
    "\n",
    "There are two pipelines for building inference datasets:\n",
    "- via a text inference dataset, in the same format as the original paper's data\n",
    "- via a TFRecord inference dataset for faster operations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text based inference datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this pipeline, the steps are:\n",
    "\n",
    "1. Create a text inference file and a metadata file from FASTQ and ALN with `create_infer_ds_from_fastq`\n",
    "2. Create a `tf.data.TextLineDataset` from the text inference dataset\n",
    "3. Transform it into an inference/training dataset with `.map` and `strings_input_batch_to_tensors`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _map_read_to_label(read, refseq_meta, p2mapping=None):\n",
    "    \"\"\"\"\"\"\n",
    "    if p2mapping is None:\n",
    "        p2mapping = PACKAGE_ROOT / 'data/ncbi/refsequences/taxonomyid-label-mapping.json'\n",
    "        assert p2mapping.is_file()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def create_infer_ds_from_fastq(\n",
    "    p2fastq: str|Path,             # Path to the fastq file (aln file path is inferred)\n",
    "    output_dir:str|Path|None=None, # Path to directory where ds file will be saved\n",
    "    overwrite_ds:bool=False,       # If True, overwrite existing ds file. If False, error is raised if ds file exists\n",
    "    nsamples:int|None=None         # Used to limit the number of reads to use for inference, use all if None\n",
    ")-> (Path, Path, pd.DataFrame):    # Paths to dataset file, path to metadata file, dataframe with metadata\n",
    "    \"\"\"Build an inference dataset file as required by the CNN Virus model from a simreads fastq (ART format).\n",
    "    \n",
    "    Also extract the fastq read sequence metadata, saves it in a metadata file and returns them as a DataFrame\n",
    "    \"\"\"\n",
    "    fastq = FastqFileReader(p2fastq)\n",
    "    aln = AlnFileReader(p2fastq.parent / f\"{p2fastq.stem}.aln\")\n",
    "    \n",
    "    if output_dir is None:\n",
    "        p2outdir = Path()\n",
    "    else:\n",
    "        validate_path(output_dir, path_type='dir', raise_error=True)\n",
    "        p2outdir = output_dir if isinstance(output_dir, Path) else Path(output_dir)\n",
    "    \n",
    "    p2dataset = p2outdir / f\"{p2fastq.stem}_ds\"\n",
    "    p2metadata = p2outdir / f\"{p2fastq.stem}_metadata.csv\"\n",
    "    \n",
    "    if p2dataset.is_file():\n",
    "        if overwrite_ds: \n",
    "            p2dataset.unlink()\n",
    "            if p2metadata.is_file(): p2metadata.unlink()\n",
    "        else:\n",
    "            raise ValueError(f\"{p2dataset.name} already exists in {p2dataset.absolute()}\")\n",
    "    p2dataset.touch()\n",
    "    p2metadata.touch()\n",
    "    \n",
    "    read_ids = []\n",
    "    read_refseqs = []\n",
    "    read_start_pos = []\n",
    "    read_strand = []\n",
    "    \n",
    "    with open(p2dataset, 'a') as fp:\n",
    "        i = 1\n",
    "        for fastq_chunk, aln_chunk in tqdm(zip(fastq, aln)):\n",
    "            seq = fastq_chunk['sequence']\n",
    "            \n",
    "            aln_meta = aln.parse_text(aln_chunk['definition line'])\n",
    "            read_ids.append(aln_meta['readid'])\n",
    "            read_refseqs.append(aln_meta['refseqid'])\n",
    "            read_start_pos.append(aln_meta['aln_start_pos'])\n",
    "            read_strand.append(aln_meta['refseq_strand'])\n",
    "\n",
    "            label = 0\n",
    "            pos = 0\n",
    "            fp.write(f\"{seq}\\t{label}\\t{pos}\\n\")\n",
    "\n",
    "            i += 1\n",
    "            if nsamples:\n",
    "                if i > nsamples: break\n",
    "                    \n",
    "    print(f\"Dataset with {i-1:,d} reads\")\n",
    "\n",
    "    metadata = np.array(list(zip(read_ids, read_refseqs, read_start_pos, read_strand)))\n",
    "    metadata = pd.DataFrame(data={\n",
    "                'read_ids': read_ids,\n",
    "                'read_refseqs': read_refseqs,\n",
    "                'read_start_pos': read_start_pos,\n",
    "                'read_strand': read_strand})\n",
    "    metadata.to_csv(p2metadata, index=True)\n",
    "    \n",
    "    return p2dataset, p2metadata, metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/vtecftwy/metagentools/blob/main/metagentools/cnn_virus/data.py#L434){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### create_infer_ds_from_fastq\n",
       "\n",
       ">      create_infer_ds_from_fastq (p2fastq:str|pathlib.Path,\n",
       ">                                  output_dir:str|pathlib.Path|None=None,\n",
       ">                                  overwrite_ds:bool=False,\n",
       ">                                  nsamples:int|None=None)\n",
       "\n",
       "Build an inference dataset file as required by the CNN Virus model from a simreads fastq (ART format).\n",
       "\n",
       "Also extract the fastq read sequence metadata, saves it in a metadata file and returns them as a DataFrame\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| p2fastq | str \\| Path |  | Path to the fastq file (aln file path is inferred) |\n",
       "| output_dir | str \\| Path \\| None | None | Path to directory where ds file will be saved |\n",
       "| overwrite_ds | bool | False | If True, overwrite existing ds file. If False, error is raised if ds file exists |\n",
       "| nsamples | int \\| None | None | Used to limit the number of reads to use for inference, use all if None |\n",
       "| **Returns** | **(Path, Path, pd.DataFrame)** |  | **Paths to dataset file, path to metadata file, dataframe with metadata** |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/vtecftwy/metagentools/blob/main/metagentools/cnn_virus/data.py#L434){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### create_infer_ds_from_fastq\n",
       "\n",
       ">      create_infer_ds_from_fastq (p2fastq:str|pathlib.Path,\n",
       ">                                  output_dir:str|pathlib.Path|None=None,\n",
       ">                                  overwrite_ds:bool=False,\n",
       ">                                  nsamples:int|None=None)\n",
       "\n",
       "Build an inference dataset file as required by the CNN Virus model from a simreads fastq (ART format).\n",
       "\n",
       "Also extract the fastq read sequence metadata, saves it in a metadata file and returns them as a DataFrame\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| p2fastq | str \\| Path |  | Path to the fastq file (aln file path is inferred) |\n",
       "| output_dir | str \\| Path \\| None | None | Path to directory where ds file will be saved |\n",
       "| overwrite_ds | bool | False | If True, overwrite existing ds file. If False, error is raised if ds file exists |\n",
       "| nsamples | int \\| None | None | Used to limit the number of reads to use for inference, use all if None |\n",
       "| **Returns** | **(Path, Path, pd.DataFrame)** |  | **Paths to dataset file, path to metadata file, dataframe with metadata** |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(create_infer_ds_from_fastq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "163bea74c12745769090971e12d39356",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset with 100 reads\n"
     ]
    }
   ],
   "source": [
    "path2ds, path2meta, meta = create_infer_ds_from_fastq(\n",
    "    p2fastq=p2fastq, \n",
    "    output_dir=pfs.data / 'ncbi/ds/cov',\n",
    "    overwrite_ds=True, \n",
    "    nsamples=100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FASTAQ file name: /home/vtec/projects/bio/metagentools/nbs-dev/data_dev/ncbi/simreads/cov/single_1seq_150bp/single_1seq_150bp.fq\n",
      "Path to dataset:  /home/vtec/projects/bio/metagentools/nbs-dev/data_dev/ncbi/ds/cov/single_1seq_150bp_ds \n",
      "Path to metadata: /home/vtec/projects/bio/metagentools/nbs-dev/data_dev/ncbi/ds/cov/single_1seq_150bp_metadata.csv\n"
     ]
    }
   ],
   "source": [
    "print(f\"FASTAQ file name: {p2fastq.absolute()}\")\n",
    "print(f\"Path to dataset:  {path2ds.absolute()} \\nPath to metadata: {path2meta.absolute()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5-line chunk 1\n",
      "TTGTAGATGGTGTTCCTTTTGTTGTTTCAACTGGATACCATTTTCGTGAGTTAGGAGTTGTACATAATCAGGATGTAAACTTACATAGCTCGCGTCTCAGTTTCAAGGAACTTTTAGTGTATGCTGCTGATCCAGCCATGCATGCAGCTT\t0\t0\n",
      "TCATAGTACTACAGATAGAGACACCAGCTACGGTGCGAGCTCTATTCTTTGCACTAATGGCGTACTTAAGAGTCATTTGAGTTATAGTAGGGATGACATTACGCTTAGTATACGCGAAAAGTGCATCTTGATCCTCATAACTCATTGAGT\t0\t0\n",
      "TAACATAGTGGTTCGTTTATCAAGGATAATCTATCTCCATAGGTTCTTCATCATCTAACTCTGAATATTTATTCTTAGTTAGAGGCTTAAATAACTGTCTCACTATTGAACTTATTATCATGTCAAGATTCCAAATAGCAATCCTGAAAG\t0\t0\n",
      "TAATCACTGATAGCAGCATTGCCATCCTGAGCAAAGAAGAAGTGTTTTAGTTCAACAGAACTTCCTTCCTTAAAGAAACCTTTAGACACAGCAAAGTCATAAAAGTCTTTGTTAAAATTACCGGGTTTGACAGTTTGAAAAGCAACATTG\t0\t0\n",
      "CTAATGTCAGTACGCCTACAATGCCTGCATCACGCATAGCATCGCAGAATTGTACAGTCTTTAATAATGCTTGGCGTACACGTTCACCTAAGTTAGCATATACGCGCAAGATGTCAGGATTCTCTACGAAGTCATACCAATCCTTCTTAT\t0\t0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "TextFileBaseReader(path2ds, nlines=5).print_first_chunks(nchunks=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>read_ids</th>\n",
       "      <th>read_refseqs</th>\n",
       "      <th>read_start_pos</th>\n",
       "      <th>read_strand</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2591237:ncbi:1-40200</td>\n",
       "      <td>2591237:ncbi:1</td>\n",
       "      <td>14370</td>\n",
       "      <td>+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2591237:ncbi:1-40199</td>\n",
       "      <td>2591237:ncbi:1</td>\n",
       "      <td>15144</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2591237:ncbi:1-40198</td>\n",
       "      <td>2591237:ncbi:1</td>\n",
       "      <td>2971</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2591237:ncbi:1-40197</td>\n",
       "      <td>2591237:ncbi:1</td>\n",
       "      <td>15485</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2591237:ncbi:1-40196</td>\n",
       "      <td>2591237:ncbi:1</td>\n",
       "      <td>16221</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               read_ids    read_refseqs read_start_pos read_strand\n",
       "0  2591237:ncbi:1-40200  2591237:ncbi:1          14370           +\n",
       "1  2591237:ncbi:1-40199  2591237:ncbi:1          15144           -\n",
       "2  2591237:ncbi:1-40198  2591237:ncbi:1           2971           -\n",
       "3  2591237:ncbi:1-40197  2591237:ncbi:1          15485           -\n",
       "4  2591237:ncbi:1-40196  2591237:ncbi:1          16221           -"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create an input batch (tensor) from `aln` files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The CNN_Virus model requires inputs in the forms of three tensors:\n",
    "- `read_seq_batch`: a batch of read sequences, \"base hot encoded\"\n",
    "- `label_batch`: a batch of the species' labels\n",
    "- `pos_batch`: a batch of the read relative positions in the reference sequence\n",
    "\n",
    "To go from an alignment file to this input format, we will perform two steps:\n",
    "- extract batches of strings from the `aln` file. Each string will comply with the original data string format, i.e. `read_sequence \\t label \\ position`. This is done by the generator available in the AlnFileReader method.\n",
    "- convert this batch of strings into the three tensors. This is done by the function `string_input_batch_to_tensors`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/vtecftwy/metagentools/blob/main/metagentools/cnn_virus/data.py#L411){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### AlnFileReader.cnn_virus_input_generator\n",
       "\n",
       ">      AlnFileReader.cnn_virus_input_generator (label:int=118, bs:int=32)\n",
       "\n",
       "Create a generator yielding a metadata batch (dict) and a read batch (tensor of strings)\n",
       "\n",
       "The metadata dictionary contains lists of metadata for each read in the batch. For example:\n",
       "``` \n",
       "{\n",
       "    'readid': ['2591237:ncbi:1-40200','2591237:ncbi:1-40199','2591237:ncbi:1-40198', ...],\n",
       "    'refseqid': ['2591237:ncbi:1','2591237:ncbi:1','2591237:ncbi:1', ...],\n",
       "    'read_pos': [5, 6, 1, ...],\n",
       "    'refsource': ['ncbi', 'ncbi', 'ncbi', ...],\n",
       "    ...\n",
       "}\n",
       "```\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| label | int | 118 | label for this batch (assuming all reads are from the same species) |\n",
       "| bs | int | 32 | batch size |\n",
       "| **Returns** | **tuple[dict[list], tf.Tensor]** |  | **dict of metadata list and tensor of strings** |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/vtecftwy/metagentools/blob/main/metagentools/cnn_virus/data.py#L411){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### AlnFileReader.cnn_virus_input_generator\n",
       "\n",
       ">      AlnFileReader.cnn_virus_input_generator (label:int=118, bs:int=32)\n",
       "\n",
       "Create a generator yielding a metadata batch (dict) and a read batch (tensor of strings)\n",
       "\n",
       "The metadata dictionary contains lists of metadata for each read in the batch. For example:\n",
       "``` \n",
       "{\n",
       "    'readid': ['2591237:ncbi:1-40200','2591237:ncbi:1-40199','2591237:ncbi:1-40198', ...],\n",
       "    'refseqid': ['2591237:ncbi:1','2591237:ncbi:1','2591237:ncbi:1', ...],\n",
       "    'read_pos': [5, 6, 1, ...],\n",
       "    'refsource': ['ncbi', 'ncbi', 'ncbi', ...],\n",
       "    ...\n",
       "}\n",
       "```\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| label | int | 118 | label for this batch (assuming all reads are from the same species) |\n",
       "| bs | int | 32 | batch size |\n",
       "| **Returns** | **tuple[dict[list], tf.Tensor]** |  | **dict of metadata list and tensor of strings** |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(AlnFileReader.cnn_virus_input_generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's load an alignment file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@SQ\t2591237:ncbi:1\t1\tMK211378\t2591237\tncbi\tCoronavirus BtRs-BetaCoV/YN2018D\t30213\n"
     ]
    }
   ],
   "source": [
    "p2aln = pfs.data / 'ncbi/simreads/cov/single_1seq_150bp/single_1seq_150bp.aln'\n",
    "assert p2aln.exists()\n",
    "aln = AlnFileReader(p2aln)\n",
    "print('\\n'.join(aln.header['reference sequences']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a file including reads from on single corona virus sequence (label: 117).\n",
    "\n",
    "`AlnFileReader.cnn_virus_input_generator` returns a generator that will yield batches of strings in the correct format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1:\n",
      "tf.Tensor(\n",
      "[b'TTGTAGATGGTGTTCCTTTTGTTGTTTCAACTGGATACCATTTTCGTGAGTTAGGAGTTGTACATAATCAGGATGTAAACTTACATAGCTCGCGTCTCAGTTTCAAGGAACTTTTAGTGTATGCTGCTGATCCAGCCATGCATGCAGCTT\\t117\\t5'\n",
      " b'TCATAGTACTACAGATAGAGACACCAGCTACGGTGCGAGCTCTATTCTTTGCACTAATGGCGTACTTAAGAGTCATTTGAGTTATAGTAGGGATGACATTACGCTTAGTATACGCGAAAAGTGCATCTTGATCCTCATAACTCATTGAGT\\t117\\t6'\n",
      " b'TAACATAGTGGTTCGTTTATCAAGGATAATCTATCTCCATAGGTTCTTCATCATCTAACTCTGAATATTTATTCTTAGTTAGAGGCTTAAATAACTGTCTCACTATTGAACTTATTATCATGTCAAGATTCCAAATAGCAATCCTGAAAG\\t117\\t1'\n",
      " b'TAATCACTGATAGCAGCATTGCCATCCTGAGCAAAGAAGAAGTGTTTTAGTTCAACAGAACTTCCTTCCTTAAAGAAACCTTTAGACACAGCAAAGTCATAAAAGTCTTTGTTAAAATTACCGGGTTTGACAGTTTGAAAAGCAACATTG\\t117\\t6'], shape=(4,), dtype=string)\n",
      "Batch 2:\n",
      "tf.Tensor(\n",
      "[b'CTAATGTCAGTACGCCTACAATGCCTGCATCACGCATAGCATCGCAGAATTGTACAGTCTTTAATAATGCTTGGCGTACACGTTCACCTAAGTTAGCATATACGCGCAAGATGTCAGGATTCTCTACGAAGTCATACCAATCCTTCTTAT\\t117\\t6'\n",
      " b'AAGCTGAAGCATACATAACACAGTCCTTAAGCCGATAACCAGACAAGCTAGTGTCAGCCAATTCAAGCCATGTCATAATACGCATCACCCAGCTAGCAGGCATGTAGACCATATTAAAGTAAGCAACTGTTGCAAGAGAAGGTAACAGGA\\t117\\t7'\n",
      " b'AGTGGAAGAACTTCACCGTCAAGATGAAACTCGACGGGGCTCTCCAAAGTGTGGTAAACAATTTTGTCACCACGCTTAAGAAACTCAACACCTAACTCTGTACGCTGTCCTGAATAGGACCAATCTCTGTAAGAGCCAGCCAAAGAAACT\\t117\\t9'\n",
      " b'GCGTCTCGAGTGCTTCGAGTTCACCGTTCTTGAGAACAACCTCCTCAGAGGTAAGTACTGTGTCATGTGAATCACCTTCAAGAAAGGTGACTTCTTTTGGTGCCTTAAGAGGCATGAGTAGTTGCAGCTGTTCCTTGCCACGTATACACT\\t117\\t10'], shape=(4,), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "aln.reset_iterator()\n",
    "for i, (batch_metadata, batch_reads) in enumerate(aln.cnn_virus_input_generator(bs=4, label=117)):\n",
    "    print(f\"Batch {i+1}:\")\n",
    "    print(batch_reads)\n",
    "    if i+1 >=2: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1:\n",
      "{'aln_start_pos': ['14370', '15144', '2971', '15485'],\n",
      " 'read_pos': [5, 6, 1, 6],\n",
      " 'readid': ['2591237:ncbi:1-40200',\n",
      "            '2591237:ncbi:1-40199',\n",
      "            '2591237:ncbi:1-40198',\n",
      "            '2591237:ncbi:1-40197'],\n",
      " 'readnb': ['40200', '40199', '40198', '40197'],\n",
      " 'refseq_strand': ['+', '-', '-', '-'],\n",
      " 'refseqid': ['2591237:ncbi:1',\n",
      "              '2591237:ncbi:1',\n",
      "              '2591237:ncbi:1',\n",
      "              '2591237:ncbi:1'],\n",
      " 'refseqnb': ['1', '1', '1', '1'],\n",
      " 'refsource': ['ncbi', 'ncbi', 'ncbi', 'ncbi'],\n",
      " 'reftaxonomyid': ['2591237', '2591237', '2591237', '2591237']}\n",
      "Batch 2:\n",
      "{'aln_start_pos': ['16221', '18953', '25360', '27644'],\n",
      " 'read_pos': [6, 7, 9, 10],\n",
      " 'readid': ['2591237:ncbi:1-40196',\n",
      "            '2591237:ncbi:1-40195',\n",
      "            '2591237:ncbi:1-40194',\n",
      "            '2591237:ncbi:1-40193'],\n",
      " 'readnb': ['40196', '40195', '40194', '40193'],\n",
      " 'refseq_strand': ['-', '-', '-', '-'],\n",
      " 'refseqid': ['2591237:ncbi:1',\n",
      "              '2591237:ncbi:1',\n",
      "              '2591237:ncbi:1',\n",
      "              '2591237:ncbi:1'],\n",
      " 'refseqnb': ['1', '1', '1', '1'],\n",
      " 'refsource': ['ncbi', 'ncbi', 'ncbi', 'ncbi'],\n",
      " 'reftaxonomyid': ['2591237', '2591237', '2591237', '2591237']}\n"
     ]
    }
   ],
   "source": [
    "aln.reset_iterator()\n",
    "for i, (batch_metadata, batch_reads) in enumerate(aln.cnn_virus_input_generator(bs=4, label=117)):\n",
    "    print(f\"Batch {i+1}:\")\n",
    "    pprint(batch_metadata)\n",
    "    if i+1 >=2: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def string_input_batch_to_tensors(\n",
    "    b: tf.Tensor,   # batch of strings representing the inputs (kmer, label, position)\n",
    "    k: int = 150    # maximum read length in the batch\n",
    "    ):\n",
    "    \"\"\"Function converting a batch of bp strings into three tensors: (x_seqs, (y_labels, y_pos))\n",
    "\n",
    "    Expects input strings to have the format: 'read sequence \\t label \\t position' where:\n",
    "\n",
    "    - read sequence: a string of length k (kmer)\n",
    "    - label: an integer between 0 and 186 representing the read virus label\n",
    "    - position: an integer between 0 and 9 representing the read position in the genome\n",
    "    \"\"\"\n",
    "    \n",
    "    # Split the string in three: \n",
    "    # returns a ragged tensor which needs to be converted into a normal tensor using .to_tensor()\n",
    "    t = tf.strings.split(b, '\\t').to_tensor(default_value = '', shape=[None, 3])\n",
    "\n",
    "    # Split each sequence string into a list of single base strings:\n",
    "    # 'TCAAAATAATCA' -> ['T','C','A','A','A','A','T','A','A','T','C','A']\n",
    "    seqs = tf.strings.bytes_split(t[:, 0]).to_tensor(shape=(None, k))\n",
    "\n",
    "\n",
    "    # BHE sequences\n",
    "    # Each base letter (A, C, G, T, N) is replaced by a OHE vector\n",
    "    #     \"A\" converted into [1,0,0,0,0]\n",
    "    #     \"C\" converted into [0,1,0,0,0]\n",
    "    #     \"G\" converted into [0,0,1,0,0]\n",
    "    #     \"T\" converted into [0,0,0,1,0]\n",
    "    #     \"N\" converted into [0,0,0,0,1]\n",
    "    # \n",
    "    # Technical Notes:\n",
    "    # a. The batch of sequence `seqs` has a shape (batch_size, 50) after splitting each byte. \n",
    "    #    Must flatten it first, then apply the transform on each base, then reshape to original shape\n",
    "    # b. We need to map each letter to one vector/tensor. \n",
    "    #    1. Cast bytes seqs into integer sequence (uint8 to work byte by byte)\n",
    "    #    2. For each base letter (A, C, G, T, N) create one tensor (batch_size, 50) (seqs_A, _C, _G, _T, _N)\n",
    "    #    3. Value is 1 if it is the base in the sequence, otherwise 0\n",
    "    #    4. Concatenate these 5 tensors into a tensor of shape (batch_size, 50, 5)\n",
    " \n",
    "    seqs_uint8 = tf.io.decode_raw(seqs, out_type=tf.uint8)\n",
    "    # note: tf.io.decode_raw adds one dimension at the end in the process\n",
    "    #       [b'C', b'A', b'T'] will return [[67], [65], [84]] and not [67, 65, 84]\n",
    "    #       this is actually what we want to concatenate the values for each base letter\n",
    "\n",
    "    A, C, G, T, N = 65, 67, 71, 84, 78\n",
    "\n",
    "    seqs_A = tf.cast(seqs_uint8 == A, tf.float32)\n",
    "    seqs_C = tf.cast(seqs_uint8 == C, tf.float32)\n",
    "    seqs_G = tf.cast(seqs_uint8 == G, tf.float32)\n",
    "    seqs_T = tf.cast(seqs_uint8 == T, tf.float32)\n",
    "    seqs_N = tf.cast(seqs_uint8 == N , tf.float32)\n",
    "\n",
    "    x_seqs = tf.concat([seqs_A, seqs_C, seqs_G, seqs_T, seqs_N], axis=2)\n",
    "\n",
    "    # OHE labels\n",
    "    n_labels = 187\n",
    "    y_labels = tf.strings.to_number(t[:, 1], out_type=tf.int32)\n",
    "    y_labels = tf.gather(tf.eye(n_labels), y_labels)\n",
    "\n",
    "    # OHE positions\n",
    "    n_pos = 10\n",
    "    y_pos = tf.strings.to_number(t[:, 2], out_type=tf.int32)\n",
    "    y_pos= tf.gather(tf.eye(n_pos), y_pos)\n",
    "\n",
    "    return (x_seqs, (y_labels, y_pos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/vtecftwy/metagentools/blob/main/metagentools/cnn_virus/data.py#L502){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### string_input_batch_to_tensors\n",
       "\n",
       ">      string_input_batch_to_tensors (b:tensorflow.python.framework.ops.Tensor,\n",
       ">                                     k:int=150)\n",
       "\n",
       "Function converting a batch of bp strings into three tensors: (x_seqs, (y_labels, y_pos))\n",
       "\n",
       "Expects input strings to have the format: 'read sequence     label   position' where:\n",
       "\n",
       "- read sequence: a string of length k (kmer)\n",
       "- label: an integer between 0 and 186 representing the read virus label\n",
       "- position: an integer between 0 and 9 representing the read position in the genome\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| b | tf.Tensor |  | batch of strings representing the inputs (kmer, label, position) |\n",
       "| k | int | 150 | maximum read length in the batch |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/vtecftwy/metagentools/blob/main/metagentools/cnn_virus/data.py#L502){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### string_input_batch_to_tensors\n",
       "\n",
       ">      string_input_batch_to_tensors (b:tensorflow.python.framework.ops.Tensor,\n",
       ">                                     k:int=150)\n",
       "\n",
       "Function converting a batch of bp strings into three tensors: (x_seqs, (y_labels, y_pos))\n",
       "\n",
       "Expects input strings to have the format: 'read sequence     label   position' where:\n",
       "\n",
       "- read sequence: a string of length k (kmer)\n",
       "- label: an integer between 0 and 186 representing the read virus label\n",
       "- position: an integer between 0 and 9 representing the read position in the genome\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| b | tf.Tensor |  | batch of strings representing the inputs (kmer, label, position) |\n",
       "| k | int | 150 | maximum read length in the batch |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(string_input_batch_to_tensors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can apply `strong_input_batch_to_tensors` function to `AlnFileReader` generated batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([32, 150, 5]), TensorShape([32, 187]), TensorShape([32, 10]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aln.reset_iterator()\n",
    "for (batch_read_ids, batch_reads) in aln.cnn_virus_input_generator():\n",
    "    read_kmers_batch, (labels_batch, positions_batch) = string_input_batch_to_tensors(batch_reads, k=150)\n",
    "    break\n",
    "\n",
    "read_kmers_batch.shape, labels_batch.shape, positions_batch.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have our \"hot-base-encoded\" kmer, and our one-hot-encoded labels and positions tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 1., 0.],\n",
       "       [0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 1., 0.],\n",
       "       [1., 0., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0.],\n",
       "       [1., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0.],\n",
       "       [0., 0., 1., 0., 0.],\n",
       "       [0., 0., 1., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "read_kmers_batch.numpy()[0, :10, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_batch[0, :].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 1., 0., 0., 0., 0.], dtype=float32)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positions_batch[0].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def strings_to_tensors(\n",
    "    b: tf.Tensor        # batch of strings \n",
    "    ):\n",
    "    \"\"\"Deprecated function, Replace by base_\"\"\"\n",
    "\n",
    "    msg = \"\"\"\n",
    "    `strings_to_tensors` is deprecated. \n",
    "    Use `string_input_batch_to_tensors` instead, with same capabilities and more.\"\"\"\n",
    "\n",
    "    raise DeprecationWarning(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def split_kmer_batch_into_50mers(\n",
    "    kmer: tf.Tensor        # tensor representing a batch of k-mer reads, after base encoding\n",
    "    ):\n",
    "    \"\"\"Converts a batch of k-mer reads into several 50-mer reads, by shifting the k-mer one base at a time.\n",
    "\n",
    "    for a batch of `b` k-mer reads, returns a batch of `b - 49` 50-mer reads\n",
    "    \"\"\"\n",
    "    def fn(accumulated, elem):\n",
    "        return tf.roll(accumulated, shift=-elem, axis=1)\n",
    "\n",
    "    b = kmer.shape[0]\n",
    "    k = kmer.shape[1]\n",
    "    n = k - 49\n",
    "\n",
    "    # Create a tensor of integers, with a 0 as first element and 1 for all other elements, for shifts\n",
    "    shifts = tf.convert_to_tensor([0] + [1] * (n-1))\n",
    "\n",
    "    # Use tf.scan to shift the original read nb_splits times\n",
    "    shifted = tf.scan(fn, shifts, kmer, reverse=False)\n",
    "    # print(shifted.shape)\n",
    "\n",
    "    # de-interlace the 50-reads\n",
    "    indices = [(read_nb, batch_nb) for batch_nb in range(b) for read_nb in range(n)]\n",
    "    shifted = tf.gather_nd(shifted[:, :, :50, :], indices)\n",
    "    # print(shifted.shape)\n",
    "    \n",
    "    # return shifted[:, :50, :]  # return the tensor with shifted kmer, sliced to only keep 50 bases\n",
    "    return shifted  # return the tensor with shifted kmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/vtecftwy/metagentools/blob/main/metagentools/cnn_virus/data.py#L757){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### split_kmer_batch_into_50mers\n",
       "\n",
       ">      split_kmer_batch_into_50mers\n",
       ">                                    (kmer:tensorflow.python.framework.ops.Tenso\n",
       ">                                    r)\n",
       "\n",
       "Converts a batch of k-mer reads into several 50-mer reads, by shifting the k-mer one base at a time.\n",
       "\n",
       "for a batch of `b` k-mer reads, returns a batch of `b - 49` 50-mer reads\n",
       "\n",
       "|    | **Type** | **Details** |\n",
       "| -- | -------- | ----------- |\n",
       "| kmer | tf.Tensor | tensor representing a batch of k-mer reads, after base encoding |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/vtecftwy/metagentools/blob/main/metagentools/cnn_virus/data.py#L757){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### split_kmer_batch_into_50mers\n",
       "\n",
       ">      split_kmer_batch_into_50mers\n",
       ">                                    (kmer:tensorflow.python.framework.ops.Tenso\n",
       ">                                    r)\n",
       "\n",
       "Converts a batch of k-mer reads into several 50-mer reads, by shifting the k-mer one base at a time.\n",
       "\n",
       "for a batch of `b` k-mer reads, returns a batch of `b - 49` 50-mer reads\n",
       "\n",
       "|    | **Type** | **Details** |\n",
       "| -- | -------- | ----------- |\n",
       "| kmer | tf.Tensor | tensor representing a batch of k-mer reads, after base encoding |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(split_kmer_batch_into_50mers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([32, 150, 5]), 3232)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "read_kmers_batch.shape, (read_kmers_batch.shape[1]-49) * read_kmers_batch.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([3232, 50, 5])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "read_50mer_batch = split_kmer_batch_into_50mers(read_kmers_batch)\n",
    "read_50mer_batch.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TFRecord based inference datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this pipeline, the steps are:\n",
    "\n",
    "1. Go from FASTQ and ALN to a RFRecord file and a metadata file with `tfrecord_from_fastq` or `tfrecord_from_text`\n",
    "2. Create a `tf.data.TFRecordDataset` from the saved TFRecord file\n",
    "3. Transform it into an inference/training dataset with `.map` and `tfr_to_tensors`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def _bytes_feature(value):\n",
    "    \"\"\"Returns a bytes_list from a string / byte.\"\"\"\n",
    "    if isinstance(value, type(tf.constant(0))): # if value is tensor\n",
    "        value = value.numpy() # get value of tensor\n",
    "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
    "\n",
    "def _float_feature(value):\n",
    "  \"\"\"Returns a floast_list from a float / double.\"\"\"\n",
    "  return tf.train.Feature(float_list=tf.train.FloatList(value=[value]))\n",
    "\n",
    "def _int64_feature(value):\n",
    "  \"\"\"Returns an int64_list from a bool / enum / int / uint.\"\"\"\n",
    "  return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\n",
    "\n",
    "def _serialize_array(array):\n",
    "  array = serialize_tensor(array)\n",
    "  return array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def _base_hot_encode(\n",
    "    line: str        # one string (one line in text dataset)\n",
    "    ):\n",
    "    \"\"\"Convert a line from text dataset into three tensors: read sequence (BHE), virus label and position\"\"\"\n",
    "    \n",
    "    # Split the line (string) in three : read, label, position\n",
    "    t = tf.strings.split(line.replace('\\n', ''), '\\t')\n",
    "\n",
    "    # Split the sequence string into a list of single base strings:\n",
    "    # 'TCAAAATAATCA' -> ['T','C','A','A','A','A','T','A','A','T','C','A']\n",
    "    read = tf.strings.bytes_split(t[0])\n",
    "\n",
    "    # Base Hot Encode sequences (BHE)\n",
    "    # Each base letter (A, C, G, T, N) is replaced by a OHE vector\n",
    "    #     \"A\" converted into [1,0,0,0,0]\n",
    "    #     \"C\" converted into [0,1,0,0,0]\n",
    "    #     \"G\" converted into [0,0,1,0,0]\n",
    "    #     \"T\" converted into [0,0,0,1,0]\n",
    "    #     \"N\" converted into [0,0,0,0,1]\n",
    "    \n",
    "    # Decode the base letters A, C, ... into their ASCII code for easy conversion into BHE\n",
    "    # ASCII code for A, C, G, T and N:\n",
    "    A, C, G, T, N = 65, 67, 71, 84, 78\n",
    "    read_uint8 = tf.io.decode_raw(read, out_type=tf.uint8)\n",
    "\n",
    "    # Technical Notes: \n",
    "    #   tf.io.decode_raw adds one dimension at the end in the process\n",
    "    #   [b'C', b'A', b'T'] will return [[67], [65], [84]] and not [67, 65, 84]\n",
    "    #   this is actually what we want to contatenate the values for each base letter\n",
    "    read_A = tf.cast(read_uint8 == A, tf.float32)\n",
    "    read_C = tf.cast(read_uint8 == C, tf.float32)\n",
    "    read_G = tf.cast(read_uint8 == G, tf.float32)\n",
    "    read_T = tf.cast(read_uint8 == T, tf.float32)\n",
    "    read_N = tf.cast(read_uint8 == N , tf.float32)\n",
    "    x_reads = tf.concat([read_A, read_C, read_G, read_T, read_N], axis=1)\n",
    "\n",
    "    # OHE labels\n",
    "    n_labels = 187\n",
    "    y_labels = tf.strings.to_number(t[1], out_type=tf.int32) # int32 so it can be used an index in gather\n",
    "    y_labels = tf.gather(tf.eye(n_labels, dtype=tf.float32), y_labels)\n",
    "\n",
    "    # OHE positions\n",
    "    n_pos = 10\n",
    "    y_pos = tf.strings.to_number(t[2], out_type=tf.int32) # int32 so it can be used an index in gather\n",
    "    y_pos= tf.gather(tf.eye(n_pos, dtype=tf.float32), y_pos)\n",
    "\n",
    "    return x_reads, y_labels, y_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "def split_kmer_into_50mers(\n",
    "    kmer: tf.Tensor        # tensor representing a single k-mer read, after base\n",
    "    ):\n",
    "    \"\"\"Converts a k-mer read into several 50-mer reads, by shifting the k-mer one base at a time.\"\"\"\n",
    "    def fn(accumulated, elem):\n",
    "        return tf.roll(accumulated, shift=-elem, axis=0)\n",
    "\n",
    "    k, n = kmer.shape[0], 50\n",
    "    nb_splits = (k-n+1)\n",
    "    # Create a tensor of integers, with a 0 as first element and 1 for all other elements, for shifts\n",
    "    shifts = np.ones(shape=nb_splits, dtype=int)\n",
    "    shifts[0] = 0 \n",
    "    shifts = tf.convert_to_tensor(shifts, dtype=tf.int64)\n",
    "    # Use tf.scan to shift the original read nb_splits times\n",
    "    shifted = tf.scan(fn, shifts, kmer, reverse=False)\n",
    "    return shifted[:, :n, :]  # return the tensor with shifted kmer, sliced to only keep 50 bases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "def base_string_kmers_to_tensors(\n",
    "    b: tf.Tensor,   # batch of strings \n",
    "    k: int = 50     # maximum read length in the batch\n",
    "    ):\n",
    "    \"\"\"Function converting a batch of bp strings into three tensors: (x_seqs, (y_labels, y_pos))\"\"\"\n",
    "    \n",
    "    # Split the string in three: \n",
    "    # returns a ragged tensor which needs to be converted into a normal tensor using .to_tensor()\n",
    "    t = tf.strings.split(b, '\\t').to_tensor(default_value = '', shape=[None, 3])\n",
    "\n",
    "    # Split each sequence string into a list of single base strings:\n",
    "    # 'TCAAAATAATCA' -> ['T','C','A','A','A','A','T','A','A','T','C','A']\n",
    "    seqs = tf.strings.bytes_split(t[:, 0]).to_tensor(shape=(None, k))\n",
    "\n",
    "\n",
    "    # BHE sequences\n",
    "    # Each base letter (A, C, G, T, N) is replaced by a OHE vector\n",
    "    #     \"A\" converted into [1,0,0,0,0]\n",
    "    #     \"C\" converted into [0,1,0,0,0]\n",
    "    #     \"G\" converted into [0,0,1,0,0]\n",
    "    #     \"T\" converted into [0,0,0,1,0]\n",
    "    #     \"N\" converted into [0,0,0,0,1]\n",
    "    # \n",
    "    # Technical Notes:\n",
    "    # a. The batch of sequence `seqs` has a shape (batch_size, 50) after splitting each byte. \n",
    "    #    Must flatten it first, then apply the transform on each base, then reshape to original shape\n",
    "    # b. We need to map each letter to one vector/tensor. \n",
    "    #    1. Cast bytes seqs into integer sequence (uint8 to work byte by byte)\n",
    "    #    2. For each base letter (A, C, G, T, N) create one tensor (batch_size, 50) (seqs_A, _C, _G, _T, _N)\n",
    "    #    3. Value is 1 if it is the base in the sequence, otherwise 0\n",
    "    #    4. Concatenate these 5 tensors into a tensor of shape (batch_size, 50, 5)\n",
    " \n",
    "    seqs_uint8 = tf.io.decode_raw(seqs, out_type=tf.uint8)\n",
    "    # note: tf.io.decode_raw adds one dimension at the end in the process\n",
    "    #       [b'C', b'A', b'T'] will return [[67], [65], [84]] and not [67, 65, 84]\n",
    "    #       this is actually what we want to contatenate the values for each base letter\n",
    "\n",
    "    A, C, G, T, N = 65, 67, 71, 84, 78\n",
    "\n",
    "    seqs_A = tf.cast(seqs_uint8 == A, tf.float32)\n",
    "    seqs_C = tf.cast(seqs_uint8 == C, tf.float32)\n",
    "    seqs_G = tf.cast(seqs_uint8 == G, tf.float32)\n",
    "    seqs_T = tf.cast(seqs_uint8 == T, tf.float32)\n",
    "    seqs_N = tf.cast(seqs_uint8 == N , tf.float32)\n",
    "\n",
    "    x_seqs = tf.concat([seqs_A, seqs_C, seqs_G, seqs_T, seqs_N], axis=2)\n",
    "\n",
    "    # OHE labels\n",
    "    n_labels = 187\n",
    "    y_labels = tf.strings.to_number(t[:, 1], out_type=tf.int32)\n",
    "    y_labels = tf.gather(tf.eye(n_labels), y_labels)\n",
    "\n",
    "    # OHE positions\n",
    "    n_pos = 10\n",
    "    y_pos = tf.strings.to_number(t[:, 2], out_type=tf.int32)\n",
    "    y_pos= tf.gather(tf.eye(n_pos), y_pos)\n",
    "\n",
    "    return (x_seqs, (y_labels, y_pos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "def split_kmer_batch_into_50mers(\n",
    "    kmer: tf.Tensor        # tensor representing a batch of k-mer reads, after base encoding\n",
    "    ):\n",
    "    \"\"\"Converts a batch of k-mer reads into several 50-mer reads, by shifting the k-mer one base at a time.\n",
    "\n",
    "    for a batch of `b` k-mer reads, returns a batch of `b - 49` 50-mer reads\n",
    "    \"\"\"\n",
    "    def fn(accumulated, elem):\n",
    "        return tf.roll(accumulated, shift=-elem, axis=1)\n",
    "\n",
    "    b = kmer.shape[0]\n",
    "    k = kmer.shape[1]\n",
    "    n = k - 49\n",
    "\n",
    "    # Create a tensor of integers, with a 0 as first element and 1 for all other elements, for shifts\n",
    "    shifts = tf.convert_to_tensor([0] + [1] * (n-1))\n",
    "\n",
    "    # Use tf.scan to shift the original read nb_splits times\n",
    "    shifted = tf.scan(fn, shifts, kmer, reverse=False)\n",
    "    # print(shifted.shape)\n",
    "\n",
    "    # de-interlace the 50-reads\n",
    "    indices = [(read_nb, batch_nb) for batch_nb in range(b) for read_nb in range(n)]\n",
    "    shifted = tf.gather_nd(shifted[:, :, :50, :], indices)\n",
    "    # print(shifted.shape)\n",
    "    \n",
    "    # return shifted[:, :50, :]  # return the tensor with shifted kmer, sliced to only keep 50 bases\n",
    "    return shifted  # return the tensor with shifted kmer, sliced to only keep 50 bases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def tfrecord_from_fastq(\n",
    "    p2fastq:Path,              # Path to the fastaq file (should be associated with a aln file)\n",
    "    p2tfrds:Path|None=None,    # Path to the TFRecord file, default creates a file in saved directory\n",
    "    overwrite:bool=False       # When True, overides any existing file, When False, raises an error\n",
    "    ) -> (Path, Path):         # Paths to the saved TFRecord file and the metadata csv file\n",
    "    \"\"\"Creates a TFRecord dataset for inference from fastq and aln files, as well as a csv metadata file\n",
    "\n",
    "    The TFRecord dataset can be used for training or prediction, using the original CNN Virus model.\n",
    "    The metadata file is a Pandas DataFrame converted into csv\n",
    "    \"\"\"\n",
    "    # Setup paths\n",
    "    if p2tfrds is None:\n",
    "        p2tfrds = ProjectFileSystem().data / 'saved/cnn_virus_datasets' / f\"{p2fastq.stem}.tfrecords\"\n",
    "    p2metadata = p2tfrds.parent / f\"{p2tfrds.stem}.metadata\"\n",
    "\n",
    "    if p2tfrds.exists():\n",
    "        if overwrite:\n",
    "            p2tfrds.unlink()\n",
    "            if p2metadata.exists(): p2metadata.unlink()\n",
    "        else: \n",
    "            raise ValueError(f\"{p2tfrds.name} already exists. To overwrite, set parameter `overwrite` to True\")\n",
    "\n",
    "    p2aln = p2fastq.parent / f\"{p2fastq.stem}.aln\"\n",
    "    assert p2aln.is_file(), f\"No ALN file associated with {fastq.name}\"\n",
    "\n",
    "    # Create file readers\n",
    "    fastq = FastqFileReader(p2fastq)\n",
    "    aln = AlnFileReader(p2aln)\n",
    "\n",
    "    read_ids, read_refseqs, read_start_pos, read_strand = [], [], [], []\n",
    "\n",
    "    # Create TFRecord writer and loop through reads\n",
    "    writer = tf.io.TFRecordWriter(str(p2tfrds.absolute())) \n",
    "    for i, (fastq_element, aln_element) in tqdm(enumerate(zip(fastq, aln))):\n",
    "        # Extract read text sequence from fastq and metadata from aln files\n",
    "        seq = fastq_element['sequence']           \n",
    "        aln_meta = aln.parse_text(aln_element['definition line'])\n",
    "        read_ids.append(aln_meta['readid'])\n",
    "        read_refseqs.append(aln_meta['refseqid'])\n",
    "        read_start_pos.append(aln_meta['aln_start_pos'])\n",
    "        read_strand.append(aln_meta['refseq_strand'])\n",
    "\n",
    "        # Create and write one Example, including BHE sequence, the label and the position\n",
    "        bhe_seq, label, pos = _base_hot_encode(f\"{seq}\\t{0}\\t{0}\\n\")\n",
    "        data = {\n",
    "            'read' : _bytes_feature(_serialize_array(bhe_seq)),\n",
    "            'label' : _bytes_feature(_serialize_array(label)),\n",
    "            'pos' : _bytes_feature(_serialize_array(pos))\n",
    "        }\n",
    "        out = tf.train.Example(features=tf.train.Features(feature=data))\n",
    "        writer.write(out.SerializeToString())\n",
    "\n",
    "    writer.close()\n",
    "    print(f\"Wrote {i+1} reads to TFRecord file {p2tfrds.name}\")\n",
    "\n",
    "    metadata = np.array(list(zip(read_ids, read_refseqs, read_start_pos, read_strand)))\n",
    "    metadata = pd.DataFrame(data={\n",
    "                'read_ids': read_ids,\n",
    "                'read_refseqs': read_refseqs,\n",
    "                'read_start_pos': read_start_pos,\n",
    "                'read_strand': read_strand})\n",
    "    metadata.to_csv(p2metadata, index=True)\n",
    "    \n",
    "    return p2tfrds, p2metadata, metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def tfrecord_from_text(\n",
    "    p2ds,                      # Path to the text dataset, in the format of original CNN Virus data\n",
    "    p2tfrds:Path|None=None,    # Path to the TFRecord file, default creates a file in savec directory\n",
    "    overwrite:bool=False       # When True, overides any existing file, When False, raises an error\n",
    "    ) -> Path:                 # Path to the saved TFRecord file\n",
    "    # Setup paths\n",
    "    if p2tfrds is None:\n",
    "        p2tfrds = ProjectFileSystem().data / 'saved/cnn_virus_datasets' / f\"{p2ds.stem}.tfrecords\"\n",
    "    # p2metadata = p2tfrds.parent / f\"{p2tfrds.stem}.metadata\"\n",
    "\n",
    "    if p2tfrds.exists():\n",
    "        if overwrite:\n",
    "            p2tfrds.unlink()\n",
    "            # if p2metadata.exists(): p2metadata.unlink()\n",
    "        else: \n",
    "            raise ValueError(f\"{p2tfrds.name} already exists. To overwrite, set parameter `overwrite` to True\")\n",
    "\n",
    "    reads = TextFileBaseReader(p2ds, nlines=1)\n",
    "    writer = tf.io.TFRecordWriter(str(p2tfrds.absolute())) \n",
    "\n",
    "    for i, line in enumerate(reads):\n",
    "        # Create and write one Example, including BHE sequence, the label and the position\n",
    "        bhe_seq, label, pos = _base_hot_encode(line)\n",
    "        data = {\n",
    "            'read' : _bytes_feature(_serialize_array(bhe_seq)),\n",
    "            'label' : _bytes_feature(_serialize_array(label)),\n",
    "            'pos' : _bytes_feature(_serialize_array(pos))\n",
    "            }\n",
    "        out = tf.train.Example(features=tf.train.Features(feature=data))\n",
    "        writer.write(out.SerializeToString())\n",
    "\n",
    "    writer.close()\n",
    "    print(f\"Wrote {i+1} reads to TFRecord\")\n",
    "    return p2tfrds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def _parse_tfr_element(element):\n",
    "    # Define the underlying structure of the data (mirror the dta structure above)\n",
    "    data = {    \n",
    "        'read' : FixedLenFeature([], tf.string),\n",
    "        'label' : FixedLenFeature([], tf.string),\n",
    "        'pos' : FixedLenFeature([], tf.string) \n",
    "    }\n",
    "\n",
    "    content = tf.io.parse_single_example(element, data)\n",
    "  \n",
    "    read_bytes = content['read']\n",
    "    label_bytes = content['label']\n",
    "    pos_bytes = content['pos']\n",
    "    \n",
    "    # Parse the string tensor into a real tensors, with proper types\n",
    "    read = tf.io.parse_tensor(read_bytes, out_type=tf.float32)\n",
    "    label = tf.io.parse_tensor(label_bytes, out_type=tf.float32)\n",
    "    pos = tf.io.parse_tensor(pos_bytes, out_type=tf.float32)\n",
    "    \n",
    "    return (read, (label, pos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def get_dataset_from_tfr(\n",
    "    p2tfrds:Path,      # Path to the TFRecord dataset\n",
    "    batch_size:int = 1 # Desired batch side for the dataset\n",
    "    ) -> tf.data.Dataset: # dataset, formated with the batch size\n",
    "    # Create a dataset from the TFRecord file\n",
    "    dataset = tf.data.TFRecordDataset(p2tfrds)\n",
    "    # Convert the strings into the proper format using the parsing function\n",
    "    dataset = dataset.map(_parse_tfr_element)\n",
    "    return dataset.batch(batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a dataset from an existing TFRecord file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorSpec(shape=<unknown>, dtype=tf.float32, name=None),\n",
       " (TensorSpec(shape=<unknown>, dtype=tf.float32, name=None),\n",
       "  TensorSpec(shape=<unknown>, dtype=tf.float32, name=None)))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Check how to define shape of the elements\n",
    "p2ds = pfs.data / 'ncbi/ds/cov/single_1seq_50bp-10reads.tfrecords'\n",
    "assert p2ds.exists()\n",
    "ds = get_dataset_from_tfr(p2ds)\n",
    "ds.element_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 50, 5) (1, 187) (1, 10)\n",
      "(1, 50, 5) (1, 187) (1, 10)\n",
      "(1, 50, 5) (1, 187) (1, 10)\n",
      "(1, 50, 5) (1, 187) (1, 10)\n",
      "(1, 50, 5) (1, 187) (1, 10)\n",
      "(1, 50, 5) (1, 187) (1, 10)\n",
      "(1, 50, 5) (1, 187) (1, 10)\n",
      "(1, 50, 5) (1, 187) (1, 10)\n"
     ]
    }
   ],
   "source": [
    "for r, (l, p) in ds.take(8):\n",
    "    print(r.shape, l.shape, p.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can convert this dataset into a batch dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 1, 50, 5) (2, 1, 187) (2, 1, 10)\n",
      "(2, 1, 50, 5) (2, 1, 187) (2, 1, 10)\n",
      "(2, 1, 50, 5) (2, 1, 187) (2, 1, 10)\n",
      "(2, 1, 50, 5) (2, 1, 187) (2, 1, 10)\n",
      "(2, 1, 50, 5) (2, 1, 187) (2, 1, 10)\n"
     ]
    }
   ],
   "source": [
    "ds_batched = ds.batch(2)\n",
    "for r, (l, p) in ds_batched.take(8):\n",
    "    print(r.shape, l.shape, p.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Postprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "def combine_predictions(\n",
    "    labels:tf.Tensor,         # Label predictions for a set of 50-mer corresponding to a single k-mer\n",
    "    label_probs: tf.Tensor,   # Probabilities for the labels\n",
    "    positions: tf.Tensor      # Position predictions for a set of 50-mer corresponding to a single k-mer\n",
    "    ):\n",
    "    \"\"\"Combine set of 50-mer predictions into one final prediction for label and position\"\"\"\n",
    "\n",
    "    # Filter our any prediction with low predicted probability \n",
    "    valid_preds_mask = tf.reduce_max(label_probs, axis=1) >= 0.9\n",
    "    valid_labels = labels[valid_preds_mask].numpy()\n",
    "    valid_positions = positions[valid_preds_mask].numpy()\n",
    "\n",
    "    # Return prediction outside the label and position ranges if no valid prediction\n",
    "    if len(valid_labels) == 0:\n",
    "        return 187, 10\n",
    "\n",
    "    # Take most frequent label, and most frequent position for the selected label\n",
    "    else:\n",
    "        unique_labels, _, counts = tf.unique_with_counts(valid_labels)\n",
    "        combined_label = unique_labels[tf.argmax(counts)].numpy()\n",
    "\n",
    "        df = pd.DataFrame({'labels': valid_labels, 'positions': valid_positions})\n",
    "        gb = df.groupby('labels')\n",
    "        counter_pos = collections.Counter(df.positions.iloc[gb.groups[combined_label]].tolist())\n",
    "        combined_pos = counter_pos.most_common(1)[0][0]\n",
    "        return combined_label, combined_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "def combine_prediction_batch(\n",
    "    probs_elements: tuple[tf.Tensor, tf.Tensor]  # Tuple of label and position probabilities for a batch of 50-mer\n",
    "    ):\n",
    "    \"\"\"Combine a batch of 50-mer probabilities into one batch of final prediction for label and position\n",
    "\n",
    "    Note: the input must be reshape to (batch_size, k, n) where n is the number of labels or positions\n",
    "    \"\"\"\n",
    "\n",
    "    label_probs = probs_elements[0]\n",
    "    position_probs = probs_elements[1]\n",
    "\n",
    "    labels_preds = tf.argmax(label_probs, axis=1)\n",
    "    positions_preds = tf.argmax(position_probs, axis=1)\n",
    "\n",
    "    valid_labels_filter = tf.reduce_max(label_probs, axis=1) > 0.9\n",
    "    valid_labels_preds = labels_preds[valid_labels_filter]\n",
    "    \n",
    "    valid_positions_preds = positions_preds[valid_labels_filter]\n",
    "\n",
    "\n",
    "    if valid_labels_preds.shape[0] == 0:\n",
    "        combined_label = tf.constant(187, shape=(1,), dtype=tf.int64)\n",
    "        combined_position = tf.constant(10, shape=(1,), dtype=tf.int64)\n",
    "\n",
    "    else:\n",
    "        uniques, _, counts = tf.unique_with_counts(valid_labels_preds)\n",
    "        combined_label = uniques[tf.argmax(counts)]\n",
    "\n",
    "        # filter which reads give the majority label prediction\n",
    "        combined_label_filter = valid_labels_preds == combined_label\n",
    "\n",
    "        # pick the corresponding position predictions\n",
    "        filtered_positions = valid_positions_preds[combined_label_filter]\n",
    "        unique_positions, _, counts = tf.unique_with_counts(filtered_positions)\n",
    "        combined_position = unique_positions[tf.argmax(counts)]\n",
    "\n",
    "        combined_pred = tf.concat([combined_label, combined_position], axis=0)\n",
    "\n",
    "    return combined_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Original Code (Refactored)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selected classes and functions refactored from the original code, coming with the paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class DataGenerator_from_50mer(Sequence):\n",
    "    \"\"\"data generator for generating batches of data from 50-mers\"\"\"\n",
    "\n",
    "    d_nucl = {\"A\": 0,\"C\": 1,\"G\": 2,\"T\": 3,\"N\":4}\n",
    "\n",
    "    def __init__(self, f_matrix, f_labels, f_pos, batch_size=1024,n_classes=187, shuffle=True):\n",
    "        self.batch_size = batch_size\n",
    "        self.labels = f_labels\n",
    "        self.matrix = f_matrix\n",
    "        self.pos = f_pos\n",
    "        self.n_classes = n_classes\n",
    "        self.shuffle = shuffle\n",
    "        self.on_epoch_end()\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.labels) / self.batch_size))\n",
    "    def __getitem__(self, index):\n",
    "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "        X, y= self.__data_generation(indexes)\n",
    "        return X,y\n",
    "    def on_epoch_end(self):\n",
    "        self.indexes = np.arange(len(self.labels))\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)\n",
    "    def __data_generation(self, index):\n",
    "        x_train=[]\n",
    "        for i in index:\n",
    "            seq=self.matrix[i]\n",
    "            seq_list=[j for j in seq]\n",
    "            x_train.append(seq_list)\n",
    "        x_train=np.array(x_train)\n",
    "        x_tensor=np.zeros(list(x_train.shape)+[5])\n",
    "        for row in range(len(x_train)):\n",
    "            for col in range(50):\n",
    "                x_tensor[row,col,self.d_nucl[x_train[row,col]]]=1\n",
    "        y_pos=[]\n",
    "        y_label=[self.labels[i] for i in index]\n",
    "        y_label=np.array(y_label)\n",
    "        y_label=to_categorical(y_label, num_classes=self.n_classes)\n",
    "        y_pos=[self.pos[i] for i in index]\n",
    "        y_pos=np.array(y_pos)\n",
    "        y_pos=to_categorical(y_pos, num_classes=10)\n",
    "        return x_tensor,{'labels': y_label, 'pos': y_pos}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/vtecftwy/metagentools/blob/main/metagentools/cnn_virus/data.py#L917){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### DataGenerator_from_50mer\n",
       "\n",
       ">      DataGenerator_from_50mer (f_matrix, f_labels, f_pos, batch_size=1024,\n",
       ">                                n_classes=187, shuffle=True)\n",
       "\n",
       "data generator for generating batches of data from 50-mers"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/vtecftwy/metagentools/blob/main/metagentools/cnn_virus/data.py#L917){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### DataGenerator_from_50mer\n",
       "\n",
       ">      DataGenerator_from_50mer (f_matrix, f_labels, f_pos, batch_size=1024,\n",
       ">                                n_classes=187, shuffle=True)\n",
       "\n",
       "data generator for generating batches of data from 50-mers"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(DataGenerator_from_50mer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def get_learning_weights(filepath):\n",
    "    \"\"\"get different learning weights for different classes, from file\"\"\"\n",
    "    f = open(filepath,\"r\").readlines()\n",
    "    d_weights = {}\n",
    "    for i in f:\n",
    "        i = i.strip().split(\"\\t\")\n",
    "        d_weights[float(i[0])]=float(i[1])\n",
    "    return d_weights\n",
    "\n",
    "def get_params_50mer():\n",
    "    \"\"\"set default params for generating batches of 50-mer\"\"\"\n",
    "    params = {'batch_size': 1024,\n",
    "    'n_classes': 187,\n",
    "    'shuffle': True}\n",
    "    return params\n",
    "\n",
    "def get_params_150mer():\n",
    "    \"\"\" set default params for generating batches of 150-mer\"\"\"\n",
    "    params = {'batch_size': 101,\n",
    "    'n_classes': 187,\n",
    "    'shuffle': False}\n",
    "    return params\n",
    "\n",
    "def get_kmer_from_50mer(filepath, max_seqs=None):\n",
    "    \"\"\"Load data from sequence file and returns three tensors, with max nbr sequences\"\"\"\n",
    "    f_matrix=[]\n",
    "    f_labels=[]\n",
    "    f_pos=[]\n",
    "    with open(filepath, 'r') as fp:\n",
    "        i = 0\n",
    "        while True:\n",
    "            line = fp.readline()\n",
    "            i += 1\n",
    "            # EOF\n",
    "            if line == '':\n",
    "                break\n",
    "            # Reached max number of k-mers to load from file\n",
    "            elif max_seqs is not None and i > max_seqs:\n",
    "                break\n",
    "            else:\n",
    "                seq, label, pos = line.strip().split('\\t')\n",
    "                f_matrix.append(seq)\n",
    "                f_labels.append(label)\n",
    "                f_pos.append(pos)\n",
    "    return f_matrix,f_labels,f_pos\n",
    "\n",
    "def get_kmer_from_150mer(filepath, max_seqs=None):\n",
    "    \"\"\"Load data from sequence file and returns three tensors, with max nbr sequences\"\"\"\n",
    "    f_matrix=[]\n",
    "    f_labels=[]\n",
    "    f_pos=[]\n",
    "    with open(filepath,\"r\") as fp:\n",
    "        i = 0\n",
    "        while True:\n",
    "            line = fp.readline()\n",
    "            i += 1\n",
    "            # EOF\n",
    "            if line == '':\n",
    "                break\n",
    "            # Reached max number of k-mers to load from file\n",
    "            elif max_seqs is not None and i > max_seqs:\n",
    "                break\n",
    "            else:\n",
    "                seq, label, pos = line.strip().split('\\t')\n",
    "                # Split 150-mer into 101 50-mers, shifted by one nucleotide\n",
    "                for i in range(len(seq)-49):\n",
    "                    kmer=seq[i:i+50]\n",
    "                    f_matrix.append(kmer)\n",
    "                    f_labels.append(label)\n",
    "                    f_pos.append(pos)\n",
    "    return f_matrix,f_labels,f_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/vtecftwy/metagentools/blob/main/metagentools/cnn_virus/data.py#L984){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### get_kmer_from_50mer\n",
       "\n",
       ">      get_kmer_from_50mer (filepath, max_seqs=None)\n",
       "\n",
       "Load data from sequence file and returns three tensors, with max nbr sequences"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/vtecftwy/metagentools/blob/main/metagentools/cnn_virus/data.py#L984){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### get_kmer_from_50mer\n",
       "\n",
       ">      get_kmer_from_50mer (filepath, max_seqs=None)\n",
       "\n",
       "Load data from sequence file and returns three tensors, with max nbr sequences"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(get_kmer_from_50mer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/vtecftwy/metagentools/blob/main/metagentools/cnn_virus/data.py#L1007){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### get_kmer_from_150mer\n",
       "\n",
       ">      get_kmer_from_150mer (filepath, max_seqs=None)\n",
       "\n",
       "Load data from sequence file and returns three tensors, with max nbr sequences"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/vtecftwy/metagentools/blob/main/metagentools/cnn_virus/data.py#L1007){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### get_kmer_from_150mer\n",
       "\n",
       ">      get_kmer_from_150mer (filepath, max_seqs=None)\n",
       "\n",
       "Load data from sequence file and returns three tensors, with max nbr sequences"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(get_kmer_from_150mer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path for the training file\n",
    "filepath_50mer= pfs.data / 'CNN_Virus_data/50mer_ds_100_seq'\n",
    "filepath_150mer= pfs.data / 'CNN_Virus_data/150mer_ds_100_seq'\n",
    "assert filepath_50mer.is_file(), filepath_50mer\n",
    "assert filepath_150mer.is_file(), filepath_150mer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['AAAAAGATTTTGAGAGAGGTCGACCTGTCCTCCTAAAACGTTTACAAAAG',\n",
       " 'CATGTAACGCAGCTTAGTCCGATCGTGGCTATAATCCGTCTTTCGATTTG',\n",
       " 'AACAACATCTTGTTGATGATAACCGTCAAAGTGTTTTGGGTCTGGAGGGA',\n",
       " 'AGTACCTGGAGAGCGTTAAGAAACACAAACGGCTGGATGTAGTGCCGCGC',\n",
       " 'CCACGTCGATGAAGCTCCGACGAGAGTCGGCGCTGAGCCCGCGCACCTCC']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f_matrix,f_labels,f_pos = get_kmer_from_50mer(filepath_50mer, max_seqs=5)\n",
    "f_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['71', '1', '158', '6', '71'], ['0', '7', '6', '7', '6'])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f_labels, f_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CTACATGACCCTGACACTCAGCTACGAGATGTCAAATTTTGGGGGCAATG',\n",
       " 'TACATGACCCTGACACTCAGCTACGAGATGTCAAATTTTGGGGGCAATGA',\n",
       " 'ACATGACCCTGACACTCAGCTACGAGATGTCAAATTTTGGGGGCAATGAA',\n",
       " 'CATGACCCTGACACTCAGCTACGAGATGTCAAATTTTGGGGGCAATGAAA',\n",
       " 'ATGACCCTGACACTCAGCTACGAGATGTCAAATTTTGGGGGCAATGAAAG']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f_matrix,f_labels,f_pos = get_kmer_from_150mer(filepath_150mer, max_seqs=1)\n",
    "f_matrix[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['120', '120', '120', '120', '120'], ['3', '3', '3', '3', '3'])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f_labels[:5], f_pos[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "26b205197a934fdeabb71e65ac11acba": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "514ad0bfcabf4df580a9a872af814af9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "55646397fc9349d3af9e98b1f2b26f5d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "70f6be4247664b708b662c34e7abe3ee": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7179c6cc207941648c348b1bf10cb87f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "LabelModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "LabelModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "LabelView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_70f6be4247664b708b662c34e7abe3ee",
      "placeholder": "",
      "style": "IPY_MODEL_bdde467d943148ce9bb6355fd7582a5c",
      "value": "0.078 MB of 0.078 MB uploaded (0.020 MB deduped)\r"
     }
    },
    "7849f255e99b4853bdba7f8badf1054a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "VBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "VBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "VBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_7179c6cc207941648c348b1bf10cb87f",
       "IPY_MODEL_e0819a1ddcc64c08a748a2fd88350f09"
      ],
      "layout": "IPY_MODEL_26b205197a934fdeabb71e65ac11acba"
     }
    },
    "bdde467d943148ce9bb6355fd7582a5c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "e0819a1ddcc64c08a748a2fd88350f09": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_55646397fc9349d3af9e98b1f2b26f5d",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_514ad0bfcabf4df580a9a872af814af9",
      "value": 1
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
