{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference using simulated CoV sequences from NCBI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we use simreads generated from NCBI CoV reference sequences to test inference, using the original pretrained CNN Virus model.\n",
    "\n",
    "This notebook works when run locally and also should run on Colab, as long as the file system is in line with the unified file ystem (see documentation)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Imports and setup environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install and import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "`ecutilities` already installed\n",
      "`metagentools` already installed\n"
     ]
    }
   ],
   "source": [
    "# Install required custom packages if not installed yet.\n",
    "import importlib.util\n",
    "if not importlib.util.find_spec('ecutilities'):\n",
    "    print('installing package: `ecutilities`')\n",
    "    ! pip install -qqU ecutilities\n",
    "else:\n",
    "    print('`ecutilities` already installed')\n",
    "if not importlib.util.find_spec('metagentools'):\n",
    "    print('installing package: `metagentools')\n",
    "    ! pip install -qqU metagentools\n",
    "else:\n",
    "    print('`metagentools` already installed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "Set autoreload mode\n",
      "Tensorflow version: 2.8.2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import all required packages\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "from ecutilities.core import files_in_tree\n",
    "from ecutilities.ipython import nb_setup\n",
    "from IPython.display import display, Markdown, HTML\n",
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "from tqdm.notebook import tqdm, trange\n",
    "\n",
    "# Setup the notebook for development\n",
    "nb_setup()\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' # or any {'0', '1', '2'}\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.client import device_lib\n",
    "from tensorflow.keras.models import load_model\n",
    "print(f\"Tensorflow version: {tf.__version__}\\n\")\n",
    "\n",
    "from metagentools.cnn_virus.data import strings_to_tensors, create_infer_ds_from_fastq\n",
    "from metagentools.cnn_virus.data import FastaFileReader, FastqFileReader, AlnFileReader\n",
    "from metagentools.core import TextFileBaseReader, ProjectFileSystem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List all computing devices available on the machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Devices:\n",
      "  - CPU  /device:CPU:0                          \n",
      "  - GPU  /device:GPU:0  NVIDIA GeForce GTX 1050 \n"
     ]
    }
   ],
   "source": [
    "devices = device_lib.list_local_devices()\n",
    "print('\\nDevices:')\n",
    "for d in devices:\n",
    "    t = d.device_type\n",
    "    name = d.physical_device_desc\n",
    "    l = [item.split(':', 1) for item in name.split(', ')]\n",
    "    name_attr = dict([x for x in l if len(x)==2])\n",
    "    dev = name_attr.get('name', ' ')\n",
    "    print(f\"  - {t}  {d.name} {dev:25s}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Setup paths to files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Key folders and system information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running linux on local computer\n",
      "Device's home directory: /home/vtec\n",
      "Project file structure:\n",
      " - Root ........ /home/vtec/projects/bio/metagentools \n",
      " - Data Dir .... /home/vtec/projects/bio/metagentools/data \n",
      " - Notebooks ... /home/vtec/projects/bio/metagentools/nbs\n"
     ]
    }
   ],
   "source": [
    "pfs = ProjectFileSystem()\n",
    "pfs.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "*/home/vtec/projects/bio/metagentools/data*"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "This directory includes all the data for the project `metagentools`.\n",
       "\n",
       "```text\n",
       "data\n",
       "     |--- CNN_Virus_data  (all data related to CNN Virus original paper)\n",
       "     |--- models          (trained and finetuned models)\n",
       "     |--- ncbi            (refsequences, simreads, datasets and infer_results for CoV from NCBI)\n",
       "     |--- ncov_data       (refsequences, simreads, datasets and infer_results for non CoV sequences)\n",
       "     |--- ....            (raw and pre-processed data from various sources)  \n",
       "     \n",
       "```\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pfs.readme()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `p2model`: path to file with saved original pretrained model\n",
    "- `p2virus_labels` path to file with virus names and labels mapping for original model\n",
    "- `p2simreads`: path to folder where reads files are located (FASTQ and ALN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "p2model = pfs.data / 'models/cnn_virus_original/pretrained_model.h5'\n",
    "assert p2model.is_file(), f\"No file found at {p2model.absolute()}\"\n",
    "\n",
    "p2virus_labels = pfs.data / 'CNN_Virus_data/virus_name_mapping'\n",
    "assert p2virus_labels.is_file(), f\"No file found at {p2virus_labels.absolute()}\"\n",
    "\n",
    "p2simreads = pfs.data / 'ncbi/simreads/'\n",
    "assert p2simreads.is_dir(), f\"No directory found at {p2simreads.absolute()}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "*/home/vtec/projects/bio/metagentools/data/ncbi/simreads*"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "# Information on CoV simulated read sequence files\n",
       "This folder includes a set of simulated read sequence files generated from CoV sequences in `cov_data` and ARC Illumina. \n",
       "\n",
       "\n",
       "Each simread sub-directory is named as `<method>_<nb seq>_<nb bp>` where\"\n",
       "- `<method>` is either `single` or `paired` depending on the simulation method\n",
       "- `<nb seq>` is the number of reference sequences used for simulation, and refers to the `fa` files in `cov_data`\n",
       "- `<nb bp>` is the number of base pairs in the simulated read\n",
       "\n",
       "Each directory includes simreads files made using a simulation method and a specific number of reference sequences.\n",
       "- `xxx.fq` and `xxx.aln` files with method is `single`\n",
       "- `xxx1.fq`, `xxx2.fq`, `xxx1.aln` and `xxx2.aln` files with method is `paired`.\n",
       "\n",
       "Example:\n",
       "- `paired_10seq_50bp` means that the simreads were generated by using the `paired` method to simulate 50-bp reads, and using the `fa` file `/cov_data/cov_virus_sequences_010-seqs.fa`\n",
       "- `single_100seq_50bp` means that the simreads were generated by using the `single` method to simulate 50-bp reads, and using the `fa` file `/cov_data/cov_virus_sequences_100-seqs.fa`. Note that this generated 20,660,104 reads !\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pfs.readme(dir_path=p2simreads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ncbi\n",
      "  |--simreads\n",
      "  |    |--readme.md (0)\n",
      "  |    |--paired_25seq_150bp\n",
      "  |    |    |--paired_25seq_150bp2.fq (1)\n",
      "  |    |    |--paired_25seq_150bp1.aln (2)\n",
      "  |    |    |--paired_25seq_150bp2.aln (3)\n",
      "  |    |    |--paired_25seq_150bp1.fq (4)\n",
      "  |    |--paired_100seq_150bp\n",
      "  |    |    |--paired_100seq_150bp1.aln (5)\n",
      "  |    |    |--paired_100seq_150bp2.aln (6)\n",
      "  |    |    |--paired_100seq_150bp1.fq (7)\n",
      "  |    |    |--paired_100seq_150bp2.fq (8)\n",
      "  |    |--paired_100seq_50bp\n",
      "  |    |    |--paired_100seq_50bp2.aln (9)\n",
      "  |    |    |--paired_100seq_50bp1.aln (10)\n",
      "  |    |    |--paired_100seq_50bp2.fq (11)\n",
      "  |    |    |--paired_100seq_50bp1.fq (12)\n",
      "  |    |--paired_10seq_150bp\n",
      "  |    |    |--paired_10seq_150bp1.aln (13)\n",
      "  |    |    |--paired_10seq_150bp2.fq (14)\n",
      "  |    |    |--paired_10seq_150bp2.aln (15)\n",
      "  |    |    |--paired_10seq_150bp1.fq (16)\n",
      "  |    |--paired_10seq_50bp\n",
      "  |    |    |--paired_10seq_50bp2.aln (17)\n",
      "  |    |    |--paired_10seq_50bp1.fq (18)\n",
      "  |    |    |--paired_10seq_50bp1.aln (19)\n",
      "  |    |    |--paired_10seq_50bp2.fq (20)\n",
      "  |    |--single_25seq_50bp\n",
      "  |    |    |--single_25seq_50bp.aln (21)\n",
      "  |    |    |--single_25seq_50bp.fq (22)\n",
      "  |    |--single_10seq_50bp\n",
      "  |    |    |--single_10seq_50bp.fq (23)\n",
      "  |    |    |--single_10seq_50bp.aln (24)\n",
      "  |    |--single_25seq_150bp\n",
      "  |    |    |--single_25seq_150bp.fq (25)\n",
      "  |    |    |--single_25seq_150bp.aln (26)\n",
      "  |    |--single_10seq_150bp\n",
      "  |    |    |--single_10seq_150bp.fq (27)\n",
      "  |    |    |--single_10seq_150bp.aln (28)\n",
      "  |    |--single_100seq_50bp\n",
      "  |    |    |--single_100seq_50bp.fq (29)\n",
      "  |    |    |--single_100seq_50bp.aln (30)\n",
      "  |    |--single_100seq_150bp\n",
      "  |    |    |--single_100seq_150bp.fq (31)\n",
      "  |    |    |--single_100seq_150bp.aln (32)\n",
      "  |    |--paired_25seq_50bp\n",
      "  |    |    |--paired_25seq_50bp1.fq (33)\n",
      "  |    |    |--paired_25seq_50bp2.aln (34)\n",
      "  |    |    |--paired_25seq_50bp2.fq (35)\n",
      "  |    |    |--paired_25seq_50bp1.aln (36)\n"
     ]
    }
   ],
   "source": [
    "files_in_tree(path=p2simreads);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this experiment, we will use the follwoing simreads:\n",
    "- 50 bp, single\n",
    "- from 10 reference sequences\n",
    "\n",
    "This means we will use the files in `data/ncbi/simreads/single_10seq_50bp`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/home/vtec/projects/bio/metagentools/data/ncbi/simreads/single_10seq_50bp')"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p2simreads = pfs.data / 'ncbi/simreads' / 'single_10seq_50bp'\n",
    "assert p2simreads.is_dir()\n",
    "p2simreads.absolute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " fq reads file:  single_10seq_50bp.fq\n",
      " aln reads file: single_10seq_50bp.aln\n"
     ]
    }
   ],
   "source": [
    "p2fastq = p2simreads / f\"{p2simreads.stem}.fq\"\n",
    "assert p2fastq.is_file()\n",
    "p2aln = p2simreads / f\"{p2simreads.stem}.aln\"\n",
    "assert p2aln.is_file()\n",
    "\n",
    "print(f\" fq reads file:  {p2fastq.name}\\n aln reads file: {p2aln.name}\")\n",
    "\n",
    "fastq = FastqFileReader(p2fastq)\n",
    "aln = AlnFileReader(p2aln)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ART Illumina command to generate the reads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/art_illumina -i /home/vtec/projects/bio/metagentools/data/cov_data/cov_virus_sequences_ten.fa -ss HS25 -l 50 -f 100 -o /home/vtec/projects/bio/metagentools/data/cov_simreads/single_10seq_50bp/single_10seq_50bp -rs 1674660835\n"
     ]
    }
   ],
   "source": [
    "print(aln.header['command'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference Sequences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coronavirus BtRs-BetaCoV/YN2018D  scientific name\n",
      "Bovine coronavirus  scientific name\n",
      "Human coronavirus OC43  scientific name\n",
      "Human coronavirus NL63  scientific name\n",
      "Infectious bronchitis virus  scientific name\n",
      "Porcine epidemic diarrhea virus  scientific name\n",
      "Porcine epidemic diarrhea virus  scientific name\n",
      "Porcine epidemic diarrhea virus  scientific name\n",
      "Porcine epidemic diarrhea virus  scientific name\n",
      "Camel alphacoronavirus  scientific name\n"
     ]
    }
   ],
   "source": [
    "print('\\n'.join([v['species'] for v in aln.ref_sequences.values()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Create inference dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model expect a dataset file in the following format:\n",
    "\n",
    "```text\n",
    "    AAAAAGATTTTGAGAGAGGTCGACCTGTCCTCCTAAAACGTTTACAAAAG\n",
    "    CATGTAACGCAGCTTAGTCCGATCGTGGCTATAATCCGTCTTTCGATTTG\n",
    "    AACAACATCTTGTTGATGATAACCGTCAAAGTGTTTTGGGTCTGGAGGGA\n",
    "    AGTACCTGGAGAGCGTTAAGAAACACAAACGGCTGGATGTAGTGCCGCGC\n",
    "    CCACGTCGATGAAGCTCCGACGAGAGTCGGCGCTGAGCCCGCGCACCTCC\n",
    "```\n",
    "\n",
    "Each line corresponds to a **read sequence**. During inference, the model will predict the **virus species code** and the **relative position** of the read in the full reference sequence.\n",
    "\n",
    "The mapping between code and virus specie name are in the file `virus_labels.csv`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a ds file from the simulated read output files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function `create_infer_ds_from_fastq` takes reads from a `fastq` file and created an inference dataset in the format expected by the model. \n",
    "\n",
    "The function returns the path to the inference dataset file, as well as a DataFrame with all refseq metadata. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Datasets are saved into `data/ds`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "p2datasets = pfs.data / 'ncbi/ds'\n",
    "assert p2datasets.is_dir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "*/home/vtec/projects/bio/metagentools/data/ncbi/ds*"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "# Dataset Directory\n",
       "When using simread files (`fa` and `aln`) for inference, an inference dataset in a format required by the CNN Virus model must be build. In addition, metadata can be extracted to make it possible to analyse the result from different perspectives.\n",
       "\n",
       "This directory includes the generated  inference datasets and metadata for each inference experiment.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pfs.readme(dir_path=p2datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "single_25seq_50bp_metadata.csv\n",
      "single_10seq_50bp_metadata.csv\n",
      "single_25seq_50bp_ds\n",
      "single_10seq_50bp_ds\n"
     ]
    }
   ],
   "source": [
    "print('\\n'.join([p.name for p in p2datasets.glob('*') if p.suffix not in ['.md']]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the dataset and the metadata file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1522a8c8088148c4984d80b73ce3843b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset with 571,980 reads\n"
     ]
    }
   ],
   "source": [
    "nsamples = None\n",
    "\n",
    "p2ds, p2meta, reads_info = create_infer_ds_from_fastq(\n",
    "    p2fastq=p2fastq, \n",
    "    output_dir=p2datasets,\n",
    "    overwrite_ds=True, \n",
    "    nsamples=nsamples\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to inference dataset file: /home/vtec/projects/bio/metagentools/data/ncbi/ds/single_10seq_50bp_ds\n",
      "Path to read metadata file:     /home/vtec/projects/bio/metagentools/data/ncbi/ds/single_10seq_50bp_metadata.csv\n"
     ]
    }
   ],
   "source": [
    "print(f\"Path to inference dataset file: {p2ds.absolute()}\")\n",
    "print(f\"Path to read metadata file:     {p2meta.absolute()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>read_ids</th>\n",
       "      <th>read_refseqs</th>\n",
       "      <th>read_start_pos</th>\n",
       "      <th>read_strand</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2591237:ncbi:1-60400</td>\n",
       "      <td>2591237:ncbi:1</td>\n",
       "      <td>14770</td>\n",
       "      <td>+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2591237:ncbi:1-60399</td>\n",
       "      <td>2591237:ncbi:1</td>\n",
       "      <td>17012</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2591237:ncbi:1-60398</td>\n",
       "      <td>2591237:ncbi:1</td>\n",
       "      <td>9188</td>\n",
       "      <td>+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2591237:ncbi:1-60397</td>\n",
       "      <td>2591237:ncbi:1</td>\n",
       "      <td>6764</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2591237:ncbi:1-60396</td>\n",
       "      <td>2591237:ncbi:1</td>\n",
       "      <td>27357</td>\n",
       "      <td>+</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               read_ids    read_refseqs read_start_pos read_strand\n",
       "0  2591237:ncbi:1-60400  2591237:ncbi:1          14770           +\n",
       "1  2591237:ncbi:1-60399  2591237:ncbi:1          17012           -\n",
       "2  2591237:ncbi:1-60398  2591237:ncbi:1           9188           +\n",
       "3  2591237:ncbi:1-60397  2591237:ncbi:1           6764           -\n",
       "4  2591237:ncbi:1-60396  2591237:ncbi:1          27357           +"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reads_info.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the data loader for the model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define batch size and create a first dataset accessing data from the dataset text file. Batch size can be adjusted depending on the memory available on the GPU. For reference, `bs = 4096` was used with a 4GB GPU. \n",
    "\n",
    "Then transform the text dataset into a tensor dataset by applying the `string_to_tensor` preprocessing function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 4096\n",
    "\n",
    "text_ds = tf.data.TextLineDataset(p2ds).batch(bs)\n",
    "ds = text_ds.map(strings_to_tensors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The bases in the read sequences are encoded as a 5-dim one-hot-encoded vector, as the model expects.\n",
    "\n",
    "In this example, each 50bp read in converted into a tensor of shape [50,5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4096, 50, 5)\n",
      "tf.Tensor(\n",
      "[[1. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [1. 0. 0. 0. 0.]], shape=(10, 5), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "for batch, (y1b, y2b) in ds.take(1):\n",
    "    # show the shape of one batch\n",
    "    print(batch.shape)\n",
    "    # show the forst 10 bases, after one-hot-endoding\n",
    "    print(batch[0, :10, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load and review the pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model(p2model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 50, 5)]      0           []                               \n",
      "                                                                                                  \n",
      " conv1d_1 (Conv1D)              (None, 50, 512)      13312       ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " batch_normalization_1 (BatchNo  (None, 50, 512)     2048        ['conv1d_1[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " max_pooling1d_1 (MaxPooling1D)  (None, 25, 512)     0           ['batch_normalization_1[0][0]']  \n",
      "                                                                                                  \n",
      " conv1d_2 (Conv1D)              (None, 25, 512)      1311232     ['max_pooling1d_1[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_2 (BatchNo  (None, 25, 512)     2048        ['conv1d_2[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " max_pooling1d_2 (MaxPooling1D)  (None, 13, 512)     0           ['batch_normalization_2[0][0]']  \n",
      "                                                                                                  \n",
      " conv1d_3 (Conv1D)              (None, 13, 1024)     3671040     ['max_pooling1d_2[0][0]']        \n",
      "                                                                                                  \n",
      " conv1d_4 (Conv1D)              (None, 13, 1024)     7341056     ['conv1d_3[0][0]']               \n",
      "                                                                                                  \n",
      " batch_normalization_3 (BatchNo  (None, 13, 1024)    4096        ['conv1d_4[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " max_pooling1d_3 (MaxPooling1D)  (None, 7, 1024)     0           ['batch_normalization_3[0][0]']  \n",
      "                                                                                                  \n",
      " flatten_1 (Flatten)            (None, 7168)         0           ['max_pooling1d_3[0][0]']        \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 1024)         7341056     ['flatten_1[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_4 (BatchNo  (None, 1024)        4096        ['dense_1[0][0]']                \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)            (None, 1024)         0           ['batch_normalization_4[0][0]']  \n",
      "                                                                                                  \n",
      " output1 (Dense)                (None, 187)          191675      ['dropout_1[0][0]']              \n",
      "                                                                                                  \n",
      " concatenate_1 (Concatenate)    (None, 1211)         0           ['dropout_1[0][0]',              \n",
      "                                                                  'output1[0][0]']                \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 1024)         1241088     ['concatenate_1[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_5 (BatchNo  (None, 1024)        4096        ['dense_2[0][0]']                \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " output2 (Dense)                (None, 10)           10250       ['batch_normalization_5[0][0]']  \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 21,137,093\n",
      "Trainable params: 21,128,901\n",
      "Non-trainable params: 8,192\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Present the inference dataset to the model and collect prediction.\n",
    "\n",
    "The model returns two sets of probabilities:\n",
    "- `prob_preds_species`: a vector of 187 values representing the probability that each of the 187 species are the correct ones, for each input read\n",
    "- `prob_preds_pos`: a vector of 10 values representing the probability that the read is from the corresponding segment of the original sequence (1 to 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2 µs, sys: 1e+03 ns, total: 3 µs\n",
      "Wall time: 5.25 µs\n",
      "140/140 [==============================] - 131s 913ms/step\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "prob_preds_species, prob_preds_pos = model.predict(ds, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((571980, 187), (571980, 10))"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob_preds_species.shape, prob_preds_pos.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find the prediction, we pick the argmax probability, which gives us the index/code for the predicted virus species."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([117, 117, 117, 117,  32,  89, 117, 117,  94, 117])"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_preds = np.argmax(prob_preds_species, axis=1)\n",
    "class_preds.shape\n",
    "class_preds[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "117 is for `SARS` and 94 for `MERS`. We see that there are a few errors in prediction as all the reference sequences are either SARS or MERS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple evaluation of the model for CoV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Original model was trained with 187 different virus species."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original model is trained to detect 187 virus species, including 2 coronavirus species:\n",
      " - Middle_East_respiratory_syndrome-related_coronavirus    \t94\n",
      " - Severe_acute_respiratory_syndrome-related_coronavirus    \t117\n",
      "\n"
     ]
    }
   ],
   "source": [
    "p2virus_labels = pfs.data / 'CNN_Virus_data/virus_name_mapping'\n",
    "with open(p2virus_labels, 'r') as fp:\n",
    "    i, c = 0, 0\n",
    "    cov = []\n",
    "    while True:\n",
    "        line  = fp.readline()\n",
    "        if line == '': break\n",
    "        elif ('corona' in line) or ('mers' in line) : \n",
    "            c += 1\n",
    "            line = line.replace('\\t', '    \\t')\n",
    "            cov.append(f\" - {line}\")\n",
    "        i += 1\n",
    "print(f\"Original model is trained to detect {i} virus species, including {c} coronavirus species:\")\n",
    "print(''.join(cov))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our objective is to validate that the model detects either a MERS or a SARS species out of the reads: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- RefSeq 1: Coronavirus BtRs-BetaCoV/YN2018D  \n",
      "- RefSeq 2: Bovine coronavirus  \n",
      "- RefSeq 3: Human coronavirus OC43  \n",
      "- RefSeq 4: Human coronavirus NL63  \n",
      "- RefSeq 5: Infectious bronchitis virus  \n",
      "- RefSeq 6: Porcine epidemic diarrhea virus  \n",
      "- RefSeq 7: Porcine epidemic diarrhea virus  \n",
      "- RefSeq 8: Porcine epidemic diarrhea virus  \n",
      "- RefSeq 9: Porcine epidemic diarrhea virus  \n",
      "- RefSeq 10: Camel alphacoronavirus  \n"
     ]
    }
   ],
   "source": [
    "for i, v in enumerate(aln.ref_sequences.values()):\n",
    "    print(f\"- RefSeq {i+1}: {v['species'].replace('scientific name','')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We create several test functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_cov(y_preds):\n",
    "    \"\"\"Return 1 if the corresponding prediction is a corona virus, 0 otherwise\"\"\"\n",
    "    return (y_preds == 94).astype(int) + (y_preds == 117).astype(int)\n",
    "\n",
    "def is_mers(y_preds):\n",
    "    \"\"\"Returns 1 if model prediction is MERS and 0 otherwise\n",
    "\n",
    "    Note: 94 is the code for Middle_East_respiratory_syndrome-related_coronavirus\"\"\"\n",
    "    return y_preds == 94\n",
    "\n",
    "def is_sars(y_preds):\n",
    "    \"\"\"Returns 1 if model prediction is SARS and 0 otherwise\n",
    "\n",
    "    Note: 117 is the virus code for Severe_acute_respiratory_syndrome-related_coronavirus \n",
    "    \"\"\"\n",
    "    return y_preds == 117\n",
    "\n",
    "def cov_acc(y_true, y_preds):\n",
    "    \"\"\"Evaluates the accuracy of the model assuming all evaluated reads are from corona virus sequences\"\"\"\n",
    "    return is_cov(y_preds).sum()/y_preds.shape[0]\n",
    "\n",
    "def mers_acc(y_true, y_preds):\n",
    "    \"\"\"Evaluates the accuracy of the model assuming all evaluated reads are from corona virus sequences\"\"\"\n",
    "    return is_mers(y_preds).sum()/y_preds.shape[0]\n",
    "\n",
    "def sars_acc(y_true, y_preds):\n",
    "    \"\"\"Evaluates the accuracy of the model assuming all evaluated reads are from corona virus sequences\"\"\"\n",
    "    return is_sars(y_preds).sum()/y_preds.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Review all reads, broken down per reference sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52bc0db5bc434fbaade11e2b290dec86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reference Sequence: Coronavirus BtRs-BetaCoV/YN2018D  scientific name:\n",
      "  Nbr reads: 60,400\n",
      "  Accuracy CoV:   0.733\n",
      "  Accuracy MERS:  0.014\n",
      "  Accuracy SARS:  0.719\n",
      "Reference Sequence: Bovine coronavirus  scientific name:\n",
      "  Nbr reads: 61,800\n",
      "  Accuracy CoV:   0.055\n",
      "  Accuracy MERS:  0.030\n",
      "  Accuracy SARS:  0.026\n",
      "Reference Sequence: Human coronavirus OC43  scientific name:\n",
      "  Nbr reads: 61,080\n",
      "  Accuracy CoV:   0.057\n",
      "  Accuracy MERS:  0.031\n",
      "  Accuracy SARS:  0.026\n",
      "Reference Sequence: Human coronavirus NL63  scientific name:\n",
      "  Nbr reads: 55,000\n",
      "  Accuracy CoV:   0.067\n",
      "  Accuracy MERS:  0.031\n",
      "  Accuracy SARS:  0.035\n",
      "Reference Sequence: Infectious bronchitis virus  scientific name:\n",
      "  Nbr reads: 55,200\n",
      "  Accuracy CoV:   0.065\n",
      "  Accuracy MERS:  0.032\n",
      "  Accuracy SARS:  0.034\n",
      "Reference Sequence: Porcine epidemic diarrhea virus  scientific name:\n",
      "  Nbr reads: 56,000\n",
      "  Accuracy CoV:   0.068\n",
      "  Accuracy MERS:  0.032\n",
      "  Accuracy SARS:  0.036\n",
      "Reference Sequence: Porcine epidemic diarrhea virus  scientific name:\n",
      "  Nbr reads: 55,900\n",
      "  Accuracy CoV:   0.069\n",
      "  Accuracy MERS:  0.033\n",
      "  Accuracy SARS:  0.036\n",
      "Reference Sequence: Porcine epidemic diarrhea virus  scientific name:\n",
      "  Nbr reads: 55,900\n",
      "  Accuracy CoV:   0.070\n",
      "  Accuracy MERS:  0.032\n",
      "  Accuracy SARS:  0.038\n",
      "Reference Sequence: Porcine epidemic diarrhea virus  scientific name:\n",
      "  Nbr reads: 56,000\n",
      "  Accuracy CoV:   0.068\n",
      "  Accuracy MERS:  0.032\n",
      "  Accuracy SARS:  0.036\n",
      "Reference Sequence: Camel alphacoronavirus  scientific name:\n",
      "  Nbr reads: 54,700\n",
      "  Accuracy CoV:   0.074\n",
      "  Accuracy MERS:  0.035\n",
      "  Accuracy SARS:  0.039\n"
     ]
    }
   ],
   "source": [
    "aln = AlnFileReader(p2fastq.parent / f\"{p2fastq.stem}.aln\")\n",
    "\n",
    "acc_per_refseq = {}\n",
    "for refseqid in tqdm(reads_info.read_refseqs.unique()):\n",
    "    mask = reads_info.read_refseqs == refseqid\n",
    "    acc = cov_acc(None, class_preds[mask])\n",
    "    aln_refseq_meta = aln.ref_sequences[refseqid]\n",
    "    print(f\"Reference Sequence: {aln_refseq_meta['species']}:\")\n",
    "    print(f\"  Nbr reads: {class_preds[mask].shape[0]:,d}\")\n",
    "    print(f\"  Accuracy CoV:   {acc:.3f}\")\n",
    "    print(f\"  Accuracy MERS:  {mers_acc(None, class_preds[mask]):.3f}\")\n",
    "    print(f\"  Accuracy SARS:  {sars_acc(None, class_preds[mask]):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "acba6f24707946a1bb425a78f4f8b435",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ref. Sequence: Coronavirus BtRs-BetaCoV/YN2018D:\n",
      "  Accuracy :............... 0.733\n",
      "  Acc. coding strand: ..... 0.733\n",
      "  Acc. template strand: ... 0.733\n",
      "  Nbr reads: 60,400, incl. 30,099 from coding strand and 30,301 from template strand\n",
      "\n",
      "Ref. Sequence: Bovine coronavirus:\n",
      "  Accuracy :............... 0.055\n",
      "  Acc. coding strand: ..... 0.058\n",
      "  Acc. template strand: ... 0.053\n",
      "  Nbr reads: 61,800, incl. 30,928 from coding strand and 30,872 from template strand\n",
      "\n",
      "Ref. Sequence: Human coronavirus OC43:\n",
      "  Accuracy :............... 0.057\n",
      "  Acc. coding strand: ..... 0.064\n",
      "  Acc. template strand: ... 0.051\n",
      "  Nbr reads: 61,080, incl. 30,565 from coding strand and 30,515 from template strand\n",
      "\n",
      "Ref. Sequence: Human coronavirus NL63:\n",
      "  Accuracy :............... 0.067\n",
      "  Acc. coding strand: ..... 0.062\n",
      "  Acc. template strand: ... 0.071\n",
      "  Nbr reads: 55,000, incl. 27,560 from coding strand and 27,440 from template strand\n",
      "\n",
      "Ref. Sequence: Infectious bronchitis virus:\n",
      "  Accuracy :............... 0.065\n",
      "  Acc. coding strand: ..... 0.068\n",
      "  Acc. template strand: ... 0.063\n",
      "  Nbr reads: 55,200, incl. 27,632 from coding strand and 27,568 from template strand\n",
      "\n",
      "Ref. Sequence: Porcine epidemic diarrhea virus:\n",
      "  Accuracy :............... 0.068\n",
      "  Acc. coding strand: ..... 0.063\n",
      "  Acc. template strand: ... 0.072\n",
      "  Nbr reads: 56,000, incl. 28,267 from coding strand and 27,733 from template strand\n",
      "\n",
      "Ref. Sequence: Porcine epidemic diarrhea virus:\n",
      "  Accuracy :............... 0.069\n",
      "  Acc. coding strand: ..... 0.065\n",
      "  Acc. template strand: ... 0.072\n",
      "  Nbr reads: 55,900, incl. 27,964 from coding strand and 27,936 from template strand\n",
      "\n",
      "Ref. Sequence: Porcine epidemic diarrhea virus:\n",
      "  Accuracy :............... 0.070\n",
      "  Acc. coding strand: ..... 0.068\n",
      "  Acc. template strand: ... 0.071\n",
      "  Nbr reads: 55,900, incl. 27,756 from coding strand and 28,144 from template strand\n",
      "\n",
      "Ref. Sequence: Porcine epidemic diarrhea virus:\n",
      "  Accuracy :............... 0.068\n",
      "  Acc. coding strand: ..... 0.063\n",
      "  Acc. template strand: ... 0.073\n",
      "  Nbr reads: 56,000, incl. 27,963 from coding strand and 28,037 from template strand\n",
      "\n",
      "Ref. Sequence: Camel alphacoronavirus:\n",
      "  Accuracy :............... 0.074\n",
      "  Acc. coding strand: ..... 0.070\n",
      "  Acc. template strand: ... 0.077\n",
      "  Nbr reads: 54,700, incl. 27,313 from coding strand and 27,387 from template strand\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for refseqid in tqdm(reads_info.read_refseqs.unique()):\n",
    "    mask_refseq = reads_info.read_refseqs == refseqid\n",
    "    mask_strand_coding = reads_info.read_strand == '+'\n",
    "    mask_strand_template = reads_info.read_strand == '-'\n",
    "    mask_coding = (mask_strand_coding.astype(int) * mask_refseq.astype(int)).astype(bool)\n",
    "    mask_template = (mask_strand_template.astype(int) * mask_refseq.astype(int)).astype(bool)\n",
    "\n",
    "    aln_refseq_meta = aln.ref_sequences[refseqid]\n",
    "    acc = cov_acc(None, class_preds[mask_refseq])\n",
    "    acc_coding = cov_acc(None, class_preds[mask_coding])\n",
    "    acc_template = cov_acc(None, class_preds[mask_template])\n",
    "       \n",
    "    print(f\"Ref. Sequence: {aln_refseq_meta['species'].replace('scientific name', '').strip()}:\")\n",
    "    print(f\"  Accuracy :............... {acc:.3f}\")\n",
    "    print(f\"  Acc. coding strand: ..... {acc_coding:.3f}\")\n",
    "    print(f\"  Acc. template strand: ... {acc_template:.3f}\")\n",
    "    print(f\"  Nbr reads: {class_preds[mask_refseq].shape[0]:,d}, incl. {mask_coding.sum():,d} from coding strand and {mask_template.sum():,d} from template strand\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simreads generated from 100 sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "p2model = pfs.data / 'models/cnn_virus_original/pretrained_model.h5'\n",
    "p2simreads = pfs.data / 'ncbi/simreads/single_100seq_50bp'\n",
    "p2virus_labels = pfs.data / 'CNN_Virus_data/virus_name_mapping'\n",
    "assert p2model.is_file()\n",
    "assert p2simreads.is_dir()\n",
    "assert p2virus_labels.is_file()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "p2fastq = p2simreads / f\"{p2simreads.stem}.fq\"\n",
    "p2aln = p2simreads / f\"{p2simreads.stem}.aln\"\n",
    "assert p2fastq.is_file()\n",
    "assert p2aln.is_file()\n",
    "\n",
    "fastq = FastqFileReader(p2fastq)\n",
    "aln = AlnFileReader(p2aln)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "No match on this line",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [110]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# FIXME: following line trows an error: ValueError: No match on this line\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m refseqs_aln \u001b[38;5;241m=\u001b[39m \u001b[43maln\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mref_sequences\u001b[49m\n",
      "File \u001b[0;32m~/projects/bio/metagentools/metagentools/cnn_virus/data.py:343\u001b[0m, in \u001b[0;36mAlnFileReader.ref_sequences\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    340\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[1;32m    341\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mref_sequences\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    342\u001b[0m     \u001b[38;5;124;03m\"\"\"Return a dict with metadata for each reference sequence in the header\"\"\"\u001b[39;00m        \n\u001b[0;32m--> 343\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse_header_reference_sequences\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/projects/bio/metagentools/metagentools/cnn_virus/data.py:254\u001b[0m, in \u001b[0;36mAlnFileReader.parse_header_reference_sequences\u001b[0;34m(self, pattern, keys)\u001b[0m\n\u001b[1;32m    252\u001b[0m parsed \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    253\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m seq_dfn_line \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mheader[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreference sequences\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[0;32m--> 254\u001b[0m     metadata \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseq_dfn_line\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpattern\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    255\u001b[0m     parsed[metadata[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrefseqid\u001b[39m\u001b[38;5;124m'\u001b[39m]] \u001b[38;5;241m=\u001b[39m metadata\n\u001b[1;32m    257\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m parsed\n",
      "File \u001b[0;32m~/projects/bio/metagentools/metagentools/core.py:133\u001b[0m, in \u001b[0;36mTextFileBaseReader.parse_text\u001b[0;34m(self, txt, pattern, keys)\u001b[0m\n\u001b[1;32m    131\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpattern and keys must be either both None or both have a value\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    132\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 133\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parse_text_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtxt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpattern\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/projects/bio/metagentools/metagentools/core.py:107\u001b[0m, in \u001b[0;36mTextFileBaseReader._parse_text_fn\u001b[0;34m(self, txt, pattern, keys)\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m metadata\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 107\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo match on this line\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: No match on this line"
     ]
    }
   ],
   "source": [
    "# FIXME: following line trows an error: ValueError: No match on this line\n",
    "refseqs_aln = aln.ref_sequences\n",
    "\n",
    "# print(f\"reads simulated from the following {len(refseqs_aln)} reference sequences:\\n\")\n",
    "# print('\\n'.join([f\" {i:02d}: {refseq['species']}\" for i, refseq in enumerate(refseqs_aln.values())]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nsamples = 3_000_000\n",
    "p2ds, reads_info = create_infer_ds_from_fastq(p2fastq, overwrite_ds=True, nsamples=nsamples)\n",
    "\n",
    "text_ds = tf.data.TextLineDataset(p2ds).batch(32)\n",
    "ds = text_ds.map(strings_to_tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model(p2saved)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "93750/93750 [==============================] - 1928s 21ms/step\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "OOM when allocating tensor with shape[3000000,187] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc [Op:ConcatV2] name: concat",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "Input \u001b[0;32mIn [17]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m prob_preds \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/metagentools/lib/python3.10/site-packages/keras/utils/traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m---> 67\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     69\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/miniconda3/envs/metagentools/lib/python3.10/site-packages/tensorflow/python/framework/ops.py:7186\u001b[0m, in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   7184\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mraise_from_not_ok_status\u001b[39m(e, name):\n\u001b[1;32m   7185\u001b[0m   e\u001b[38;5;241m.\u001b[39mmessage \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m name: \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m name \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 7186\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_status_to_exception(e) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor with shape[3000000,187] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc [Op:ConcatV2] name: concat"
     ]
    }
   ],
   "source": [
    "prob_preds = model.predict(ds, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems that the inference runs fine for 93750 steps (3M samples) but then when the output is assembled, cannot allocate the prob_preds of shape shape `[3000000,187]` on the GPU.\n",
    "\n",
    "Questions:\n",
    "- why does this have to be on the GPU?\n",
    "- is it possible to have it on the main memory instead\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_preds[0].shape, prob_preds[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_preds = np.argmax(prob_preds[0], axis=1)\n",
    "class_preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_cov(y_preds):\n",
    "    \"\"\"Return 1 if the corresponding prediction is a corona virus, 0 otherwise\"\"\"\n",
    "    return (y_preds == 94).astype(int) + (y_preds == 117).astype(int)\n",
    "\n",
    "def cov_acc(y_true, y_preds):\n",
    "    \"\"\"Evaluates the accuracy of the model assuming all evaluated reads are from corona virus\"\"\"\n",
    "    return is_cov(y_preds).sum()/y_preds.shape[0]\n",
    "\n",
    "cov_acc(None, class_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(reads_info[:,1]).shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for refseqid in np.unique(reads_info[:,1]):\n",
    "    mask_refseq = reads_info[:,1] == refseqid\n",
    "    mask_strand_coding = reads_info[:,3] == '+'\n",
    "    mask_strand_template = reads_info[:,3] == '-'\n",
    "    mask_coding = (mask_strand_coding.astype(int) * mask_refseq.astype(int)).astype(bool)\n",
    "    mask_template = (mask_strand_template.astype(int) * mask_refseq.astype(int)).astype(bool)\n",
    "\n",
    "    aln_refseq_meta = aln.ref_sequences[refseqid]\n",
    "    acc = cov_acc(None, class_preds[mask_refseq])\n",
    "    acc_coding = cov_acc(None, class_preds[mask_coding])\n",
    "    acc_template = cov_acc(None, class_preds[mask_template])\n",
    "       \n",
    "    print(f\"Ref. Sequence: {aln_refseq_meta['species'].replace('scientific name', '').strip()}:\")\n",
    "    print(f\"  Accuracy :............... {acc:.3f}\")\n",
    "    print(f\"  Accuracy MERS:  {mers_acc(None, class_preds[mask]):.3f}\")\n",
    "    print(f\"  Accuracy SARS:  {sars_acc(None, class_preds[mask]):.3f}\")\n",
    "    print(f\"  Acc. coding strand: ..... {acc_coding:.3f}\")\n",
    "    print(f\"  Acc. template strand: ... {acc_template:.3f}\")\n",
    "    print(f\"  Nbr reads: {class_preds[mask_refseq].shape[0]:,d}, incl. {mask_coding.sum():,d} from coding strand and {mask_template.sum():,d} from template strand\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New Section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Access AWS from colab:\n",
    "- https://colab.research.google.com/github/bytehub-ai/code-examples/blob/main/tutorials/04_using_cloud_storage.ipynb\n",
    "- https://python.plainenglish.io/how-to-load-data-from-aws-s3-into-google-colab-7e76fbf534d2\n",
    "- https://medium.com/@lily_su/accessing-s3-bucket-from-google-colab-16f7ee6c5b51\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## handle GPU with tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# device = tf.config.list_physical_devices('GPU')[0]\n",
    "# device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'current': 301555968, 'peak': 330915840}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tf.config.experimental.get_memory_info(device='GPU:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invalid device or cannot modify virtual devices once initialized.\n"
     ]
    }
   ],
   "source": [
    "# physical_devices = tf.config.list_physical_devices('GPU')\n",
    "# try:\n",
    "#     tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "#     assert tf.config.experimental.get_memory_growth(physical_devices[0])\n",
    "# except:\n",
    "#     print('Invalid device or cannot modify virtual devices once initialized.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try:\n",
    "#     del model\n",
    "# except:\n",
    "#     pass\n",
    "# import gc\n",
    "# gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPU\n"
     ]
    }
   ],
   "source": [
    "# gpus = tf.config.list_physical_devices('GPU')\n",
    "# if gpus:\n",
    "#   # Restrict TensorFlow to only use the first GPU\n",
    "#   try:\n",
    "#     tf.config.set_visible_devices(gpus[0], 'GPU')\n",
    "#     logical_gpus = tf.config.list_logical_devices('GPU')\n",
    "#     print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPU\")\n",
    "#   except RuntimeError as e:\n",
    "#     # Visible devices must be set before GPUs have been initialized\n",
    "#     print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## end of section"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('metagentools')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "d79c725d3d254ae089d5edf9eb2dce3237f80d64dd85a8bedc17bd8054b8b312"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
