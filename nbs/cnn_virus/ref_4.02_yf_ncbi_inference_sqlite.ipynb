{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction on yellow fever simulated reads (Sqlite)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a a reference notebook for prediction inference on yellow fever simulated reads, using the CNN_Virus original model and saving predictions, probabilities and metadata in a sqlite database.\n",
    "\n",
    "- Simulated reads from an aligned file generated by ART Illumina simulator (`*.aln file`).\n",
    "- Uses the generator provided by `AlnFileReader.cnn_virus_input_generator` to read batches of simulates reads and their metadata.\n",
    "- Uses the `cnn_virus` model to predict the label and position probabilities and classes for each simreads.\n",
    "- Creates a prediction report and saves it in a sqlite database for easier retrieval and analysis later.\n",
    "\n",
    "> **Note**: \n",
    ">\n",
    ">When an `*aln` file counts a very large number of simulated reads, running a prediction on all of them is very time consuming. Thererofe, we also provide a function `skip_existing_predictions` applied to the generator, which allows to skip all simulated reads down to the last simulated read for which a prediction was already saved into the database. This allows to build the database step by step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Imports and setup environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "`ecutilities` already installed\n",
      "`metagentools` already installed\n"
     ]
    }
   ],
   "source": [
    "# Install required custom packages if not installed yet.\n",
    "import importlib.util\n",
    "if not importlib.util.find_spec('ecutilities'):\n",
    "    print('installing package: `ecutilities`')\n",
    "    ! pip install -qqU ecutilities\n",
    "else:\n",
    "    print('`ecutilities` already installed')\n",
    "if not importlib.util.find_spec('metagentools'):\n",
    "    print('installing package: `metagentools')\n",
    "    ! pip install -qqU metagentools\n",
    "else:\n",
    "    print('`metagentools` already installed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set autoreload mode\n",
      "Tensorflow version: 2.8.2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import all required packages\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import sqlite3\n",
    "import time\n",
    "\n",
    "from datetime import datetime\n",
    "from ecutilities.core import files_in_tree\n",
    "from ecutilities.ipython import nb_setup\n",
    "from functools import partial\n",
    "from IPython.display import display, update_display, Markdown, HTML\n",
    "from nbdev import show_doc\n",
    "from pandas import HDFStore\n",
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "from tqdm.notebook import tqdm, trange\n",
    "from typing import List, Tuple, Dict, Any, Generator\n",
    "\n",
    "# Setup the notebook for development\n",
    "nb_setup()\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' # or any {'0', '1', '2'}\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.client import device_lib\n",
    "from tensorflow.keras.models import load_model\n",
    "print(f\"Tensorflow version: {tf.__version__}\\n\")\n",
    "\n",
    "from metagentools.cnn_virus.data import _base_hot_encode, strings_to_tensors\n",
    "from metagentools.cnn_virus.data import split_kmer_into_50mers, combine_prediction_batch\n",
    "from metagentools.cnn_virus.data import FastaFileReader, FastqFileReader, AlnFileReader\n",
    "from metagentools.cnn_virus.data import OriginalLabels\n",
    "from metagentools.cnn_virus.data import string_input_batch_to_tensors, split_kmer_batch_into_50mers\n",
    "from metagentools.cnn_virus.architecture import create_model_original\n",
    "from metagentools.core import ProjectFileSystem, TextFileBaseReader, SqliteDatabase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List all computing devices available on the machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Devices:\n",
      "  - CPU  /device:CPU:0                          \n",
      "  - GPU  /device:GPU:0  NVIDIA GeForce GTX 1050 \n"
     ]
    }
   ],
   "source": [
    "devices = device_lib.list_local_devices()\n",
    "print('\\nDevices:')\n",
    "for d in devices:\n",
    "    t = d.device_type\n",
    "    name = d.physical_device_desc\n",
    "    l = [item.split(':', 1) for item in name.split(', ')]\n",
    "    name_attr = dict([x for x in l if len(x)==2])\n",
    "    dev = name_attr.get('name', ' ')\n",
    "    print(f\"  - {t}  {d.name} {dev:25s}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Setup paths to files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Key folders and system information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running linux on local computer\n",
      "Device's home directory: /home/vtec\n",
      "Project file structure:\n",
      " - Root ........ /home/vtec/projects/bio/metagentools \n",
      " - Data Dir .... /home/vtec/projects/bio/metagentools/data \n",
      " - Notebooks ... /home/vtec/projects/bio/metagentools/nbs\n"
     ]
    }
   ],
   "source": [
    "pfs = ProjectFileSystem()\n",
    "pfs.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `p2model`: path to file with saved original pretrained model\n",
    "- `p2virus_labels` path to file with virus names and labels mapping for original model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "p2model = pfs.data / 'saved/cnn_virus_original/pretrained_model.h5'\n",
    "assert p2model.is_file(), f\"No file found at {p2model.absolute()}\"\n",
    "\n",
    "p2virus_labels = pfs.data / 'CNN_Virus_data/virus_name_mapping'\n",
    "assert p2virus_labels.is_file(), f\"No file found at {p2virus_labels.absolute()}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Path to the simulated read we want to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "simreads\n",
      "  |--yf\n",
      "  |    |--single_1seq_150bp\n",
      "  |    |--single_69seq_150bp\n",
      "  |    |    |--single_69seq_150bp.fq (0)\n",
      "  |    |    |--single_69seq_150bp.aln (1)\n",
      "  |    |--paired_1seq_150bp\n",
      "  |    |--paired_69seq_150bp\n",
      "  |    |    |--paired_69seq_150bp1.fq (2)\n",
      "  |    |    |--paired_69seq_150bp2.fq (3)\n",
      "  |    |    |--paired_69seq_150bp1.aln (4)\n",
      "  |    |    |--paired_69seq_150bp2.aln (5)\n"
     ]
    }
   ],
   "source": [
    "fnames = files_in_tree(pfs.data / 'ncbi/simreads/yf', pattern='69seq')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading alignment file: single_69seq_150bp.aln:\n",
      "\n",
      "  - 1,161,034 simulated reads in file 'single_69seq_150bp.aln' from 69 reference sequences.\n",
      "  - ART command:  /usr/bin/art_illumina -i /home/vtec/projects/bio/metagentools/data/ncbi/refsequences/yf/yf_2023_yellow_fever.fa -ss HS25 -l 150 -f 250 -o /home/vtec/projects/bio/metagentools/data/ncbi/simreads/yf/single_69seq_150bp/single_69seq_150bp -rs 1724163599\n",
      "  - Reference Sequences:\n",
      "      @SQ\t11089:ncbi:1\t1\tAY968064\t11089\tncbi\tAngola_1971\t10237\n",
      "      @SQ\t11089:ncbi:2\t2\tU54798\t11089\tncbi\tIvory_Coast_1982\t10237\n",
      "      @SQ\t11089:ncbi:3\t3\tDQ235229\t11089\tncbi\tEthiopia_1961\t10237\n",
      "      @SQ\t11089:ncbi:4\t4\tAY572535\t11089\tncbi\tGambia_2001\t10237\n",
      "      @SQ\t11089:ncbi:5\t5\tMF405338\t11089\tncbi\tGhana_Hsapiens_1927\t10237\n",
      "      @SQ\t11089:ncbi:6\t6\tU21056\t11089\tncbi\tSenegal_1927\t10237\n",
      "      @SQ\t11089:ncbi:7\t7\tAY968065\t11089\tncbi\tUganda_1948\t10237\n",
      "      @SQ\t11089:ncbi:8\t8\tJX898871\t11089\tncbi\tArD114896_Senegal_1995\t10237\n",
      "      @SQ\t11089:ncbi:9\t9\tJX898872\t11089\tncbi\tSenegal_Aedes-aegypti_1995\t10237\n",
      "      @SQ\t11089:ncbi:10\t10\tGQ379163\t11089\tncbi\tPeru_Hsapiens_2007\t10237\n",
      "      @SQ\t11089:ncbi:11\t11\tDQ118157\t11089\tncbi\tSpain_Vaccine_2004\t10237\n",
      "      @SQ\t11089:ncbi:12\t12\tMF289572\t11089\tncbi\tSingapore_2017\t10237\n",
      "      @SQ\t11089:ncbi:13\t13\tKU978764\t11089\tncbi\tSudan_Hsapiens_1941\t10237\n",
      "      @SQ\t11089:ncbi:14\t14\tJX898878\t11089\tncbi\tArD181250_Senegal_2005\t10237\n",
      "      @SQ\t11089:ncbi:15\t15\tJX898879\t11089\tncbi\tArD181676_Senegal_2005\t10237\n",
      "      @SQ\t11089:ncbi:16\t16\tJX898881\t11089\tncbi\tSenegal_Aedes_luteocephalus_2005\t10237\n",
      "      @SQ\t11089:ncbi:17\t17\tJX898880\t11089\tncbi\tArD181564_Senegal_2005\t10237\n",
      "      @SQ\t11089:ncbi:18\t18\tJX898877\t11089\tncbi\tArD181464_Senegal_2005\t10237\n",
      "      @SQ\t11089:ncbi:19\t19\tJX898876\t11089\tncbi\tSenegal_Aedes_fucifer_2001\t10237\n",
      "      @SQ\t11089:ncbi:20\t20\tKU978765\t11089\tncbi\tGuinea_Bissau_Hsapiens_1965\t10237\n",
      "      @SQ\t11089:ncbi:21\t21\tJX898870\t11089\tncbi\tSenegal_Ae_fucifer_1996\t10237\n",
      "      @SQ\t11089:ncbi:22\t22\tJX898868\t11089\tncbi\tisolate_HD117294_Senegal_1995\t10237\n",
      "      @SQ\t11089:ncbi:23\t23\tJX898875\t11089\tncbi\tSenegal_Aedes_fucifer_2000\t10237\n",
      "      @SQ\t11089:ncbi:24\t24\tJX898874\t11089\tncbi\tArD149194_Senegal_2000\t10237\n",
      "      @SQ\t11089:ncbi:25\t25\tJX898873\t11089\tncbi\tArD149214_Senegal_2000\t10237\n",
      "      @SQ\t11089:ncbi:26\t26\tMK292067\t11089\tncbi\tNetherlands_Hsapiens_Gambia_2018\t10237\n",
      "      @SQ\t11089:ncbi:27\t27\tMK457701\t11089\tncbi\tNigeria_Hsapiens_2018\t10237\n",
      "      @SQ\t11089:ncbi:28\t28\tMN958078\t11089\tncbi\tNigeria_Hsapiens_2018\t10237\n",
      "      @SQ\t11089:ncbi:29\t29\tJX898869\t11089\tncbi\tCotedIvoire_Ae_africanus_1973\t10237\n",
      "      @SQ\t11089:ncbi:30\t30\tKU978763\t11089\tncbi\tNigeria_Hsapiens_1946\t10237\n",
      "      @SQ\t11089:ncbi:31\t31\tMF004382\t11089\tncbi\tBolivia_Hsapiens_1999\t10237\n",
      "      @SQ\t11089:ncbi:32\t32\tJF912181\t11089\tncbi\tBrazil_Hsapiens_1983\t10237\n",
      "      @SQ\t11089:ncbi:33\t33\tJF912179\t11089\tncbi\tBrazil_Haemagogus_sp_1980\t10237\n",
      "      @SQ\t11089:ncbi:34\t34\tJF912183\t11089\tncbi\tBrazil_Hsapiens_1984\t10237\n",
      "      @SQ\t11089:ncbi:35\t35\tJF912182\t11089\tncbi\tBrazil_Hsapiens_1984\t10237\n",
      "      @SQ\t11089:ncbi:36\t36\tJF912188\t11089\tncbi\tBrazil_Hsapiens_2000\t10237\n",
      "      @SQ\t11089:ncbi:37\t37\tJF912190\t11089\tncbi\tBrazil_Hsapiens_2002\t10237\n",
      "      @SQ\t11089:ncbi:38\t38\tHM582851\t11089\tncbi\tTrinidadandTobago_Alouetta_seniculus_2009\t10237\n",
      "      @SQ\t11089:ncbi:39\t39\tKM388814\t11089\tncbi\tVenezuela_Portuguesa_Hsapiens_2005\t10237\n",
      "      @SQ\t11089:ncbi:40\t40\tKM388815\t11089\tncbi\tVenezuela_Apure_Aseniculus_2007\t10237\n",
      "      @SQ\t11089:ncbi:41\t41\tKM388818\t11089\tncbi\tVenezuela_Barinas_Aseniculus_2006\t10237\n",
      "      @SQ\t11089:ncbi:42\t42\tKM388817\t11089\tncbi\tVenezuela_Guarico_Allouetta_seniculus_2004\t10237\n",
      "      @SQ\t11089:ncbi:43\t43\tKM388816\t11089\tncbi\tVenezuela_Monagas_Asiniculus_2010\t10237\n",
      "      @SQ\t11089:ncbi:44\t44\tMH666058\t11089\tncbi\tBrazil_Sapajus_sp_2016\t10237\n",
      "      @SQ\t11089:ncbi:45\t45\tMK583152\t11089\tncbi\tBrazil_SaoPaulo_Hsapiens_2017\t10237\n",
      "      @SQ\t11089:ncbi:46\t46\tMK583166\t11089\tncbi\tBrazil_SaoPaulo_Hsapiens_2018\t10237\n",
      "      @SQ\t11089:ncbi:47\t47\tMK760660\t11089\tncbi\tNetherlands_brazil_2018\t10237\n",
      "      @SQ\t11089:ncbi:48\t48\tMK760665\t11089\tncbi\tNetherlands_Hsapiens-from-brazil_2018\t10237\n",
      "      @SQ\t11089:ncbi:49\t49\tMF370549\t11089\tncbi\tBrazil_monkey_2015\t10237\n",
      "      @SQ\t11089:ncbi:50\t50\tMF370535\t11089\tncbi\tBrazil_Allouatta_sp_2016\t10237\n",
      "      @SQ\t11089:ncbi:51\t51\tMF370533\t11089\tncbi\tBrazil_Hsapiens_2017\t10237\n",
      "      @SQ\t11089:ncbi:52\t52\tMK333805\t11089\tncbi\tBrazil_IlhaGrande_Sabethes_chloropterus_2018\t10237\n",
      "      @SQ\t11089:ncbi:53\t53\tMF370530\t11089\tncbi\tBrazil_Haemagogus-janthinomys_2017\t10237\n",
      "      @SQ\t11089:ncbi:54\t54\tMW960207\t11089\tncbi\tYellow_fever_YF118_CAR_2018\t10237\n",
      "      @SQ\t11089:ncbi:55\t55\tJN620362\t11089\tncbi\tUganda_Hsapiens_2010\t10237\n",
      "      @SQ\t11089:ncbi:56\t56\tKY495641\t11089\tncbi\tChina_Hsapiens_2016\t10237\n",
      "      @SQ\t11089:ncbi:57\t57\tKU949599\t11089\tncbi\tshanghai_2016\t10237\n",
      "      @SQ\t11089:ncbi:58\t58\tMG589641\t11089\tncbi\tChina_Hsapiens_2016\t10237\n",
      "      @SQ\t11089:ncbi:59\t59\tKX268355\t11089\tncbi\tChina_Hsapiens_2016\t10237\n",
      "      @SQ\t11089:ncbi:60\t60\tKY587416\t11089\tncbi\tChina_Hsapiens_2016\t10237\n",
      "      @SQ\t11089:ncbi:61\t61\tMH633692\t11089\tncbi\tChina_Hsapiens_2016\t10237\n",
      "      @SQ\t11089:ncbi:62\t62\tMF004383\t11089\tncbi\t432429_S4_MF004383\t10237\n",
      "      @SQ\t11089:ncbi:63\t63\tMW960207\t11089\tncbi\typ\t10237\n",
      "      @SQ\t11089:ncbi:64\t64\tON323052\t11089\tncbi\tNigeria_2020\t10237\n",
      "      @SQ\t11089:ncbi:65\t65\tON323053\t11089\tncbi\tNigeria_2020\t10237\n",
      "      @SQ\t11089:ncbi:66\t66\tON323054\t11089\tncbi\tNigeria_2020\t10237\n",
      "      @SQ\t11089:ncbi:67\t67\tOM066735\t11089\tncbi\tVHF-21-014/GHA/Damongo/2021\t10237\n",
      "      @SQ\t11089:ncbi:68\t68\tOM066736\t11089\tncbi\tVHF-21-029/GHA/Daboya/2021\t10237\n",
      "      @SQ\t11089:ncbi:69\t69\tOM066737\t11089\tncbi\tVHF-21-037/GHA/Damongo/2021\t10237\n"
     ]
    }
   ],
   "source": [
    "file_stem = 'single_69seq_150bp'\n",
    "\n",
    "p2aln = pfs.data / f\"ncbi/simreads/yf/{file_stem[:-2] if file_stem[-1] in ['1', '2'] else file_stem}/{file_stem}.aln\"\n",
    "assert p2aln.exists()\n",
    "\n",
    "aln = AlnFileReader(p2aln)\n",
    "print(f\"Reading alignment file: {p2aln.name}:\\n\")\n",
    "for i, aln_read in enumerate(aln):\n",
    "    pass\n",
    "print(f\"  - {i+1:,d} simulated reads in file '{p2aln.name}' from {len(aln.header['reference sequences'])} reference sequences.\")\n",
    "\n",
    "print('  - ART command: ',aln.header['command'])\n",
    "print('  - Reference Sequences:')\n",
    "print('     ','\\n      '.join(aln.header['reference sequences']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. (Optional) Test each inference step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test the steps to prepare model inputs, using a small batch size:\n",
    "1. create the generator using teh `.aln` file to yiel pairs of batches (metadata and reads strings) using `aln.cnn_virus_input_generator`\n",
    "2. transform the batch of string reads into a base hot encoded tensor, using the preprocessing funtion `string_input_batch_to_tensors`\n",
    "3. split each kmer read into (k-49) 50-mer reads to present to the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/vtecftwy/metagentools/blob/main/metagentools/cnn_virus/data.py#LNone){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### AlnFileReader.cnn_virus_input_generator\n",
       "\n",
       ">      AlnFileReader.cnn_virus_input_generator (label:int=118, bs:int=32)\n",
       "\n",
       "Create a generator yielding a metadata batch (dict) and a read batch (tensor of strings)\n",
       "\n",
       "The metadata dictionary contains lists of metadata for each read in the batch. For example:\n",
       "``` \n",
       "{\n",
       "    'readid': ['2591237:ncbi:1-40200','2591237:ncbi:1-40199','2591237:ncbi:1-40198', ...],\n",
       "    'refseqid': ['2591237:ncbi:1','2591237:ncbi:1','2591237:ncbi:1', ...],\n",
       "    'read_pos': [5, 6, 1, ...],\n",
       "    'refsource': ['ncbi', 'ncbi', 'ncbi', ...],\n",
       "    ...\n",
       "}\n",
       "```\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| label | int | 118 | label for this batch (assuming all reads are from the same species) |\n",
       "| bs | int | 32 | batch size |\n",
       "| **Returns** | **tuple** |  | **dict of metadata list and tensor of strings** |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/vtecftwy/metagentools/blob/main/metagentools/cnn_virus/data.py#LNone){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### AlnFileReader.cnn_virus_input_generator\n",
       "\n",
       ">      AlnFileReader.cnn_virus_input_generator (label:int=118, bs:int=32)\n",
       "\n",
       "Create a generator yielding a metadata batch (dict) and a read batch (tensor of strings)\n",
       "\n",
       "The metadata dictionary contains lists of metadata for each read in the batch. For example:\n",
       "``` \n",
       "{\n",
       "    'readid': ['2591237:ncbi:1-40200','2591237:ncbi:1-40199','2591237:ncbi:1-40198', ...],\n",
       "    'refseqid': ['2591237:ncbi:1','2591237:ncbi:1','2591237:ncbi:1', ...],\n",
       "    'read_pos': [5, 6, 1, ...],\n",
       "    'refsource': ['ncbi', 'ncbi', 'ncbi', ...],\n",
       "    ...\n",
       "}\n",
       "```\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| label | int | 118 | label for this batch (assuming all reads are from the same species) |\n",
       "| bs | int | 32 | batch size |\n",
       "| **Returns** | **tuple** |  | **dict of metadata list and tensor of strings** |"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(aln.cnn_virus_input_generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to define the model label for yellow fever and a batch size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yellow_fever_virus. Label: 118\n"
     ]
    }
   ],
   "source": [
    "OriginalLabels().search(s='yellow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/vtecftwy/metagentools/blob/main/metagentools/cnn_virus/data.py#LNone){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### string_input_batch_to_tensors\n",
       "\n",
       ">      string_input_batch_to_tensors (b:tensorflow.python.framework.ops.Tensor,\n",
       ">                                     k:int=150)\n",
       "\n",
       "Function converting a batch of bp strings into three tensors: (x_seqs, (y_labels, y_pos))\n",
       "\n",
       "Expects input strings to have the format: 'read sequence     label   position' where:\n",
       "\n",
       "- read sequence: a string of length k (kmer)\n",
       "- label: an integer between 0 and 186 representing the read virus label\n",
       "- position: an integer between 0 and 9 representing the read position in the genome\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| b | Tensor |  | batch of strings representing the inputs (kmer, label, position) |\n",
       "| k | int | 150 | maximum read length in the batch |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/vtecftwy/metagentools/blob/main/metagentools/cnn_virus/data.py#LNone){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### string_input_batch_to_tensors\n",
       "\n",
       ">      string_input_batch_to_tensors (b:tensorflow.python.framework.ops.Tensor,\n",
       ">                                     k:int=150)\n",
       "\n",
       "Function converting a batch of bp strings into three tensors: (x_seqs, (y_labels, y_pos))\n",
       "\n",
       "Expects input strings to have the format: 'read sequence     label   position' where:\n",
       "\n",
       "- read sequence: a string of length k (kmer)\n",
       "- label: an integer between 0 and 186 representing the read virus label\n",
       "- position: an integer between 0 and 9 representing the read position in the genome\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| b | Tensor |  | batch of strings representing the inputs (kmer, label, position) |\n",
       "| k | int | 150 | maximum read length in the batch |"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(string_input_batch_to_tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review metadata batch yielded by the generator:\n",
      "  List of metadata keys:\n",
      "  - aln_start_pos\n",
      "  - readid\n",
      "  - readnb\n",
      "  - refseq_strand\n",
      "  - refseqid\n",
      "  - refseqnb\n",
      "  - refsource\n",
      "  - reftaxonomyid\n",
      "  - read_pos\n",
      "  'readid' included in this batch': 11089:ncbi:1-17000, 11089:ncbi:1-16999, 11089:ncbi:1-16998, 11089:ncbi:1-16997, 11089:ncbi:1-16996, 11089:ncbi:1-16995, 11089:ncbi:1-16994, 11089:ncbi:1-16993\n",
      "\n",
      "Review batch of string reads yielded by the generator:\n",
      "  - Shape: (8,)\n",
      "\n",
      "Review the read kmer tensor after preprocessing:\n",
      "  - Shape: (8, 150, 5)\n",
      "\n",
      "Review ground truth tensors:\n",
      "  - Shape true label tensor:    (8, 187)\n",
      "  - Shape true position tensor: (8, 10)\n"
     ]
    }
   ],
   "source": [
    "b = 8\n",
    "true_label = 118\n",
    "\n",
    "aln.reset_iterator()\n",
    "for batch_meta, batch_reads in aln.cnn_virus_input_generator(bs=b, label=true_label):\n",
    "    reads_kmer, (labels_kmer, positions_kmer) = string_input_batch_to_tensors(batch_reads, k=150)\n",
    "    break\n",
    "\n",
    "print('Review metadata batch yielded by the generator:')\n",
    "print(f\"  List of metadata keys:\")\n",
    "print('  -','\\n  - '.join(batch_meta.keys()))\n",
    "print(f\"  'readid' included in this batch':\", ', '.join(batch_meta['readid']))\n",
    "print('\\nReview batch of string reads yielded by the generator:')\n",
    "print(f\"  - Shape: {batch_reads.shape}\")\n",
    "print('\\nReview the read kmer tensor after preprocessing:')\n",
    "print(f\"  - Shape: {reads_kmer.shape}\")\n",
    "print('\\nReview ground truth tensors:')\n",
    "print(f\"  - Shape true label tensor:    {labels_kmer.shape}\")\n",
    "print(f\"  - Shape true position tensor: {positions_kmer.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model only accepts 50-mer reads, so we need to split kmer reads into 50mers. For each kmer read, k-49 50mer reads will be generated, by shifting a window of 50 nucleotides by 1 nucleotide at a time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/vtecftwy/metagentools/blob/main/metagentools/cnn_virus/data.py#LNone){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### split_kmer_batch_into_50mers\n",
       "\n",
       ">      split_kmer_batch_into_50mers\n",
       ">                                    (kmer:tensorflow.python.framework.ops.Tenso\n",
       ">                                    r)\n",
       "\n",
       "Converts a batch of k-mer reads into several 50-mer reads, by shifting the k-mer one base at a time.\n",
       "\n",
       "for a batch of `b` k-mer reads, returns a batch of `b - 49` 50-mer reads\n",
       "\n",
       "|    | **Type** | **Details** |\n",
       "| -- | -------- | ----------- |\n",
       "| kmer | Tensor | tensor representing a batch of k-mer reads, after base encoding |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/vtecftwy/metagentools/blob/main/metagentools/cnn_virus/data.py#LNone){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### split_kmer_batch_into_50mers\n",
       "\n",
       ">      split_kmer_batch_into_50mers\n",
       ">                                    (kmer:tensorflow.python.framework.ops.Tenso\n",
       ">                                    r)\n",
       "\n",
       "Converts a batch of k-mer reads into several 50-mer reads, by shifting the k-mer one base at a time.\n",
       "\n",
       "for a batch of `b` k-mer reads, returns a batch of `b - 49` 50-mer reads\n",
       "\n",
       "|    | **Type** | **Details** |\n",
       "| -- | -------- | ----------- |\n",
       "| kmer | Tensor | tensor representing a batch of k-mer reads, after base encoding |"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(split_kmer_batch_into_50mers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Each kmer is split into 101 50-mer reads. Total 50-mer reads in a batch: 808\n",
      "\n",
      "Review the reads tensor:\n",
      "  - Shape kmer tensor:   (8, 150, 5)\n",
      "  - Shape 50-mer tensor: (808, 50, 5)\n"
     ]
    }
   ],
   "source": [
    "reads_50mer = split_kmer_batch_into_50mers(reads_kmer)\n",
    "nb_50mer_per_kmer = reads_kmer.shape[1]-49 \n",
    "nb_50mer_reads = (nb_50mer_per_kmer) * reads_kmer.shape[0]\n",
    "assert reads_50mer.shape == (nb_50mer_reads, 50,5)\n",
    "\n",
    "print(f\"Each kmer is split into {nb_50mer_per_kmer} 50-mer reads. Total 50-mer reads in a batch: {nb_50mer_reads}\")\n",
    "print('\\nReview the reads tensor:')\n",
    "print(f\"  - Shape kmer tensor:   {reads_kmer.shape}\")\n",
    "print(f\"  - Shape 50-mer tensor: {reads_50mer.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If all runs smoothly, our generator and preprocessing are working fine. We can run the prediction loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Run the Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tables(db, dry_run=False):\n",
    "\n",
    "    top_n = 5\n",
    "\n",
    "    db.connect()\n",
    "    # Create table for predictions and its index\n",
    "    pred_cols_str = 'readid refseqid refsource refseq_strand taxonomyid'.split(' ')\n",
    "    pred_cols_int = 'lbl_true lbl_pred pos_true pos_pred'.split(' ')\n",
    "    top_pred_cols = [f\"top_{top_n}_lbl_pred_{i}\" for i in range(top_n)]\n",
    "    query = \"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS predictions (\n",
    "        id INTEGER PRIMARY KEY,\n",
    "    \"\"\"\n",
    "    for col in pred_cols_str:\n",
    "        query += f\"{col} TEXT, \"\n",
    "    for col in pred_cols_int:\n",
    "        query += f\"{col} INTEGER, \"\n",
    "    for col in top_pred_cols:\n",
    "        query += f\"{col} INTEGER, \"\n",
    "    query = query[:-2]+') ;'\n",
    "    print(query)\n",
    "    if not dry_run: db.execute(query)\n",
    "\n",
    "    query = \"CREATE INDEX IF NOT EXISTS idx_preds_refseqid ON predictions (refseqid);\"\n",
    "    print(query)\n",
    "    if not dry_run: db.execute(query)\n",
    "\n",
    "    query = \"CREATE INDEX IF NOT EXISTS idx_preds_3 ON predictions (readid, refseqid, pos_true);\"\n",
    "    print(query)\n",
    "    if not dry_run: db.execute(query)\n",
    "\n",
    "    # Create table for probabilities (one per 50-mer in order to keep small nb or columns in table)\n",
    "    query = f\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS label_probabilities (\n",
    "        id INTEGER PRIMARY KEY,\n",
    "        read_kmer_id TEXT,\n",
    "        read_50mer_nb INTEGER,\n",
    "        refseqid TEXT, \n",
    "    \"\"\"\n",
    "    query += ' '.join([f\"prob_{i:03d} REAL, \" for i in range(187)])\n",
    "    query += \"FOREIGN KEY (read_kmer_id) REFERENCES predictions(readid)\"\n",
    "    query += ')'\n",
    "    print(query)\n",
    "    if not dry_run: db.execute(query)\n",
    "\n",
    "    query = \"CREATE INDEX IF NOT EXISTS idx_probs_refseqid ON label_probabilities (refseqid);\"\n",
    "    print(query)\n",
    "    if not dry_run: db.execute(query)\n",
    "\n",
    "    query = \"CREATE INDEX IF NOT EXISTS idx_probs_ids ON label_probabilities (refseqid, id);\"\n",
    "    print(query)\n",
    "    if not dry_run: db.execute(query)\n",
    "\n",
    "    query = \"CREATE INDEX IF NOT EXISTS idx_probs_3 ON label_probabilities (read_kmer_id, read_50mer_nb, refseqid);\"\n",
    "    print(query)\n",
    "    if not dry_run: db.execute(query)\n",
    "\n",
    "\n",
    "    # Create view joining predictions and label_probabilities\n",
    "    view_name = 'preds_probs'\n",
    "\n",
    "    # top prediction columns from table predictions:\n",
    "    top_lbl_pred_n = ','.join([f\"p.top_5_lbl_pred_{i}\" for i in range(5)])\n",
    "\n",
    "    # probabilities columns from table label_probabilities \n",
    "    probs_n = ','.join([f\"lp.prob_{i:03d}\" for i in range(187)])\n",
    "\n",
    "    query = f\"\"\"\n",
    "    CREATE VIEW IF NOT EXISTS {view_name} AS\n",
    "    SELECT \n",
    "        lp.id,\n",
    "        lp.refseqid,\n",
    "        p.lbl_true, p.lbl_pred,\n",
    "        p.pos_true, p.pos_pred,\n",
    "        {top_lbl_pred_n},\n",
    "        lp.read_kmer_id, lp.read_50mer_nb,\n",
    "        {probs_n}\n",
    "    FROM \n",
    "        label_probabilities lp\n",
    "    INNER JOIN \n",
    "        predictions p\n",
    "    ON \n",
    "        lp.read_kmer_id = p.readid\n",
    "    \"\"\"\n",
    "    print(query)\n",
    "    if not dry_run: db.execute(query)\n",
    "\n",
    "# p2db = pfs.data / '/mnt/s/metagentools/ncbi/infer_results/yf-ncbi/test-2.db'\n",
    "# db = SqliteDatabase(p2db)\n",
    "# db.close()\n",
    "# create_tables(db, dry_run=False)\n",
    "# db.print_schema()\n",
    "# db.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def skip_existing_predictions(gen: Generator,           # generator of batches (metadata, reads)\n",
    "                              db: sqlite3.Connection,   # path to the sqlite database \n",
    "                              bs: int                   # batch size\n",
    "                              ) -> Tuple[int, int]:     # number of batches and kmer reads skipped\n",
    "    \n",
    "    # Identify the readnb for the last saved prediction\n",
    "    print('Checking predictions already in database...')\n",
    "    last_predictions_id = db.execute(\"SELECT MAX(id) FROM predictions\").fetchone()[0]\n",
    "    if last_predictions_id is None:\n",
    "        return 0, 0\n",
    "    else:\n",
    "        last_readid = db.execute(f\"SELECT readid FROM predictions WHERE id = {last_predictions_id};\").fetchone()[0]\n",
    "        \n",
    "    # print(f\"Database includes {nb_predictions:,d} predictions, corresponding to {nb_predictions//bs:,d} batches\")\n",
    "    print(f\"Last prediction id: {last_predictions_id:,d} for kmer read '{last_readid}'\")\n",
    "    print(f\"Skipping already processed predictions ...\")\n",
    "\n",
    "    for i, (batch_meta, batch_reads) in enumerate(gen):\n",
    "        if i%100 == 0: \n",
    "            print(f\"   Skipped first {(i+1)*bs:,d} kmer reads ({i+1:,d} batches)\")\n",
    " \n",
    "        if last_readid in batch_meta['readid']:\n",
    "            print(f\"   Reached last batch of saved prediction (batch {i+1:,d})\")\n",
    "            print('Can procees with normal prediction inference')\n",
    "            nb_batches_skipped = i+1\n",
    "            break\n",
    "    return nb_batches_skipped, nb_batches_skipped * bs\n",
    "\n",
    "# aln.reset_iterator()\n",
    "# gen2 = aln.cnn_virus_input_generator(bs=512, label=118)\n",
    "# # p2db = pfs.data / '/mnt/s/metagentools/ncbi/infer_results/yf-ncbi/test-2.db'\n",
    "# p2db = pfs.data / '/mnt/s/metagentools/ncbi/infer_results/yf-ncbi/single_69seq_150bp.db'\n",
    "# db = SqliteDatabase(p2db)\n",
    "# db.connect();\n",
    "# nbs, nrs = skip_existing_predictions(gen=gen2, db=db, bs=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_predictions(probs, n=5):\n",
    "    \"\"\"Returns the top n top predictions for each kmer read\"\"\"\n",
    "\n",
    "    def top_n_most_frequent(preds, n=5):\n",
    "        \"\"\"Returns the top n most frequent predictions for each 50read\"\"\"\n",
    "        # print(preds.shape)\n",
    "        uniques, counts = np.unique(preds, return_counts=True)\n",
    "        top_idx = np.argsort(counts)[-n:]\n",
    "        return uniques.take(top_idx)\n",
    "\n",
    "    top_preds_in_50mers = np.argsort(probs, axis=-1)[:, :, -n:]\n",
    "    nb_kmers, nb_50mers, nb_lbls = top_preds_in_50mers.shape\n",
    "    # print(top_preds_in_50mers.shape)\n",
    "    top_preds_in_kmer = top_preds_in_50mers.reshape(nb_kmers,nb_50mers * nb_lbls)\n",
    "    # print(top_preds_in_kmer.shape)\n",
    "\n",
    "    return np.apply_along_axis(top_n_most_frequent, axis=1, arr=top_preds_in_kmer, n=n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference for 25% of the simreads on 69 sequences ALN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "simreads\n",
      "  |--yf\n",
      "  |    |--single_1seq_150bp\n",
      "  |    |--single_69seq_150bp\n",
      "  |    |    |--single_69seq_150bp.fq (0)\n",
      "  |    |    |--single_69seq_150bp.aln (1)\n",
      "  |    |--paired_1seq_150bp\n",
      "  |    |--paired_69seq_150bp\n",
      "  |    |    |--paired_69seq_150bp1.fq (2)\n",
      "  |    |    |--paired_69seq_150bp2.fq (3)\n",
      "  |    |    |--paired_69seq_150bp1.aln (4)\n",
      "  |    |    |--paired_69seq_150bp2.aln (5)\n"
     ]
    }
   ],
   "source": [
    "fnames = files_in_tree(pfs.data / 'ncbi/simreads/yf', pattern='69seq')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading alignment file: 'single_69seq_150bp.aln' (in /home/vtec/projects/bio/metagentools/data/ncbi/simreads/yf/single_69seq_150bp)\n",
      "\n",
      "  - 1,161,034 simulated reads in file 'single_69seq_150bp.aln' from 69 reference sequences.\n",
      "  - ART command:  /usr/bin/art_illumina -i /home/vtec/projects/bio/metagentools/data/ncbi/refsequences/yf/yf_2023_yellow_fever.fa -ss HS25 -l 150 -f 250 -o /home/vtec/projects/bio/metagentools/data/ncbi/simreads/yf/single_69seq_150bp/single_69seq_150bp -rs 1724163599\n",
      "  - Reference Sequences:\n",
      "      @SQ\t11089:ncbi:1\t1\tAY968064\t11089\tncbi\tAngola_1971\t10237\n",
      "      @SQ\t11089:ncbi:2\t2\tU54798\t11089\tncbi\tIvory_Coast_1982\t10237\n",
      "      @SQ\t11089:ncbi:3\t3\tDQ235229\t11089\tncbi\tEthiopia_1961\t10237\n",
      "      @SQ\t11089:ncbi:4\t4\tAY572535\t11089\tncbi\tGambia_2001\t10237\n",
      "      @SQ\t11089:ncbi:5\t5\tMF405338\t11089\tncbi\tGhana_Hsapiens_1927\t10237\n",
      "      @SQ\t11089:ncbi:6\t6\tU21056\t11089\tncbi\tSenegal_1927\t10237\n",
      "      @SQ\t11089:ncbi:7\t7\tAY968065\t11089\tncbi\tUganda_1948\t10237\n",
      "      @SQ\t11089:ncbi:8\t8\tJX898871\t11089\tncbi\tArD114896_Senegal_1995\t10237\n",
      "      @SQ\t11089:ncbi:9\t9\tJX898872\t11089\tncbi\tSenegal_Aedes-aegypti_1995\t10237\n",
      "      @SQ\t11089:ncbi:10\t10\tGQ379163\t11089\tncbi\tPeru_Hsapiens_2007\t10237\n",
      "      @SQ\t11089:ncbi:11\t11\tDQ118157\t11089\tncbi\tSpain_Vaccine_2004\t10237\n",
      "      @SQ\t11089:ncbi:12\t12\tMF289572\t11089\tncbi\tSingapore_2017\t10237\n",
      "      @SQ\t11089:ncbi:13\t13\tKU978764\t11089\tncbi\tSudan_Hsapiens_1941\t10237\n",
      "      @SQ\t11089:ncbi:14\t14\tJX898878\t11089\tncbi\tArD181250_Senegal_2005\t10237\n",
      "      @SQ\t11089:ncbi:15\t15\tJX898879\t11089\tncbi\tArD181676_Senegal_2005\t10237\n",
      "      @SQ\t11089:ncbi:16\t16\tJX898881\t11089\tncbi\tSenegal_Aedes_luteocephalus_2005\t10237\n",
      "      @SQ\t11089:ncbi:17\t17\tJX898880\t11089\tncbi\tArD181564_Senegal_2005\t10237\n",
      "      @SQ\t11089:ncbi:18\t18\tJX898877\t11089\tncbi\tArD181464_Senegal_2005\t10237\n",
      "      @SQ\t11089:ncbi:19\t19\tJX898876\t11089\tncbi\tSenegal_Aedes_fucifer_2001\t10237\n",
      "      @SQ\t11089:ncbi:20\t20\tKU978765\t11089\tncbi\tGuinea_Bissau_Hsapiens_1965\t10237\n",
      "      @SQ\t11089:ncbi:21\t21\tJX898870\t11089\tncbi\tSenegal_Ae_fucifer_1996\t10237\n",
      "      @SQ\t11089:ncbi:22\t22\tJX898868\t11089\tncbi\tisolate_HD117294_Senegal_1995\t10237\n",
      "      @SQ\t11089:ncbi:23\t23\tJX898875\t11089\tncbi\tSenegal_Aedes_fucifer_2000\t10237\n",
      "      @SQ\t11089:ncbi:24\t24\tJX898874\t11089\tncbi\tArD149194_Senegal_2000\t10237\n",
      "      @SQ\t11089:ncbi:25\t25\tJX898873\t11089\tncbi\tArD149214_Senegal_2000\t10237\n",
      "      @SQ\t11089:ncbi:26\t26\tMK292067\t11089\tncbi\tNetherlands_Hsapiens_Gambia_2018\t10237\n",
      "      @SQ\t11089:ncbi:27\t27\tMK457701\t11089\tncbi\tNigeria_Hsapiens_2018\t10237\n",
      "      @SQ\t11089:ncbi:28\t28\tMN958078\t11089\tncbi\tNigeria_Hsapiens_2018\t10237\n",
      "      @SQ\t11089:ncbi:29\t29\tJX898869\t11089\tncbi\tCotedIvoire_Ae_africanus_1973\t10237\n",
      "      @SQ\t11089:ncbi:30\t30\tKU978763\t11089\tncbi\tNigeria_Hsapiens_1946\t10237\n",
      "      @SQ\t11089:ncbi:31\t31\tMF004382\t11089\tncbi\tBolivia_Hsapiens_1999\t10237\n",
      "      @SQ\t11089:ncbi:32\t32\tJF912181\t11089\tncbi\tBrazil_Hsapiens_1983\t10237\n",
      "      @SQ\t11089:ncbi:33\t33\tJF912179\t11089\tncbi\tBrazil_Haemagogus_sp_1980\t10237\n",
      "      @SQ\t11089:ncbi:34\t34\tJF912183\t11089\tncbi\tBrazil_Hsapiens_1984\t10237\n",
      "      @SQ\t11089:ncbi:35\t35\tJF912182\t11089\tncbi\tBrazil_Hsapiens_1984\t10237\n",
      "      @SQ\t11089:ncbi:36\t36\tJF912188\t11089\tncbi\tBrazil_Hsapiens_2000\t10237\n",
      "      @SQ\t11089:ncbi:37\t37\tJF912190\t11089\tncbi\tBrazil_Hsapiens_2002\t10237\n",
      "      @SQ\t11089:ncbi:38\t38\tHM582851\t11089\tncbi\tTrinidadandTobago_Alouetta_seniculus_2009\t10237\n",
      "      @SQ\t11089:ncbi:39\t39\tKM388814\t11089\tncbi\tVenezuela_Portuguesa_Hsapiens_2005\t10237\n",
      "      @SQ\t11089:ncbi:40\t40\tKM388815\t11089\tncbi\tVenezuela_Apure_Aseniculus_2007\t10237\n",
      "      @SQ\t11089:ncbi:41\t41\tKM388818\t11089\tncbi\tVenezuela_Barinas_Aseniculus_2006\t10237\n",
      "      @SQ\t11089:ncbi:42\t42\tKM388817\t11089\tncbi\tVenezuela_Guarico_Allouetta_seniculus_2004\t10237\n",
      "      @SQ\t11089:ncbi:43\t43\tKM388816\t11089\tncbi\tVenezuela_Monagas_Asiniculus_2010\t10237\n",
      "      @SQ\t11089:ncbi:44\t44\tMH666058\t11089\tncbi\tBrazil_Sapajus_sp_2016\t10237\n",
      "      @SQ\t11089:ncbi:45\t45\tMK583152\t11089\tncbi\tBrazil_SaoPaulo_Hsapiens_2017\t10237\n",
      "      @SQ\t11089:ncbi:46\t46\tMK583166\t11089\tncbi\tBrazil_SaoPaulo_Hsapiens_2018\t10237\n",
      "      @SQ\t11089:ncbi:47\t47\tMK760660\t11089\tncbi\tNetherlands_brazil_2018\t10237\n",
      "      @SQ\t11089:ncbi:48\t48\tMK760665\t11089\tncbi\tNetherlands_Hsapiens-from-brazil_2018\t10237\n",
      "      @SQ\t11089:ncbi:49\t49\tMF370549\t11089\tncbi\tBrazil_monkey_2015\t10237\n",
      "      @SQ\t11089:ncbi:50\t50\tMF370535\t11089\tncbi\tBrazil_Allouatta_sp_2016\t10237\n",
      "      @SQ\t11089:ncbi:51\t51\tMF370533\t11089\tncbi\tBrazil_Hsapiens_2017\t10237\n",
      "      @SQ\t11089:ncbi:52\t52\tMK333805\t11089\tncbi\tBrazil_IlhaGrande_Sabethes_chloropterus_2018\t10237\n",
      "      @SQ\t11089:ncbi:53\t53\tMF370530\t11089\tncbi\tBrazil_Haemagogus-janthinomys_2017\t10237\n",
      "      @SQ\t11089:ncbi:54\t54\tMW960207\t11089\tncbi\tYellow_fever_YF118_CAR_2018\t10237\n",
      "      @SQ\t11089:ncbi:55\t55\tJN620362\t11089\tncbi\tUganda_Hsapiens_2010\t10237\n",
      "      @SQ\t11089:ncbi:56\t56\tKY495641\t11089\tncbi\tChina_Hsapiens_2016\t10237\n",
      "      @SQ\t11089:ncbi:57\t57\tKU949599\t11089\tncbi\tshanghai_2016\t10237\n",
      "      @SQ\t11089:ncbi:58\t58\tMG589641\t11089\tncbi\tChina_Hsapiens_2016\t10237\n",
      "      @SQ\t11089:ncbi:59\t59\tKX268355\t11089\tncbi\tChina_Hsapiens_2016\t10237\n",
      "      @SQ\t11089:ncbi:60\t60\tKY587416\t11089\tncbi\tChina_Hsapiens_2016\t10237\n",
      "      @SQ\t11089:ncbi:61\t61\tMH633692\t11089\tncbi\tChina_Hsapiens_2016\t10237\n",
      "      @SQ\t11089:ncbi:62\t62\tMF004383\t11089\tncbi\t432429_S4_MF004383\t10237\n",
      "      @SQ\t11089:ncbi:63\t63\tMW960207\t11089\tncbi\typ\t10237\n",
      "      @SQ\t11089:ncbi:64\t64\tON323052\t11089\tncbi\tNigeria_2020\t10237\n",
      "      @SQ\t11089:ncbi:65\t65\tON323053\t11089\tncbi\tNigeria_2020\t10237\n",
      "      @SQ\t11089:ncbi:66\t66\tON323054\t11089\tncbi\tNigeria_2020\t10237\n",
      "      @SQ\t11089:ncbi:67\t67\tOM066735\t11089\tncbi\tVHF-21-014/GHA/Damongo/2021\t10237\n",
      "      @SQ\t11089:ncbi:68\t68\tOM066736\t11089\tncbi\tVHF-21-029/GHA/Daboya/2021\t10237\n",
      "      @SQ\t11089:ncbi:69\t69\tOM066737\t11089\tncbi\tVHF-21-037/GHA/Damongo/2021\t10237\n"
     ]
    }
   ],
   "source": [
    "file_stem = 'single_69seq_150bp'\n",
    "\n",
    "p2aln = pfs.data / f\"ncbi/simreads/yf/{file_stem[:-2] if file_stem[-1] in ['1', '2'] else file_stem}/{file_stem}.aln\"\n",
    "assert p2aln.exists()\n",
    "\n",
    "aln = AlnFileReader(p2aln)\n",
    "print(f\"Reading alignment file: '{p2aln.name}' (in {p2aln.parent})\\n\")\n",
    "for i, aln_read in enumerate(aln):\n",
    "    pass\n",
    "nb_kmer_reads = i\n",
    "print(f\"  - {i+1:,d} simulated reads in file '{p2aln.name}' from {len(aln.header['reference sequences'])} reference sequences.\")\n",
    "\n",
    "print('  - ART command: ',aln.header['command'])\n",
    "print('  - Reference Sequences:')\n",
    "print('     ','\\n      '.join(aln.header['reference sequences']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'single_69seq_150bp.aln' used for prediction (in /home/vtec/projects/bio/metagentools/data/ncbi/simreads/yf/single_69seq_150bp).\n"
     ]
    }
   ],
   "source": [
    "print(f\"'{aln.path.name}' used for prediction (in {aln.path.parent}).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'single_selected_7seq_150bp.db' sqlite db used, (in /mnt/s/metagentools/ncbi/infer_results/yf-ncbi)\n"
     ]
    }
   ],
   "source": [
    "# p2db = pfs.data / 'ncbi/infer_results/yf-ncbi' / f'{p2aln.stem}.db'\n",
    "# p2db = pfs.data / 'ncbi/infer_results/yf-ncbi' / f'selected_test.db'\n",
    "# p2db = Path('/mnt/s/metagentools') / 'ncbi/infer_results/yf-ncbi' / f'{p2aln.stem}.db'\n",
    "p2db = Path('/mnt/s/metagentools') / 'ncbi/infer_results/yf-ncbi' / f'single_selected_7seq_150bp.db'\n",
    "print(f\"'{p2db.name}' sqlite db used, (in {p2db.parent})\")\n",
    "\n",
    "nl = '\\n'\n",
    "msg = f\"Are you sure you want to use this database?{nl}Database file '{p2db.name}' does not correspond to aln '{aln.path.name}'\"\n",
    "if aln.path.stem != p2db.stem and 'selected' not in p2db.stem: raise Warning(msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **NOTE**\n",
    ">\n",
    "> Estimated space required to save prediction and probability reports for the simreads simulated on 69 sequence is *470 Gb*. \n",
    ">\n",
    "> This is currently too large to save even on my NAS. \n",
    ">\n",
    "> Will first build a prediction dataset with 25% of the total reads: `nb_batches_to_run = int(nb_kmer_reads / b * 0.25)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         refseqid        Accession        Description     \n",
      "      ---------------   ----------    --------------------\n",
      " 1.   11089:ncbi:10      GQ379163      Peru_Hsapiens_2007 \n",
      " 2.   11089:ncbi:13      KU978764     Sudan_Hsapiens_1941 \n",
      " 3.   11089:ncbi:30      KU978763     Nigeria_Hsapiens_1946\n",
      " 4.   11089:ncbi:37      JF912190     Brazil_Hsapiens_2002\n",
      " 5.   11089:ncbi:32      JF912181     Brazil_Hsapiens_1983\n",
      " 6.   11089:ncbi:35      JF912182     Brazil_Hsapiens_1984\n",
      " 7.   11089:ncbi:1       AY968064         Angola_1971     \n"
     ]
    }
   ],
   "source": [
    "refseq_metadata = aln.parse_header_reference_sequences()\n",
    "\n",
    "selected_refseqs = ['11089:ncbi:10','11089:ncbi:13', '11089:ncbi:30','11089:ncbi:37', '11089:ncbi:32', '11089:ncbi:35', '11089:ncbi:1' ]\n",
    "# Originaly, 11089:ncbi:27 was also selected, but its accession (MK457701) is not in the distance matrix\n",
    "print(f\"      {'refseqid':^15s}  {'Accession':^13s}  {'Description':^20s}\")\n",
    "print(f\"      {'-'*15:^15s}  {'-'*10:^13s}  {'-'*20:^20s}\")\n",
    "for i,rsid in enumerate(selected_refseqs):\n",
    "    print(f\"{i+1:2d}.   {rsid:15s}  {refseq_metadata[rsid]['refseq_accession']:^13s}  {refseq_metadata[rsid]['organism']:^20s}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating CNN Model (Original)\n",
      "Loading parameters from pretrained_model.h5\n",
      "Created pretrained model\n",
      "Model loaded and ready to run ...\n",
      "Empty database. Creating tables ...\n",
      "\n",
      "    CREATE TABLE IF NOT EXISTS predictions (\n",
      "        id INTEGER PRIMARY KEY,\n",
      "    readid TEXT, refseqid TEXT, refsource TEXT, refseq_strand TEXT, taxonomyid TEXT, lbl_true INTEGER, lbl_pred INTEGER, pos_true INTEGER, pos_pred INTEGER, top_5_lbl_pred_0 INTEGER, top_5_lbl_pred_1 INTEGER, top_5_lbl_pred_2 INTEGER, top_5_lbl_pred_3 INTEGER, top_5_lbl_pred_4 INTEGER) ;\n",
      "CREATE INDEX IF NOT EXISTS idx_preds_refseqid ON predictions (refseqid);\n",
      "CREATE INDEX IF NOT EXISTS idx_preds_3 ON predictions (readid, refseqid, pos_true);\n",
      "\n",
      "    CREATE TABLE IF NOT EXISTS label_probabilities (\n",
      "        id INTEGER PRIMARY KEY,\n",
      "        read_kmer_id TEXT,\n",
      "        read_50mer_nb INTEGER,\n",
      "        refseqid TEXT, \n",
      "    prob_000 REAL,  prob_001 REAL,  prob_002 REAL,  prob_003 REAL,  prob_004 REAL,  prob_005 REAL,  prob_006 REAL,  prob_007 REAL,  prob_008 REAL,  prob_009 REAL,  prob_010 REAL,  prob_011 REAL,  prob_012 REAL,  prob_013 REAL,  prob_014 REAL,  prob_015 REAL,  prob_016 REAL,  prob_017 REAL,  prob_018 REAL,  prob_019 REAL,  prob_020 REAL,  prob_021 REAL,  prob_022 REAL,  prob_023 REAL,  prob_024 REAL,  prob_025 REAL,  prob_026 REAL,  prob_027 REAL,  prob_028 REAL,  prob_029 REAL,  prob_030 REAL,  prob_031 REAL,  prob_032 REAL,  prob_033 REAL,  prob_034 REAL,  prob_035 REAL,  prob_036 REAL,  prob_037 REAL,  prob_038 REAL,  prob_039 REAL,  prob_040 REAL,  prob_041 REAL,  prob_042 REAL,  prob_043 REAL,  prob_044 REAL,  prob_045 REAL,  prob_046 REAL,  prob_047 REAL,  prob_048 REAL,  prob_049 REAL,  prob_050 REAL,  prob_051 REAL,  prob_052 REAL,  prob_053 REAL,  prob_054 REAL,  prob_055 REAL,  prob_056 REAL,  prob_057 REAL,  prob_058 REAL,  prob_059 REAL,  prob_060 REAL,  prob_061 REAL,  prob_062 REAL,  prob_063 REAL,  prob_064 REAL,  prob_065 REAL,  prob_066 REAL,  prob_067 REAL,  prob_068 REAL,  prob_069 REAL,  prob_070 REAL,  prob_071 REAL,  prob_072 REAL,  prob_073 REAL,  prob_074 REAL,  prob_075 REAL,  prob_076 REAL,  prob_077 REAL,  prob_078 REAL,  prob_079 REAL,  prob_080 REAL,  prob_081 REAL,  prob_082 REAL,  prob_083 REAL,  prob_084 REAL,  prob_085 REAL,  prob_086 REAL,  prob_087 REAL,  prob_088 REAL,  prob_089 REAL,  prob_090 REAL,  prob_091 REAL,  prob_092 REAL,  prob_093 REAL,  prob_094 REAL,  prob_095 REAL,  prob_096 REAL,  prob_097 REAL,  prob_098 REAL,  prob_099 REAL,  prob_100 REAL,  prob_101 REAL,  prob_102 REAL,  prob_103 REAL,  prob_104 REAL,  prob_105 REAL,  prob_106 REAL,  prob_107 REAL,  prob_108 REAL,  prob_109 REAL,  prob_110 REAL,  prob_111 REAL,  prob_112 REAL,  prob_113 REAL,  prob_114 REAL,  prob_115 REAL,  prob_116 REAL,  prob_117 REAL,  prob_118 REAL,  prob_119 REAL,  prob_120 REAL,  prob_121 REAL,  prob_122 REAL,  prob_123 REAL,  prob_124 REAL,  prob_125 REAL,  prob_126 REAL,  prob_127 REAL,  prob_128 REAL,  prob_129 REAL,  prob_130 REAL,  prob_131 REAL,  prob_132 REAL,  prob_133 REAL,  prob_134 REAL,  prob_135 REAL,  prob_136 REAL,  prob_137 REAL,  prob_138 REAL,  prob_139 REAL,  prob_140 REAL,  prob_141 REAL,  prob_142 REAL,  prob_143 REAL,  prob_144 REAL,  prob_145 REAL,  prob_146 REAL,  prob_147 REAL,  prob_148 REAL,  prob_149 REAL,  prob_150 REAL,  prob_151 REAL,  prob_152 REAL,  prob_153 REAL,  prob_154 REAL,  prob_155 REAL,  prob_156 REAL,  prob_157 REAL,  prob_158 REAL,  prob_159 REAL,  prob_160 REAL,  prob_161 REAL,  prob_162 REAL,  prob_163 REAL,  prob_164 REAL,  prob_165 REAL,  prob_166 REAL,  prob_167 REAL,  prob_168 REAL,  prob_169 REAL,  prob_170 REAL,  prob_171 REAL,  prob_172 REAL,  prob_173 REAL,  prob_174 REAL,  prob_175 REAL,  prob_176 REAL,  prob_177 REAL,  prob_178 REAL,  prob_179 REAL,  prob_180 REAL,  prob_181 REAL,  prob_182 REAL,  prob_183 REAL,  prob_184 REAL,  prob_185 REAL,  prob_186 REAL, FOREIGN KEY (read_kmer_id) REFERENCES predictions(readid))\n",
      "CREATE INDEX IF NOT EXISTS idx_probs_refseqid ON label_probabilities (refseqid);\n",
      "CREATE INDEX IF NOT EXISTS idx_probs_ids ON label_probabilities (refseqid, id);\n",
      "CREATE INDEX IF NOT EXISTS idx_probs_3 ON label_probabilities (read_kmer_id, read_50mer_nb, refseqid);\n",
      "\n",
      "    CREATE VIEW IF NOT EXISTS preds_probs AS\n",
      "    SELECT \n",
      "        lp.id,\n",
      "        lp.refseqid,\n",
      "        p.lbl_true, p.lbl_pred,\n",
      "        p.pos_true, p.pos_pred,\n",
      "        p.top_5_lbl_pred_0,p.top_5_lbl_pred_1,p.top_5_lbl_pred_2,p.top_5_lbl_pred_3,p.top_5_lbl_pred_4,\n",
      "        lp.read_kmer_id, lp.read_50mer_nb,\n",
      "        lp.prob_000,lp.prob_001,lp.prob_002,lp.prob_003,lp.prob_004,lp.prob_005,lp.prob_006,lp.prob_007,lp.prob_008,lp.prob_009,lp.prob_010,lp.prob_011,lp.prob_012,lp.prob_013,lp.prob_014,lp.prob_015,lp.prob_016,lp.prob_017,lp.prob_018,lp.prob_019,lp.prob_020,lp.prob_021,lp.prob_022,lp.prob_023,lp.prob_024,lp.prob_025,lp.prob_026,lp.prob_027,lp.prob_028,lp.prob_029,lp.prob_030,lp.prob_031,lp.prob_032,lp.prob_033,lp.prob_034,lp.prob_035,lp.prob_036,lp.prob_037,lp.prob_038,lp.prob_039,lp.prob_040,lp.prob_041,lp.prob_042,lp.prob_043,lp.prob_044,lp.prob_045,lp.prob_046,lp.prob_047,lp.prob_048,lp.prob_049,lp.prob_050,lp.prob_051,lp.prob_052,lp.prob_053,lp.prob_054,lp.prob_055,lp.prob_056,lp.prob_057,lp.prob_058,lp.prob_059,lp.prob_060,lp.prob_061,lp.prob_062,lp.prob_063,lp.prob_064,lp.prob_065,lp.prob_066,lp.prob_067,lp.prob_068,lp.prob_069,lp.prob_070,lp.prob_071,lp.prob_072,lp.prob_073,lp.prob_074,lp.prob_075,lp.prob_076,lp.prob_077,lp.prob_078,lp.prob_079,lp.prob_080,lp.prob_081,lp.prob_082,lp.prob_083,lp.prob_084,lp.prob_085,lp.prob_086,lp.prob_087,lp.prob_088,lp.prob_089,lp.prob_090,lp.prob_091,lp.prob_092,lp.prob_093,lp.prob_094,lp.prob_095,lp.prob_096,lp.prob_097,lp.prob_098,lp.prob_099,lp.prob_100,lp.prob_101,lp.prob_102,lp.prob_103,lp.prob_104,lp.prob_105,lp.prob_106,lp.prob_107,lp.prob_108,lp.prob_109,lp.prob_110,lp.prob_111,lp.prob_112,lp.prob_113,lp.prob_114,lp.prob_115,lp.prob_116,lp.prob_117,lp.prob_118,lp.prob_119,lp.prob_120,lp.prob_121,lp.prob_122,lp.prob_123,lp.prob_124,lp.prob_125,lp.prob_126,lp.prob_127,lp.prob_128,lp.prob_129,lp.prob_130,lp.prob_131,lp.prob_132,lp.prob_133,lp.prob_134,lp.prob_135,lp.prob_136,lp.prob_137,lp.prob_138,lp.prob_139,lp.prob_140,lp.prob_141,lp.prob_142,lp.prob_143,lp.prob_144,lp.prob_145,lp.prob_146,lp.prob_147,lp.prob_148,lp.prob_149,lp.prob_150,lp.prob_151,lp.prob_152,lp.prob_153,lp.prob_154,lp.prob_155,lp.prob_156,lp.prob_157,lp.prob_158,lp.prob_159,lp.prob_160,lp.prob_161,lp.prob_162,lp.prob_163,lp.prob_164,lp.prob_165,lp.prob_166,lp.prob_167,lp.prob_168,lp.prob_169,lp.prob_170,lp.prob_171,lp.prob_172,lp.prob_173,lp.prob_174,lp.prob_175,lp.prob_176,lp.prob_177,lp.prob_178,lp.prob_179,lp.prob_180,lp.prob_181,lp.prob_182,lp.prob_183,lp.prob_184,lp.prob_185,lp.prob_186\n",
      "    FROM \n",
      "        label_probabilities lp\n",
      "    INNER JOIN \n",
      "        predictions p\n",
      "    ON \n",
      "        lp.read_kmer_id = p.readid\n",
      "    \n",
      "predictions (table)\n",
      " columns: id, readid, refseqid, refsource, refseq_strand, taxonomyid, lbl_true, lbl_pred, pos_true, pos_pred, top_5_lbl_pred_0, top_5_lbl_pred_1, top_5_lbl_pred_2, top_5_lbl_pred_3, top_5_lbl_pred_4\n",
      " index: idx_preds_3\n",
      "   indexed columns: readid, refseqid, pos_true\n",
      " index: idx_preds_refseqid\n",
      "   indexed columns: refseqid\n",
      "\n",
      "label_probabilities (table)\n",
      " columns: id, read_kmer_id, read_50mer_nb, refseqid, prob_000, prob_001, prob_002, prob_003, prob_004, prob_005, prob_006, prob_007, prob_008, prob_009, prob_010, prob_011, prob_012, prob_013, prob_014, prob_015, prob_016, prob_017, prob_018, prob_019, prob_020, prob_021, prob_022, prob_023, prob_024, prob_025, prob_026, prob_027, prob_028, prob_029, prob_030, prob_031, prob_032, prob_033, prob_034, prob_035, prob_036, prob_037, prob_038, prob_039, prob_040, prob_041, prob_042, prob_043, prob_044, prob_045, prob_046, prob_047, prob_048, prob_049, prob_050, prob_051, prob_052, prob_053, prob_054, prob_055, prob_056, prob_057, prob_058, prob_059, prob_060, prob_061, prob_062, prob_063, prob_064, prob_065, prob_066, prob_067, prob_068, prob_069, prob_070, prob_071, prob_072, prob_073, prob_074, prob_075, prob_076, prob_077, prob_078, prob_079, prob_080, prob_081, prob_082, prob_083, prob_084, prob_085, prob_086, prob_087, prob_088, prob_089, prob_090, prob_091, prob_092, prob_093, prob_094, prob_095, prob_096, prob_097, prob_098, prob_099, prob_100, prob_101, prob_102, prob_103, prob_104, prob_105, prob_106, prob_107, prob_108, prob_109, prob_110, prob_111, prob_112, prob_113, prob_114, prob_115, prob_116, prob_117, prob_118, prob_119, prob_120, prob_121, prob_122, prob_123, prob_124, prob_125, prob_126, prob_127, prob_128, prob_129, prob_130, prob_131, prob_132, prob_133, prob_134, prob_135, prob_136, prob_137, prob_138, prob_139, prob_140, prob_141, prob_142, prob_143, prob_144, prob_145, prob_146, prob_147, prob_148, prob_149, prob_150, prob_151, prob_152, prob_153, prob_154, prob_155, prob_156, prob_157, prob_158, prob_159, prob_160, prob_161, prob_162, prob_163, prob_164, prob_165, prob_166, prob_167, prob_168, prob_169, prob_170, prob_171, prob_172, prob_173, prob_174, prob_175, prob_176, prob_177, prob_178, prob_179, prob_180, prob_181, prob_182, prob_183, prob_184, prob_185, prob_186\n",
      " index: idx_probs_3\n",
      "   indexed columns: read_kmer_id, read_50mer_nb, refseqid\n",
      " index: idx_probs_ids\n",
      "   indexed columns: refseqid, id\n",
      " index: idx_probs_refseqid\n",
      "   indexed columns: refseqid\n",
      "\n",
      "preds_probs (view)\n",
      " columns: id,refseqid,lbl_true,lbl_pred,pos_true,pos_pred,top_5_lbl_pred_0,top_5_lbl_pred_1,top_5_lbl_pred_2,top_5_lbl_pred_3,top_5_lbl_pred_4,read_kmer_id,read_50mer_nb,prob_000,prob_001,prob_002,prob_003,prob_004,prob_005,prob_006,prob_007,prob_008,prob_009,prob_010,prob_011,prob_012,prob_013,prob_014,prob_015,prob_016,prob_017,prob_018,prob_019,prob_020,prob_021,prob_022,prob_023,prob_024,prob_025,prob_026,prob_027,prob_028,prob_029,prob_030,prob_031,prob_032,prob_033,prob_034,prob_035,prob_036,prob_037,prob_038,prob_039,prob_040,prob_041,prob_042,prob_043,prob_044,prob_045,prob_046,prob_047,prob_048,prob_049,prob_050,prob_051,prob_052,prob_053,prob_054,prob_055,prob_056,prob_057,prob_058,prob_059,prob_060,prob_061,prob_062,prob_063,prob_064,prob_065,prob_066,prob_067,prob_068,prob_069,prob_070,prob_071,prob_072,prob_073,prob_074,prob_075,prob_076,prob_077,prob_078,prob_079,prob_080,prob_081,prob_082,prob_083,prob_084,prob_085,prob_086,prob_087,prob_088,prob_089,prob_090,prob_091,prob_092,prob_093,prob_094,prob_095,prob_096,prob_097,prob_098,prob_099,prob_100,prob_101,prob_102,prob_103,prob_104,prob_105,prob_106,prob_107,prob_108,prob_109,prob_110,prob_111,prob_112,prob_113,prob_114,prob_115,prob_116,prob_117,prob_118,prob_119,prob_120,prob_121,prob_122,prob_123,prob_124,prob_125,prob_126,prob_127,prob_128,prob_129,prob_130,prob_131,prob_132,prob_133,prob_134,prob_135,prob_136,prob_137,prob_138,prob_139,prob_140,prob_141,prob_142,prob_143,prob_144,prob_145,prob_146,prob_147,prob_148,prob_149,prob_150,prob_151,prob_152,prob_153,prob_154,prob_155,prob_156,prob_157,prob_158,prob_159,prob_160,prob_161,prob_162,prob_163,prob_164,prob_165,prob_166,prob_167,prob_168,prob_169,prob_170,prob_171,prob_172,prob_173,prob_174,prob_175,prob_176,prob_177,prob_178,prob_179,prob_180,prob_181,prob_182,prob_183,prob_184,prob_185,prob_186\n",
      "\n",
      "Run prediction loop with the following parameters:\n",
      "   850 k-mer per batch; 150 bp per sequence; keep top-5 predictions\n",
      "22:47:58    Starting prediction loop ...\n",
      "Checking predictions already in database...\n",
      "22:47:58    Skipped 0 batches (0 kmer reads)\n",
      "22:47:58    Batch   1 (aln batch   1) ...\n",
      "22:47:59      Starting prediction for 850 kmer reads ...\n",
      "22:48:48      Reshaping predictions ...\n",
      "22:48:48      Combining predictions ...\n",
      "22:49:04      Preparing prediction report ...\n",
      "22:49:04      Saving batch prediction report to db...\n",
      "22:49:04      Preparing label probabilities report ...\n",
      "22:49:33      Saving batch label probabilities report to db...\n"
     ]
    }
   ],
   "source": [
    "# Review and set following parameters\n",
    "\n",
    "# 2. Data parameters\n",
    "b = 850             # number of k-mer in a batch 850 5to cover half a set of one refseq (1700)\n",
    "# b = 4             \n",
    "k = 150             # read length\n",
    "true_label = 118    # yellow fever virus\n",
    "top_n = 5           # n for top-n prediction to keep\n",
    "\n",
    "# 3. Inference loop parameters\n",
    "run_all_batches = True\n",
    "# nb_batches_to_run = 2\n",
    "# nb_batches_to_run = int(nb_kmer_reads / b * 0.25)\n",
    "\n",
    "#====================================================================================================\n",
    "# Setup prediction Loop\n",
    "#====================================================================================================\n",
    "nb_50mer = k - 49\n",
    "uid = datetime.today().strftime('%Y-%m-%d_%H_%M_%S')\n",
    "\n",
    "aln.reset_iterator()\n",
    "model = create_model_original(path2parameters=p2model)\n",
    "print(f\"Model loaded and ready to run ...\")\n",
    "\n",
    "# Open connection to sqlite db and create tables if empty database\n",
    "db = SqliteDatabase(p2db)\n",
    "db.connect()\n",
    "tables = db.get_result(\"SELECT name FROM sqlite_master WHERE type='table';\")\n",
    "if 'predictions' not in [t[0] for t in tables]:\n",
    "    print(\"Empty database. Creating tables ...\")\n",
    "    create_tables(db, dry_run=False)\n",
    "    db.print_schema()\n",
    "\n",
    "# Create list of columns for prediction and probabilities reports\n",
    "pred_cols_str = 'readid refseqid refsource refseq_strand taxonomyid'.split(' ')\n",
    "pred_cols_int = 'lbl_true lbl_pred pos_true pos_pred'.split(' ')\n",
    "top_pred_cols = [f\"top_{top_n}_lbl_pred_{i}\" for i in range(top_n)]\n",
    "prob_cols = [f\"prob_{i:03d}\" for i in range(187)]\n",
    "\n",
    "def tprint(string):\n",
    "    print(f\"{datetime.now().strftime('%H:%M:%S')}    {string}\")\n",
    "\n",
    "#====================================================================================================\n",
    "# Setup prediction Loop\n",
    "#====================================================================================================\n",
    "print(f\"Run prediction loop with the following parameters:\")\n",
    "print(f\"   {b} k-mer per batch; {k} bp per sequence; keep top-{top_n} predictions\")\n",
    "tprint(f\"Starting prediction loop ...\")\n",
    "gen = aln.cnn_virus_input_generator(bs=b, label=true_label)\n",
    "\n",
    "# Skip kmer reads that are already processed\n",
    "nb_batches_skipped, nb_reads_skipped = skip_existing_predictions(gen=gen, db=db, bs=b)\n",
    "tprint(f\"Skipped {nb_batches_skipped:,d} batches ({nb_reads_skipped:,d} kmer reads)\")\n",
    "\n",
    "# Proceed with prediction inference \n",
    "for i,(metadata_batch, reads_batch) in enumerate(gen):\n",
    "    # skip any reference sequence not in the selected list\n",
    "    any_selected_refseq_in_batch = any([rsid in selected_refseqs for rsid in metadata_batch['refseqid']])\n",
    "    if not any_selected_refseq_in_batch:\n",
    "        tprint(f\"Skipping batch because does not include any selected reference sequence\")\n",
    "        continue\n",
    "\n",
    "    loop_start = datetime.now()\n",
    "    tprint(f\"Batch {i+1:3,d} (aln batch {nb_batches_skipped+i+1:3,d}) ...\")\n",
    "\n",
    "    reads_kmer, (labels_true, position_true) = string_input_batch_to_tensors(reads_batch, k=k)\n",
    "    reads_50mer = split_kmer_batch_into_50mers(reads_kmer)\n",
    "    assert reads_50mer.shape == ((reads_kmer.shape[1]-49) * b, 50, 5), f\"Problem with shape in batch {i+1}: {reads_50mer.shape}\"\n",
    "\n",
    "    tprint(f'  Starting prediction for {b:,} kmer reads ...')\n",
    "    label_probs, position_probs = model.predict(reads_50mer)\n",
    "\n",
    "    tprint('  Reshaping predictions ...')\n",
    "    label_probs_kmer = tf.reshape(label_probs, shape=(b,nb_50mer,-1))\n",
    "    position_probs_kmer = tf.reshape(position_probs, shape=(b,nb_50mer,-1))\n",
    "\n",
    "    tprint('  Combining predictions ...')\n",
    "    combined_predictions = tf.map_fn(\n",
    "        fn=combine_prediction_batch,\n",
    "        elems=[label_probs_kmer, position_probs_kmer], \n",
    "        fn_output_signature=tf.int64\n",
    "        )\n",
    "\n",
    "    label_predictions = combined_predictions[:,0]\n",
    "    position_predictions = combined_predictions[:,1]\n",
    "    top_preds = top_predictions(label_probs_kmer, n=top_n)\n",
    "\n",
    "    # Add results for current batch\n",
    "    tprint('  Preparing prediction report ...')\n",
    "    preds_report = np.concatenate(\n",
    "        [\n",
    "            np.expand_dims(np.array(metadata_batch['readid']), axis=1),         # readid \n",
    "            np.expand_dims(np.array(metadata_batch['refseqid']), axis=1),       # refseqid\n",
    "            np.expand_dims(np.array(metadata_batch['refsource']), axis=1),      # refsource\n",
    "            np.expand_dims(np.array(metadata_batch['refseq_strand']), axis=1),  # refseq_strand\n",
    "            np.expand_dims(np.array(metadata_batch['reftaxonomyid']), axis=1),  # taxonomyid\n",
    "            np.expand_dims(np.array([true_label]*b), axis=1),                   # lbl_true\n",
    "            np.expand_dims(label_predictions, axis=1),                          # lbl_pred\n",
    "            np.expand_dims(np.array(metadata_batch['aln_start_pos']), axis=1),  # pos_true\n",
    "            np.expand_dims(position_predictions, axis=1),                       # pos_pred\n",
    "            top_preds[:, ::-1],                                                 # top_5_lbl_pred_0, top_5_lbl_pred_1, top_5_lbl_pred_2, top_5_lbl_pred_3, top_5_lbl_pred_4\n",
    "        ],\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "    df_preds = pd.DataFrame(\n",
    "        data=preds_report, \n",
    "        columns=pred_cols_str + pred_cols_int + top_pred_cols\n",
    "        )\n",
    "    tprint('  Saving batch prediction report to db...')\n",
    "    db.dataframe_to_table(df_preds, 'predictions', if_exists='append', index=False)\n",
    "\n",
    "    tprint('  Preparing label probabilities report ...')\n",
    "    df_probs = None\n",
    "    for read_50mer_nb in range(nb_50mer):\n",
    "        probs_report = np.concatenate(\n",
    "            [\n",
    "                np.expand_dims(np.array(metadata_batch['readid']), axis=1),     # readid \n",
    "                np.expand_dims(np.array([read_50mer_nb]*b), axis=1),            # read_50mer_nb\n",
    "                np.expand_dims(np.array(metadata_batch['refseqid']), axis=1),   # refseqid\n",
    "                label_probs_kmer[:, read_50mer_nb, :]                           # label probabilities\n",
    "            ],\n",
    "            axis=1\n",
    "        )\n",
    "\n",
    "        df = pd.DataFrame(\n",
    "            data=probs_report, \n",
    "            columns=['read_kmer_id', 'read_50mer_nb', 'refseqid'] + prob_cols\n",
    "            )\n",
    "        df_probs = df if df_probs is None else pd.concat([df_probs, df], axis=0)\n",
    "\n",
    "    tprint('  Saving batch label probabilities report to db...')\n",
    "    db.dataframe_to_table(df_probs, 'label_probabilities', if_exists='append', index=False)\n",
    "\n",
    "    tprint(f\"  Batch processing time: {(datetime.now() - loop_start).total_seconds():.2f} sec\")\n",
    "    if not run_all_batches and i+1 >= nb_batches_to_run: \n",
    "        print('Stopping')\n",
    "        break\n",
    "\n",
    "db.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Review reads distribution in the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Path('/mnt/s/metagentools/ncbi/infer_results/yf-ncbi/single_selected_8seq_150bp.db')"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p2db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53,248 predictions and 5,378,048 in database\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>refseqid</th>\n",
       "      <th>organism</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11089:ncbi:1</td>\n",
       "      <td>Angola_1971</td>\n",
       "      <td>17000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11089:ncbi:10</td>\n",
       "      <td>Peru_Hsapiens_2007</td>\n",
       "      <td>17000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11089:ncbi:11</td>\n",
       "      <td>Spain_Vaccine_2004</td>\n",
       "      <td>496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11089:ncbi:12</td>\n",
       "      <td>Singapore_2017</td>\n",
       "      <td>224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11089:ncbi:13</td>\n",
       "      <td>Sudan_Hsapiens_1941</td>\n",
       "      <td>17000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>11089:ncbi:14</td>\n",
       "      <td>ArD181250_Senegal_2005</td>\n",
       "      <td>184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>11089:ncbi:2</td>\n",
       "      <td>Ivory_Coast_1982</td>\n",
       "      <td>408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>11089:ncbi:26</td>\n",
       "      <td>Netherlands_Hsapiens_Gambia_2018</td>\n",
       "      <td>144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>11089:ncbi:27</td>\n",
       "      <td>Nigeria_Hsapiens_2018</td>\n",
       "      <td>368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>11089:ncbi:9</td>\n",
       "      <td>Senegal_Aedes-aegypti_1995</td>\n",
       "      <td>424</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        refseqid                          organism  count\n",
       "0   11089:ncbi:1                       Angola_1971  17000\n",
       "1  11089:ncbi:10                Peru_Hsapiens_2007  17000\n",
       "2  11089:ncbi:11                Spain_Vaccine_2004    496\n",
       "3  11089:ncbi:12                    Singapore_2017    224\n",
       "4  11089:ncbi:13               Sudan_Hsapiens_1941  17000\n",
       "5  11089:ncbi:14            ArD181250_Senegal_2005    184\n",
       "6   11089:ncbi:2                  Ivory_Coast_1982    408\n",
       "7  11089:ncbi:26  Netherlands_Hsapiens_Gambia_2018    144\n",
       "8  11089:ncbi:27             Nigeria_Hsapiens_2018    368\n",
       "9   11089:ncbi:9        Senegal_Aedes-aegypti_1995    424"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['11089:ncbi:1', '11089:ncbi:10', '11089:ncbi:13', '11089:ncbi:27', '11089:ncbi:30', '11089:ncbi:32', '11089:ncbi:35', '11089:ncbi:37']\n"
     ]
    }
   ],
   "source": [
    "df = db.get_dataframe(\"SELECT refseqid, COUNT(*) AS count FROM predictions GROUP BY refseqid\")\n",
    "df['organism'] = [refseq_metadata[i]['organism'] for i in df['refseqid']]\n",
    "df = df.loc[:, ['refseqid', 'organism', 'count']]\n",
    "total_count = df['count'].sum()\n",
    "print(f\"{total_count:,d} predictions and {total_count * 101:,d} in database\")\n",
    "display(df)\n",
    "print(sorted(selected_refseqs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need to rerun this for the entire dataset, which would lead to 136,000 kmer reads and 13.5 million 50mer reads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# db.execute(\"DROP TABLE IF EXISTS predictions;\")\n",
    "# db.execute(\"DROP TABLE IF EXISTS label_probabilities;\")\n",
    "# db.execute(\"DROP VIEW IF EXISTS preds_probs;\")\n",
    "# db.print_schema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display(db.get_dataframe(\"SELECT * FROM predictions;\"))\n",
    "# display(db.get_dataframe(\"SELECT * FROM predictions WHERE readid = '11089:ncbi:1-17000';\"))\n",
    "# display(db.get_dataframe(\"SELECT * FROM label_probabilities\"))\n",
    "# display(db.get_dataframe(\"SELECT * FROM label_probabilities WHERE read_kmer_id = '11089:ncbi:1-17000';\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(53248, '11089:ncbi:27-16633', '27-16633')"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_readid = db.execute(\"SELECT MAX(id) FROM predictions\").fetchone()[0]\n",
    "readid = db.execute(f\"SELECT readid FROM predictions WHERE id = {last_readid};\").fetchone()[0]\n",
    "regex = re.compile(r'\\d*:ncbi:(?P<read_nb>\\d*-\\d*)')\n",
    "m = regex.search(readid)\n",
    "read_nb = '???' if m is None else m.group('read_nb')\n",
    "\n",
    "last_readid, readid, read_nb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last id: 53,248, Nbr predictions: 53,248, last readid: 11089:ncbi:27-16633\n"
     ]
    }
   ],
   "source": [
    "last_predictions_id = db.execute(\"SELECT MAX(id) FROM predictions\").fetchone()[0]\n",
    "nb_predictions = db.execute(\"SELECT COUNT(*) FROM predictions\").fetchone()[0]\n",
    "last_readid = db.execute(f\"SELECT readid FROM predictions WHERE id = {last_predictions_id};\").fetchone()[0]\n",
    "\n",
    "print(f\"Last id: {last_predictions_id:,d}, Nbr predictions: {nb_predictions:,d}, last readid: {last_readid}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Technical note to accelerate `skip_existing_predictions`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The slow step in `skip_existing_predictions` is the query to get the total number of rows in the table. We can accelerate this by maintaining an accurate row count in a separate column when inserting new rows into a SQLite table named \"predictions\" that has a primary key \"id\" and an indexed column \"readid\". \n",
    "\n",
    "This can be done with the following steps:\n",
    "\n",
    "1. **Create a trigger** that fires after an INSERT operation on the \"predictions\" table. The trigger will update the row count in a separate table or column.\n",
    "\n",
    "2. **Create a table** to store the row count, for example:\n",
    "```sql\n",
    "    CREATE TABLE table_stats (\n",
    "    id INTEGER PRIMARY KEY,\n",
    "    row_count INTEGER NOT NULL\n",
    "    );\n",
    "```\n",
    "\n",
    "3. **Create the trigger** to update the row count after an INSERT:\n",
    "```sql\n",
    "    CREATE TRIGGER update_prediction_count\n",
    "    AFTER INSERT ON predictions\n",
    "    BEGIN\n",
    "    UPDATE table_stats \n",
    "    SET row_count = row_count + 1\n",
    "    WHERE id = 1;\n",
    "    END;\n",
    "```\n",
    "\n",
    "This trigger assumes there is only one row in the \"prediction_stats\" table with an id of 1. If you want to store the count per \"readid\", you can modify the trigger to:\n",
    "\n",
    "```sql\n",
    "    CREATE TRIGGER update_prediction_count\n",
    "    AFTER INSERT ON predictions  \n",
    "    BEGIN\n",
    "    INSERT INTO table_stats (readid, row_count)\n",
    "    VALUES (NEW.readid, 1)\n",
    "    ON CONFLICT(readid) DO UPDATE SET row_count = row_count + 1;\n",
    "    END;\n",
    "```\n",
    "\n",
    "This will insert a new row for each unique \"readid\" with an initial count of 1, and update the row_count if the \"readid\" already exists.\n",
    "\n",
    "4. **Insert a row** into the \"prediction_stats\" table with an initial count:\n",
    "```sql\n",
    "    INSERT INTO table_stats (id, row_count) VALUES (1, 0);\n",
    "```\n",
    "\n",
    "Now, whenever a new row is inserted into the \"predictions\" table, the trigger will automatically update the row count in the \"prediction_stats\" table. This maintains an accurate count without needing to perform a full table scan with COUNT(*).\n",
    "\n",
    "Remember to handle deletions as well by creating a BEFORE DELETE trigger that decrements the row count accordingly.\n",
    "\n",
    "Citations:\n",
    "- 1. https://www.sqlitetutorial.net/sqlite-insert/\n",
    "- 2. https://www.sqlitetutorial.net/sqlite-count-function/\n",
    "- 3. https://stackoverflow.com/questions/55007800/dynamic-way-to-insert-data-into-sqlite-table-when-column-counts-change\n",
    "- 4. https://docs.python.org/es/3/library/sqlite3.html\n",
    "- 5. https://www.sql-easy.com/learn/sqlite-count/\n",
    "- 6. https://stackoverflow.com/questions/4474873/what-is-the-most-efficient-way-to-count-rows-in-a-table-in-sqlite\n",
    "- 7. https://sqlite.org/forum/info/57c04743e1b6aa10\n",
    "- 8. https://sqlite.org/forum/forumpost/f832398c19"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End of Section"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('metagentools')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "d79c725d3d254ae089d5edf9eb2dce3237f80d64dd85a8bedc17bd8054b8b312"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
