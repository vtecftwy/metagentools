{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction on yellow fever simulated reads (Sqlite)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a a reference notebook for prediction inference on yellow fever simulated reads, using the CNN_Virus original model and saving predictions, probabilities and metadata in a sqlite database.\n",
    "\n",
    "- Simulated reads from an aligned file generated by ART Illumina simulator (`*.aln file`).\n",
    "- Uses the generator provided by `AlnFileReader.cnn_virus_input_generator` to read batches of simulates reads and their metadata.\n",
    "- Uses the `cnn_virus` model to predict the label and position probabilities and classes for each simreads.\n",
    "- Creates a prediction report and saves it in a sqlite database for easier retrieval and analysis later.\n",
    "\n",
    "> **Note**: \n",
    ">\n",
    ">When an `*aln` file counts a very large number of simulated reads, running a prediction on all of them is very time consuming. Thererofe, we also provide a function `skip_existing_predictions` applied to the generator, which allows to skip all simulated reads down to the last simulated read for which a prediction was already saved into the database. This allows to build the database step by step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Imports and setup environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "`ecutilities` already installed\n",
      "`metagentools` already installed\n"
     ]
    }
   ],
   "source": [
    "# Install required custom packages if not installed yet.\n",
    "import importlib.util\n",
    "if not importlib.util.find_spec('ecutilities'):\n",
    "    print('installing package: `ecutilities`')\n",
    "    ! pip install -qqU ecutilities\n",
    "else:\n",
    "    print('`ecutilities` already installed')\n",
    "if not importlib.util.find_spec('metagentools'):\n",
    "    print('installing package: `metagentools')\n",
    "    ! pip install -qqU metagentools\n",
    "else:\n",
    "    print('`metagentools` already installed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set autoreload mode\n",
      "Tensorflow version: 2.8.2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import all required packages\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import sqlite3\n",
    "import time\n",
    "\n",
    "from datetime import datetime\n",
    "from ecutilities.core import files_in_tree\n",
    "from ecutilities.ipython import nb_setup\n",
    "from functools import partial\n",
    "from IPython.display import display, update_display, Markdown, HTML\n",
    "from nbdev import show_doc\n",
    "from pandas import HDFStore\n",
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "from tqdm.notebook import tqdm, trange\n",
    "from typing import List, Tuple, Dict, Any, Generator\n",
    "\n",
    "# Setup the notebook for development\n",
    "nb_setup()\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' # or any {'0', '1', '2'}\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.client import device_lib\n",
    "from tensorflow.keras.models import load_model\n",
    "print(f\"Tensorflow version: {tf.__version__}\\n\")\n",
    "\n",
    "from metagentools.cnn_virus.data import _base_hot_encode, strings_to_tensors\n",
    "from metagentools.cnn_virus.data import split_kmer_into_50mers, combine_prediction_batch\n",
    "from metagentools.cnn_virus.data import FastaFileReader, FastqFileReader, AlnFileReader\n",
    "from metagentools.cnn_virus.data import OriginalLabels\n",
    "from metagentools.cnn_virus.data import string_input_batch_to_tensors, split_kmer_batch_into_50mers\n",
    "from metagentools.cnn_virus.architecture import create_model_original\n",
    "from metagentools.core import ProjectFileSystem, TextFileBaseReader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List all computing devices available on the machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Devices:\n",
      "  - CPU  /device:CPU:0                          \n",
      "  - GPU  /device:GPU:0  NVIDIA GeForce GTX 1050 \n"
     ]
    }
   ],
   "source": [
    "devices = device_lib.list_local_devices()\n",
    "print('\\nDevices:')\n",
    "for d in devices:\n",
    "    t = d.device_type\n",
    "    name = d.physical_device_desc\n",
    "    l = [item.split(':', 1) for item in name.split(', ')]\n",
    "    name_attr = dict([x for x in l if len(x)==2])\n",
    "    dev = name_attr.get('name', ' ')\n",
    "    print(f\"  - {t}  {d.name} {dev:25s}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Setup paths to files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Key folders and system information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running linux on local computer\n",
      "Device's home directory: /home/vtec\n",
      "Project file structure:\n",
      " - Root ........ /home/vtec/projects/bio/metagentools \n",
      " - Data Dir .... /home/vtec/projects/bio/metagentools/data \n",
      " - Notebooks ... /home/vtec/projects/bio/metagentools/nbs\n"
     ]
    }
   ],
   "source": [
    "pfs = ProjectFileSystem()\n",
    "pfs.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `p2model`: path to file with saved original pretrained model\n",
    "- `p2virus_labels` path to file with virus names and labels mapping for original model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "p2model = pfs.data / 'saved/cnn_virus_original/pretrained_model.h5'\n",
    "assert p2model.is_file(), f\"No file found at {p2model.absolute()}\"\n",
    "\n",
    "p2virus_labels = pfs.data / 'CNN_Virus_data/virus_name_mapping'\n",
    "assert p2virus_labels.is_file(), f\"No file found at {p2virus_labels.absolute()}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Path to the simulated read we want to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "simreads\n",
      "  |--yf\n",
      "  |    |--single_1seq_150bp\n",
      "  |    |--single_69seq_150bp\n",
      "  |    |    |--single_69seq_150bp.fq (0)\n",
      "  |    |    |--single_69seq_150bp.aln (1)\n",
      "  |    |--paired_1seq_150bp\n",
      "  |    |--paired_69seq_150bp\n",
      "  |    |    |--paired_69seq_150bp1.fq (2)\n",
      "  |    |    |--paired_69seq_150bp2.fq (3)\n",
      "  |    |    |--paired_69seq_150bp1.aln (4)\n",
      "  |    |    |--paired_69seq_150bp2.aln (5)\n"
     ]
    }
   ],
   "source": [
    "fnames = files_in_tree(pfs.data / 'ncbi/simreads/yf', pattern='69seq')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading alignment file: single_69seq_150bp.aln:\n",
      "\n",
      "  - 1,161,034 simulated reads in file 'single_69seq_150bp.aln' from 69 reference sequences.\n",
      "  - ART command:  /usr/bin/art_illumina -i /home/vtec/projects/bio/metagentools/data/ncbi/refsequences/yf/yf_2023_yellow_fever.fa -ss HS25 -l 150 -f 250 -o /home/vtec/projects/bio/metagentools/data/ncbi/simreads/yf/single_69seq_150bp/single_69seq_150bp -rs 1724163599\n",
      "  - Reference Sequences:\n",
      "      @SQ\t11089:ncbi:1\t1\tAY968064\t11089\tncbi\tAngola_1971\t10237\n",
      "      @SQ\t11089:ncbi:2\t2\tU54798\t11089\tncbi\tIvory_Coast_1982\t10237\n",
      "      @SQ\t11089:ncbi:3\t3\tDQ235229\t11089\tncbi\tEthiopia_1961\t10237\n",
      "      @SQ\t11089:ncbi:4\t4\tAY572535\t11089\tncbi\tGambia_2001\t10237\n",
      "      @SQ\t11089:ncbi:5\t5\tMF405338\t11089\tncbi\tGhana_Hsapiens_1927\t10237\n",
      "      @SQ\t11089:ncbi:6\t6\tU21056\t11089\tncbi\tSenegal_1927\t10237\n",
      "      @SQ\t11089:ncbi:7\t7\tAY968065\t11089\tncbi\tUganda_1948\t10237\n",
      "      @SQ\t11089:ncbi:8\t8\tJX898871\t11089\tncbi\tArD114896_Senegal_1995\t10237\n",
      "      @SQ\t11089:ncbi:9\t9\tJX898872\t11089\tncbi\tSenegal_Aedes-aegypti_1995\t10237\n",
      "      @SQ\t11089:ncbi:10\t10\tGQ379163\t11089\tncbi\tPeru_Hsapiens_2007\t10237\n",
      "      @SQ\t11089:ncbi:11\t11\tDQ118157\t11089\tncbi\tSpain_Vaccine_2004\t10237\n",
      "      @SQ\t11089:ncbi:12\t12\tMF289572\t11089\tncbi\tSingapore_2017\t10237\n",
      "      @SQ\t11089:ncbi:13\t13\tKU978764\t11089\tncbi\tSudan_Hsapiens_1941\t10237\n",
      "      @SQ\t11089:ncbi:14\t14\tJX898878\t11089\tncbi\tArD181250_Senegal_2005\t10237\n",
      "      @SQ\t11089:ncbi:15\t15\tJX898879\t11089\tncbi\tArD181676_Senegal_2005\t10237\n",
      "      @SQ\t11089:ncbi:16\t16\tJX898881\t11089\tncbi\tSenegal_Aedes_luteocephalus_2005\t10237\n",
      "      @SQ\t11089:ncbi:17\t17\tJX898880\t11089\tncbi\tArD181564_Senegal_2005\t10237\n",
      "      @SQ\t11089:ncbi:18\t18\tJX898877\t11089\tncbi\tArD181464_Senegal_2005\t10237\n",
      "      @SQ\t11089:ncbi:19\t19\tJX898876\t11089\tncbi\tSenegal_Aedes_fucifer_2001\t10237\n",
      "      @SQ\t11089:ncbi:20\t20\tKU978765\t11089\tncbi\tGuinea_Bissau_Hsapiens_1965\t10237\n",
      "      @SQ\t11089:ncbi:21\t21\tJX898870\t11089\tncbi\tSenegal_Ae_fucifer_1996\t10237\n",
      "      @SQ\t11089:ncbi:22\t22\tJX898868\t11089\tncbi\tisolate_HD117294_Senegal_1995\t10237\n",
      "      @SQ\t11089:ncbi:23\t23\tJX898875\t11089\tncbi\tSenegal_Aedes_fucifer_2000\t10237\n",
      "      @SQ\t11089:ncbi:24\t24\tJX898874\t11089\tncbi\tArD149194_Senegal_2000\t10237\n",
      "      @SQ\t11089:ncbi:25\t25\tJX898873\t11089\tncbi\tArD149214_Senegal_2000\t10237\n",
      "      @SQ\t11089:ncbi:26\t26\tMK292067\t11089\tncbi\tNetherlands_Hsapiens_Gambia_2018\t10237\n",
      "      @SQ\t11089:ncbi:27\t27\tMK457701\t11089\tncbi\tNigeria_Hsapiens_2018\t10237\n",
      "      @SQ\t11089:ncbi:28\t28\tMN958078\t11089\tncbi\tNigeria_Hsapiens_2018\t10237\n",
      "      @SQ\t11089:ncbi:29\t29\tJX898869\t11089\tncbi\tCotedIvoire_Ae_africanus_1973\t10237\n",
      "      @SQ\t11089:ncbi:30\t30\tKU978763\t11089\tncbi\tNigeria_Hsapiens_1946\t10237\n",
      "      @SQ\t11089:ncbi:31\t31\tMF004382\t11089\tncbi\tBolivia_Hsapiens_1999\t10237\n",
      "      @SQ\t11089:ncbi:32\t32\tJF912181\t11089\tncbi\tBrazil_Hsapiens_1983\t10237\n",
      "      @SQ\t11089:ncbi:33\t33\tJF912179\t11089\tncbi\tBrazil_Haemagogus_sp_1980\t10237\n",
      "      @SQ\t11089:ncbi:34\t34\tJF912183\t11089\tncbi\tBrazil_Hsapiens_1984\t10237\n",
      "      @SQ\t11089:ncbi:35\t35\tJF912182\t11089\tncbi\tBrazil_Hsapiens_1984\t10237\n",
      "      @SQ\t11089:ncbi:36\t36\tJF912188\t11089\tncbi\tBrazil_Hsapiens_2000\t10237\n",
      "      @SQ\t11089:ncbi:37\t37\tJF912190\t11089\tncbi\tBrazil_Hsapiens_2002\t10237\n",
      "      @SQ\t11089:ncbi:38\t38\tHM582851\t11089\tncbi\tTrinidadandTobago_Alouetta_seniculus_2009\t10237\n",
      "      @SQ\t11089:ncbi:39\t39\tKM388814\t11089\tncbi\tVenezuela_Portuguesa_Hsapiens_2005\t10237\n",
      "      @SQ\t11089:ncbi:40\t40\tKM388815\t11089\tncbi\tVenezuela_Apure_Aseniculus_2007\t10237\n",
      "      @SQ\t11089:ncbi:41\t41\tKM388818\t11089\tncbi\tVenezuela_Barinas_Aseniculus_2006\t10237\n",
      "      @SQ\t11089:ncbi:42\t42\tKM388817\t11089\tncbi\tVenezuela_Guarico_Allouetta_seniculus_2004\t10237\n",
      "      @SQ\t11089:ncbi:43\t43\tKM388816\t11089\tncbi\tVenezuela_Monagas_Asiniculus_2010\t10237\n",
      "      @SQ\t11089:ncbi:44\t44\tMH666058\t11089\tncbi\tBrazil_Sapajus_sp_2016\t10237\n",
      "      @SQ\t11089:ncbi:45\t45\tMK583152\t11089\tncbi\tBrazil_SaoPaulo_Hsapiens_2017\t10237\n",
      "      @SQ\t11089:ncbi:46\t46\tMK583166\t11089\tncbi\tBrazil_SaoPaulo_Hsapiens_2018\t10237\n",
      "      @SQ\t11089:ncbi:47\t47\tMK760660\t11089\tncbi\tNetherlands_brazil_2018\t10237\n",
      "      @SQ\t11089:ncbi:48\t48\tMK760665\t11089\tncbi\tNetherlands_Hsapiens-from-brazil_2018\t10237\n",
      "      @SQ\t11089:ncbi:49\t49\tMF370549\t11089\tncbi\tBrazil_monkey_2015\t10237\n",
      "      @SQ\t11089:ncbi:50\t50\tMF370535\t11089\tncbi\tBrazil_Allouatta_sp_2016\t10237\n",
      "      @SQ\t11089:ncbi:51\t51\tMF370533\t11089\tncbi\tBrazil_Hsapiens_2017\t10237\n",
      "      @SQ\t11089:ncbi:52\t52\tMK333805\t11089\tncbi\tBrazil_IlhaGrande_Sabethes_chloropterus_2018\t10237\n",
      "      @SQ\t11089:ncbi:53\t53\tMF370530\t11089\tncbi\tBrazil_Haemagogus-janthinomys_2017\t10237\n",
      "      @SQ\t11089:ncbi:54\t54\tMW960207\t11089\tncbi\tYellow_fever_YF118_CAR_2018\t10237\n",
      "      @SQ\t11089:ncbi:55\t55\tJN620362\t11089\tncbi\tUganda_Hsapiens_2010\t10237\n",
      "      @SQ\t11089:ncbi:56\t56\tKY495641\t11089\tncbi\tChina_Hsapiens_2016\t10237\n",
      "      @SQ\t11089:ncbi:57\t57\tKU949599\t11089\tncbi\tshanghai_2016\t10237\n",
      "      @SQ\t11089:ncbi:58\t58\tMG589641\t11089\tncbi\tChina_Hsapiens_2016\t10237\n",
      "      @SQ\t11089:ncbi:59\t59\tKX268355\t11089\tncbi\tChina_Hsapiens_2016\t10237\n",
      "      @SQ\t11089:ncbi:60\t60\tKY587416\t11089\tncbi\tChina_Hsapiens_2016\t10237\n",
      "      @SQ\t11089:ncbi:61\t61\tMH633692\t11089\tncbi\tChina_Hsapiens_2016\t10237\n",
      "      @SQ\t11089:ncbi:62\t62\tMF004383\t11089\tncbi\t432429_S4_MF004383\t10237\n",
      "      @SQ\t11089:ncbi:63\t63\tMW960207\t11089\tncbi\typ\t10237\n",
      "      @SQ\t11089:ncbi:64\t64\tON323052\t11089\tncbi\tNigeria_2020\t10237\n",
      "      @SQ\t11089:ncbi:65\t65\tON323053\t11089\tncbi\tNigeria_2020\t10237\n",
      "      @SQ\t11089:ncbi:66\t66\tON323054\t11089\tncbi\tNigeria_2020\t10237\n",
      "      @SQ\t11089:ncbi:67\t67\tOM066735\t11089\tncbi\tVHF-21-014/GHA/Damongo/2021\t10237\n",
      "      @SQ\t11089:ncbi:68\t68\tOM066736\t11089\tncbi\tVHF-21-029/GHA/Daboya/2021\t10237\n",
      "      @SQ\t11089:ncbi:69\t69\tOM066737\t11089\tncbi\tVHF-21-037/GHA/Damongo/2021\t10237\n"
     ]
    }
   ],
   "source": [
    "file_stem = 'single_69seq_150bp'\n",
    "\n",
    "p2aln = pfs.data / f\"ncbi/simreads/yf/{file_stem[:-2] if file_stem[-1] in ['1', '2'] else file_stem}/{file_stem}.aln\"\n",
    "assert p2aln.exists()\n",
    "\n",
    "aln = AlnFileReader(p2aln)\n",
    "print(f\"Reading alignment file: {p2aln.name}:\\n\")\n",
    "for i, aln_read in enumerate(aln):\n",
    "    pass\n",
    "print(f\"  - {i+1:,d} simulated reads in file '{p2aln.name}' from {len(aln.header['reference sequences'])} reference sequences.\")\n",
    "\n",
    "print('  - ART command: ',aln.header['command'])\n",
    "print('  - Reference Sequences:')\n",
    "print('     ','\\n      '.join(aln.header['reference sequences']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. (Optional) Test each inference step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test the steps to prepare model inputs, using a small batch size:\n",
    "1. create the generator using teh `.aln` file to yiel pairs of batches (metadata and reads strings) using `aln.cnn_virus_input_generator`\n",
    "2. transform the batch of string reads into a base hot encoded tensor, using the preprocessing funtion `string_input_batch_to_tensors`\n",
    "3. split each kmer read into (k-49) 50-mer reads to present to the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/vtecftwy/metagentools/blob/main/metagentools/cnn_virus/data.py#LNone){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### AlnFileReader.cnn_virus_input_generator\n",
       "\n",
       ">      AlnFileReader.cnn_virus_input_generator (label:int=118, bs:int=32)\n",
       "\n",
       "Create a generator yielding a metadata batch (dict) and a read batch (tensor of strings)\n",
       "\n",
       "The metadata dictionary contains lists of metadata for each read in the batch. For example:\n",
       "``` \n",
       "{\n",
       "    'readid': ['2591237:ncbi:1-40200','2591237:ncbi:1-40199','2591237:ncbi:1-40198', ...],\n",
       "    'refseqid': ['2591237:ncbi:1','2591237:ncbi:1','2591237:ncbi:1', ...],\n",
       "    'read_pos': [5, 6, 1, ...],\n",
       "    'refsource': ['ncbi', 'ncbi', 'ncbi', ...],\n",
       "    ...\n",
       "}\n",
       "```\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| label | int | 118 | label for this batch (assuming all reads are from the same species) |\n",
       "| bs | int | 32 | batch size |\n",
       "| **Returns** | **tuple** |  | **dict of metadata list and tensor of strings** |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/vtecftwy/metagentools/blob/main/metagentools/cnn_virus/data.py#LNone){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### AlnFileReader.cnn_virus_input_generator\n",
       "\n",
       ">      AlnFileReader.cnn_virus_input_generator (label:int=118, bs:int=32)\n",
       "\n",
       "Create a generator yielding a metadata batch (dict) and a read batch (tensor of strings)\n",
       "\n",
       "The metadata dictionary contains lists of metadata for each read in the batch. For example:\n",
       "``` \n",
       "{\n",
       "    'readid': ['2591237:ncbi:1-40200','2591237:ncbi:1-40199','2591237:ncbi:1-40198', ...],\n",
       "    'refseqid': ['2591237:ncbi:1','2591237:ncbi:1','2591237:ncbi:1', ...],\n",
       "    'read_pos': [5, 6, 1, ...],\n",
       "    'refsource': ['ncbi', 'ncbi', 'ncbi', ...],\n",
       "    ...\n",
       "}\n",
       "```\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| label | int | 118 | label for this batch (assuming all reads are from the same species) |\n",
       "| bs | int | 32 | batch size |\n",
       "| **Returns** | **tuple** |  | **dict of metadata list and tensor of strings** |"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(aln.cnn_virus_input_generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to define the model label for yellow fever and a batch size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yellow_fever_virus. Label: 118\n"
     ]
    }
   ],
   "source": [
    "OriginalLabels().search(s='yellow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/vtecftwy/metagentools/blob/main/metagentools/cnn_virus/data.py#LNone){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### string_input_batch_to_tensors\n",
       "\n",
       ">      string_input_batch_to_tensors (b:tensorflow.python.framework.ops.Tensor,\n",
       ">                                     k:int=150)\n",
       "\n",
       "Function converting a batch of bp strings into three tensors: (x_seqs, (y_labels, y_pos))\n",
       "\n",
       "Expects input strings to have the format: 'read sequence     label   position' where:\n",
       "\n",
       "- read sequence: a string of length k (kmer)\n",
       "- label: an integer between 0 and 186 representing the read virus label\n",
       "- position: an integer between 0 and 9 representing the read position in the genome\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| b | Tensor |  | batch of strings representing the inputs (kmer, label, position) |\n",
       "| k | int | 150 | maximum read length in the batch |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/vtecftwy/metagentools/blob/main/metagentools/cnn_virus/data.py#LNone){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### string_input_batch_to_tensors\n",
       "\n",
       ">      string_input_batch_to_tensors (b:tensorflow.python.framework.ops.Tensor,\n",
       ">                                     k:int=150)\n",
       "\n",
       "Function converting a batch of bp strings into three tensors: (x_seqs, (y_labels, y_pos))\n",
       "\n",
       "Expects input strings to have the format: 'read sequence     label   position' where:\n",
       "\n",
       "- read sequence: a string of length k (kmer)\n",
       "- label: an integer between 0 and 186 representing the read virus label\n",
       "- position: an integer between 0 and 9 representing the read position in the genome\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| b | Tensor |  | batch of strings representing the inputs (kmer, label, position) |\n",
       "| k | int | 150 | maximum read length in the batch |"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(string_input_batch_to_tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review metadata batch yielded by the generator:\n",
      "  List of metadata keys:\n",
      "  - aln_start_pos\n",
      "  - readid\n",
      "  - readnb\n",
      "  - refseq_strand\n",
      "  - refseqid\n",
      "  - refseqnb\n",
      "  - refsource\n",
      "  - reftaxonomyid\n",
      "  - read_pos\n",
      "  'readid' included in this batch': 11089:ncbi:1-17000, 11089:ncbi:1-16999, 11089:ncbi:1-16998, 11089:ncbi:1-16997, 11089:ncbi:1-16996, 11089:ncbi:1-16995, 11089:ncbi:1-16994, 11089:ncbi:1-16993\n",
      "\n",
      "Review batch of string reads yielded by the generator:\n",
      "  - Shape: (8,)\n",
      "\n",
      "Review the read kmer tensor after preprocessing:\n",
      "  - Shape: (8, 150, 5)\n",
      "\n",
      "Review ground truth tensors:\n",
      "  - Shape true label tensor:    (8, 187)\n",
      "  - Shape true position tensor: (8, 10)\n"
     ]
    }
   ],
   "source": [
    "b = 8\n",
    "true_label = 118\n",
    "\n",
    "aln.reset_iterator()\n",
    "for batch_meta, batch_reads in aln.cnn_virus_input_generator(bs=b, label=true_label):\n",
    "    reads_kmer, (labels_kmer, positions_kmer) = string_input_batch_to_tensors(batch_reads, k=150)\n",
    "    break\n",
    "\n",
    "print('Review metadata batch yielded by the generator:')\n",
    "print(f\"  List of metadata keys:\")\n",
    "print('  -','\\n  - '.join(batch_meta.keys()))\n",
    "print(f\"  'readid' included in this batch':\", ', '.join(batch_meta['readid']))\n",
    "print('\\nReview batch of string reads yielded by the generator:')\n",
    "print(f\"  - Shape: {batch_reads.shape}\")\n",
    "print('\\nReview the read kmer tensor after preprocessing:')\n",
    "print(f\"  - Shape: {reads_kmer.shape}\")\n",
    "print('\\nReview ground truth tensors:')\n",
    "print(f\"  - Shape true label tensor:    {labels_kmer.shape}\")\n",
    "print(f\"  - Shape true position tensor: {positions_kmer.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model only accepts 50-mer reads, so we need to split kmer reads into 50mers. For each kmer read, k-49 50mer reads will be generated, by shifting a window of 50 nucleotides by 1 nucleotide at a time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/vtecftwy/metagentools/blob/main/metagentools/cnn_virus/data.py#LNone){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### split_kmer_batch_into_50mers\n",
       "\n",
       ">      split_kmer_batch_into_50mers\n",
       ">                                    (kmer:tensorflow.python.framework.ops.Tenso\n",
       ">                                    r)\n",
       "\n",
       "Converts a batch of k-mer reads into several 50-mer reads, by shifting the k-mer one base at a time.\n",
       "\n",
       "for a batch of `b` k-mer reads, returns a batch of `b - 49` 50-mer reads\n",
       "\n",
       "|    | **Type** | **Details** |\n",
       "| -- | -------- | ----------- |\n",
       "| kmer | Tensor | tensor representing a batch of k-mer reads, after base encoding |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/vtecftwy/metagentools/blob/main/metagentools/cnn_virus/data.py#LNone){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### split_kmer_batch_into_50mers\n",
       "\n",
       ">      split_kmer_batch_into_50mers\n",
       ">                                    (kmer:tensorflow.python.framework.ops.Tenso\n",
       ">                                    r)\n",
       "\n",
       "Converts a batch of k-mer reads into several 50-mer reads, by shifting the k-mer one base at a time.\n",
       "\n",
       "for a batch of `b` k-mer reads, returns a batch of `b - 49` 50-mer reads\n",
       "\n",
       "|    | **Type** | **Details** |\n",
       "| -- | -------- | ----------- |\n",
       "| kmer | Tensor | tensor representing a batch of k-mer reads, after base encoding |"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(split_kmer_batch_into_50mers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Each kmer is split into 101 50-mer reads. Total 50-mer reads in a batch: 808\n",
      "\n",
      "Review the reads tensor:\n",
      "  - Shape kmer tensor:   (8, 150, 5)\n",
      "  - Shape 50-mer tensor: (808, 50, 5)\n"
     ]
    }
   ],
   "source": [
    "reads_50mer = split_kmer_batch_into_50mers(reads_kmer)\n",
    "nb_50mer_per_kmer = reads_kmer.shape[1]-49 \n",
    "nb_50mer_reads = (nb_50mer_per_kmer) * reads_kmer.shape[0]\n",
    "assert reads_50mer.shape == (nb_50mer_reads, 50,5)\n",
    "\n",
    "print(f\"Each kmer is split into {nb_50mer_per_kmer} 50-mer reads. Total 50-mer reads in a batch: {nb_50mer_reads}\")\n",
    "print('\\nReview the reads tensor:')\n",
    "print(f\"  - Shape kmer tensor:   {reads_kmer.shape}\")\n",
    "print(f\"  - Shape 50-mer tensor: {reads_50mer.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If all runs smoothly, our generator and preprocessing are working fine. We can run the prediction loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Run the Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Database functions\n",
    "def open_db(p2db: Path,    #path to the sqlite database\n",
    "            k: int,        # k-mer size\n",
    "            top_n: int=5   # number of top predictions to store, 5 by default\n",
    "            ):\n",
    "    conn = sqlite3.connect(p2db)\n",
    "    cursor = conn.cursor()\n",
    "    query = \"SELECT name FROM sqlite_master WHERE type='table' AND name='predictions'\"\n",
    "    res = cursor.execute(query).fetchone()\n",
    "    if res is None or 'predictions' not in res:\n",
    "        print('Creating tables in database...')\n",
    "        create_tables(cursor, k)\n",
    "    return conn\n",
    "\n",
    "def table_columns(cursor:sqlite3.Cursor,  # cursor to the database  \n",
    "                  table: str              # name of the table\n",
    "                  ):\n",
    "    \"\"\"Returns the name of the columns in the passed table\"\"\"\n",
    "    query = f\"PRAGMA table_info({table})\"\n",
    "    cursor.execute(query)\n",
    "    cols = [row[1] for row in cursor.fetchall()]\n",
    "    return cols\n",
    "\n",
    "def create_tables(cursor: sqlite3.Cursor,   # cursor to the database  \n",
    "                  k:int,                    # k-mer size\n",
    "                  top_n: int=5              # number of top predictions to store, 5 by default   \n",
    "                  ):\n",
    "    \"\"\"Create tables for the database\"\"\"\n",
    "    # Create table for predictions\n",
    "    pred_cols_str = 'readid refseqid refsource refseq_strand taxonomyid'.split(' ')\n",
    "    pred_cols_int = 'lbl_true lbl_pred pos_true pos_pred'.split(' ')\n",
    "    top_pred_cols = [f\"top_{top_n}_lbl_pred_{i}\" for i in range(top_n)]\n",
    "\n",
    "    query = \"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS predictions (\n",
    "        id INTEGER PRIMARY KEY,\n",
    "    \"\"\"\n",
    "    for col in pred_cols_str:\n",
    "        query += f\"{col} TEXT, \"\n",
    "    for col in pred_cols_int:\n",
    "        query += f\"{col} INTEGER, \"\n",
    "    for col in top_pred_cols:\n",
    "        query += f\"{col} INTEGER, \"\n",
    "    query = query[:-2]+')'\n",
    "    # print(query)\n",
    "    cursor.execute(query)\n",
    "\n",
    "    query = \"CREATE INDEX idx_preds ON predictions (readid, refseqid, pos_true);\"\n",
    "    cursor.execute(query)\n",
    "    print('Table `predictions` created.')\n",
    "\n",
    "    # Create table for probabilities (one per 50-mer in order to keep small nb or columns in table)\n",
    "\n",
    "    query = f\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS label_probabilities (\n",
    "        id INTEGER PRIMARY KEY,\n",
    "        read_kmer_id TEXT,\n",
    "        read_50mer_nb INTEGER,\n",
    "    \"\"\"\n",
    "    query += ','.join([f\"prob_{i:03d}\" for i in range(187)]) + \" REAL, \"\n",
    "    query += \"FOREIGN KEY (read_kmer_id) REFERENCES predictions(readid)\"\n",
    "    query += ')'\n",
    "    cursor.execute(query)\n",
    "\n",
    "    query = \"CREATE INDEX idx_probs ON label_probabilities (read_kmer_id, read_50mer_nb);\"\n",
    "    cursor.execute(query)\n",
    "    print(f'Table `label_probabilities` created.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def skip_existing_predictions(gen: Generator,       # generator of batches (metadata, reads)\n",
    "                              p2db: Path,           # path to the sqlite database \n",
    "                              bs: int               # batch size\n",
    "                              ) -> Tuple[int, int]: # number of batches and kmer reads skipped\n",
    "    \n",
    "    # Identify the readnb for the last saved prediction\n",
    "    print('Checking predictions already in database...')\n",
    "    with open_db(p2db=p2db, k=150) as conn:\n",
    "        last_predictions_id = conn.execute(\"SELECT MAX(id) FROM predictions\").fetchone()[0]\n",
    "        if last_predictions_id is None:\n",
    "            return 0, 0\n",
    "        else:\n",
    "            nb_predictions = conn.execute(\"SELECT COUNT(*) FROM predictions\").fetchone()[0]\n",
    "            last_readid = conn.execute(f\"SELECT readid FROM predictions WHERE id = {last_predictions_id};\").fetchone()[0]\n",
    "        \n",
    "    print(f\"Database includes {nb_predictions:,d} predictions, corresponding to {nb_predictions//bs:,d} batches\")\n",
    "    print(f\"Last prediction id: {last_predictions_id:,d} for kmer read '{last_readid}'\")\n",
    "    print(f\"Skipping already processed predictions ...\")\n",
    "\n",
    "    for i, (batch_meta, batch_reads) in enumerate(gen):\n",
    "        if i%100 == 0: \n",
    "            print(f\"   Skipped first {(i+1)*bs:,d} kmer reads ({i+1:,d} batches)\")\n",
    " \n",
    "        if last_readid in batch_meta['readid']:\n",
    "            print(f\"   Reached last batch of saved prediction (batch {i+1:,d})\")\n",
    "            print('Can procees with normal prediction inference')\n",
    "            nb_kmer_reads_skipped = nb_predictions\n",
    "            nb_batches_skipped = i+1\n",
    "            break\n",
    "    return nb_batches_skipped, nb_kmer_reads_skipped\n",
    "\n",
    "# aln.reset_iterator()\n",
    "# gen2 = aln.cnn_virus_input_generator(bs=512, label=118)\n",
    "# nbs, nrs = skip_existing_predictions(gen=gen2, p2db=p2db, bs=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_predictions(probs, n=5):\n",
    "    \"\"\"Returns the top n top predictions for each kmer read\"\"\"\n",
    "\n",
    "    def top_n_most_frequent(preds, n=5):\n",
    "        \"\"\"Returns the top n most frequent predictions for each 50read\"\"\"\n",
    "        # print(preds.shape)\n",
    "        uniques, counts = np.unique(preds, return_counts=True)\n",
    "        top_idx = np.argsort(counts)[-n:]\n",
    "        return uniques.take(top_idx)\n",
    "\n",
    "    top_preds_in_50mers = np.argsort(probs, axis=-1)[:, :, -n:]\n",
    "    nb_kmers, nb_50mers, nb_lbls = top_preds_in_50mers.shape\n",
    "    # print(top_preds_in_50mers.shape)\n",
    "    top_preds_in_kmer = top_preds_in_50mers.reshape(nb_kmers,nb_50mers * nb_lbls)\n",
    "    # print(top_preds_in_kmer.shape)\n",
    "\n",
    "    return np.apply_along_axis(top_n_most_frequent, axis=1, arr=top_preds_in_kmer, n=n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference for 25% of the simreads on 69 sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "simreads\n",
      "  |--yf\n",
      "  |    |--single_1seq_150bp\n",
      "  |    |--single_69seq_150bp\n",
      "  |    |    |--single_69seq_150bp.fq (0)\n",
      "  |    |    |--single_69seq_150bp.aln (1)\n",
      "  |    |--paired_1seq_150bp\n",
      "  |    |--paired_69seq_150bp\n",
      "  |    |    |--paired_69seq_150bp1.fq (2)\n",
      "  |    |    |--paired_69seq_150bp2.fq (3)\n",
      "  |    |    |--paired_69seq_150bp1.aln (4)\n",
      "  |    |    |--paired_69seq_150bp2.aln (5)\n"
     ]
    }
   ],
   "source": [
    "fnames = files_in_tree(pfs.data / 'ncbi/simreads/yf', pattern='69seq')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading alignment file: 'single_69seq_150bp.aln' (in /home/vtec/projects/bio/metagentools/data/ncbi/simreads/yf/single_69seq_150bp)\n",
      "\n",
      "  - 1,161,034 simulated reads in file 'single_69seq_150bp.aln' from 69 reference sequences.\n",
      "  - ART command:  /usr/bin/art_illumina -i /home/vtec/projects/bio/metagentools/data/ncbi/refsequences/yf/yf_2023_yellow_fever.fa -ss HS25 -l 150 -f 250 -o /home/vtec/projects/bio/metagentools/data/ncbi/simreads/yf/single_69seq_150bp/single_69seq_150bp -rs 1724163599\n",
      "  - Reference Sequences:\n",
      "      @SQ\t11089:ncbi:1\t1\tAY968064\t11089\tncbi\tAngola_1971\t10237\n",
      "      @SQ\t11089:ncbi:2\t2\tU54798\t11089\tncbi\tIvory_Coast_1982\t10237\n",
      "      @SQ\t11089:ncbi:3\t3\tDQ235229\t11089\tncbi\tEthiopia_1961\t10237\n",
      "      @SQ\t11089:ncbi:4\t4\tAY572535\t11089\tncbi\tGambia_2001\t10237\n",
      "      @SQ\t11089:ncbi:5\t5\tMF405338\t11089\tncbi\tGhana_Hsapiens_1927\t10237\n",
      "      @SQ\t11089:ncbi:6\t6\tU21056\t11089\tncbi\tSenegal_1927\t10237\n",
      "      @SQ\t11089:ncbi:7\t7\tAY968065\t11089\tncbi\tUganda_1948\t10237\n",
      "      @SQ\t11089:ncbi:8\t8\tJX898871\t11089\tncbi\tArD114896_Senegal_1995\t10237\n",
      "      @SQ\t11089:ncbi:9\t9\tJX898872\t11089\tncbi\tSenegal_Aedes-aegypti_1995\t10237\n",
      "      @SQ\t11089:ncbi:10\t10\tGQ379163\t11089\tncbi\tPeru_Hsapiens_2007\t10237\n",
      "      @SQ\t11089:ncbi:11\t11\tDQ118157\t11089\tncbi\tSpain_Vaccine_2004\t10237\n",
      "      @SQ\t11089:ncbi:12\t12\tMF289572\t11089\tncbi\tSingapore_2017\t10237\n",
      "      @SQ\t11089:ncbi:13\t13\tKU978764\t11089\tncbi\tSudan_Hsapiens_1941\t10237\n",
      "      @SQ\t11089:ncbi:14\t14\tJX898878\t11089\tncbi\tArD181250_Senegal_2005\t10237\n",
      "      @SQ\t11089:ncbi:15\t15\tJX898879\t11089\tncbi\tArD181676_Senegal_2005\t10237\n",
      "      @SQ\t11089:ncbi:16\t16\tJX898881\t11089\tncbi\tSenegal_Aedes_luteocephalus_2005\t10237\n",
      "      @SQ\t11089:ncbi:17\t17\tJX898880\t11089\tncbi\tArD181564_Senegal_2005\t10237\n",
      "      @SQ\t11089:ncbi:18\t18\tJX898877\t11089\tncbi\tArD181464_Senegal_2005\t10237\n",
      "      @SQ\t11089:ncbi:19\t19\tJX898876\t11089\tncbi\tSenegal_Aedes_fucifer_2001\t10237\n",
      "      @SQ\t11089:ncbi:20\t20\tKU978765\t11089\tncbi\tGuinea_Bissau_Hsapiens_1965\t10237\n",
      "      @SQ\t11089:ncbi:21\t21\tJX898870\t11089\tncbi\tSenegal_Ae_fucifer_1996\t10237\n",
      "      @SQ\t11089:ncbi:22\t22\tJX898868\t11089\tncbi\tisolate_HD117294_Senegal_1995\t10237\n",
      "      @SQ\t11089:ncbi:23\t23\tJX898875\t11089\tncbi\tSenegal_Aedes_fucifer_2000\t10237\n",
      "      @SQ\t11089:ncbi:24\t24\tJX898874\t11089\tncbi\tArD149194_Senegal_2000\t10237\n",
      "      @SQ\t11089:ncbi:25\t25\tJX898873\t11089\tncbi\tArD149214_Senegal_2000\t10237\n",
      "      @SQ\t11089:ncbi:26\t26\tMK292067\t11089\tncbi\tNetherlands_Hsapiens_Gambia_2018\t10237\n",
      "      @SQ\t11089:ncbi:27\t27\tMK457701\t11089\tncbi\tNigeria_Hsapiens_2018\t10237\n",
      "      @SQ\t11089:ncbi:28\t28\tMN958078\t11089\tncbi\tNigeria_Hsapiens_2018\t10237\n",
      "      @SQ\t11089:ncbi:29\t29\tJX898869\t11089\tncbi\tCotedIvoire_Ae_africanus_1973\t10237\n",
      "      @SQ\t11089:ncbi:30\t30\tKU978763\t11089\tncbi\tNigeria_Hsapiens_1946\t10237\n",
      "      @SQ\t11089:ncbi:31\t31\tMF004382\t11089\tncbi\tBolivia_Hsapiens_1999\t10237\n",
      "      @SQ\t11089:ncbi:32\t32\tJF912181\t11089\tncbi\tBrazil_Hsapiens_1983\t10237\n",
      "      @SQ\t11089:ncbi:33\t33\tJF912179\t11089\tncbi\tBrazil_Haemagogus_sp_1980\t10237\n",
      "      @SQ\t11089:ncbi:34\t34\tJF912183\t11089\tncbi\tBrazil_Hsapiens_1984\t10237\n",
      "      @SQ\t11089:ncbi:35\t35\tJF912182\t11089\tncbi\tBrazil_Hsapiens_1984\t10237\n",
      "      @SQ\t11089:ncbi:36\t36\tJF912188\t11089\tncbi\tBrazil_Hsapiens_2000\t10237\n",
      "      @SQ\t11089:ncbi:37\t37\tJF912190\t11089\tncbi\tBrazil_Hsapiens_2002\t10237\n",
      "      @SQ\t11089:ncbi:38\t38\tHM582851\t11089\tncbi\tTrinidadandTobago_Alouetta_seniculus_2009\t10237\n",
      "      @SQ\t11089:ncbi:39\t39\tKM388814\t11089\tncbi\tVenezuela_Portuguesa_Hsapiens_2005\t10237\n",
      "      @SQ\t11089:ncbi:40\t40\tKM388815\t11089\tncbi\tVenezuela_Apure_Aseniculus_2007\t10237\n",
      "      @SQ\t11089:ncbi:41\t41\tKM388818\t11089\tncbi\tVenezuela_Barinas_Aseniculus_2006\t10237\n",
      "      @SQ\t11089:ncbi:42\t42\tKM388817\t11089\tncbi\tVenezuela_Guarico_Allouetta_seniculus_2004\t10237\n",
      "      @SQ\t11089:ncbi:43\t43\tKM388816\t11089\tncbi\tVenezuela_Monagas_Asiniculus_2010\t10237\n",
      "      @SQ\t11089:ncbi:44\t44\tMH666058\t11089\tncbi\tBrazil_Sapajus_sp_2016\t10237\n",
      "      @SQ\t11089:ncbi:45\t45\tMK583152\t11089\tncbi\tBrazil_SaoPaulo_Hsapiens_2017\t10237\n",
      "      @SQ\t11089:ncbi:46\t46\tMK583166\t11089\tncbi\tBrazil_SaoPaulo_Hsapiens_2018\t10237\n",
      "      @SQ\t11089:ncbi:47\t47\tMK760660\t11089\tncbi\tNetherlands_brazil_2018\t10237\n",
      "      @SQ\t11089:ncbi:48\t48\tMK760665\t11089\tncbi\tNetherlands_Hsapiens-from-brazil_2018\t10237\n",
      "      @SQ\t11089:ncbi:49\t49\tMF370549\t11089\tncbi\tBrazil_monkey_2015\t10237\n",
      "      @SQ\t11089:ncbi:50\t50\tMF370535\t11089\tncbi\tBrazil_Allouatta_sp_2016\t10237\n",
      "      @SQ\t11089:ncbi:51\t51\tMF370533\t11089\tncbi\tBrazil_Hsapiens_2017\t10237\n",
      "      @SQ\t11089:ncbi:52\t52\tMK333805\t11089\tncbi\tBrazil_IlhaGrande_Sabethes_chloropterus_2018\t10237\n",
      "      @SQ\t11089:ncbi:53\t53\tMF370530\t11089\tncbi\tBrazil_Haemagogus-janthinomys_2017\t10237\n",
      "      @SQ\t11089:ncbi:54\t54\tMW960207\t11089\tncbi\tYellow_fever_YF118_CAR_2018\t10237\n",
      "      @SQ\t11089:ncbi:55\t55\tJN620362\t11089\tncbi\tUganda_Hsapiens_2010\t10237\n",
      "      @SQ\t11089:ncbi:56\t56\tKY495641\t11089\tncbi\tChina_Hsapiens_2016\t10237\n",
      "      @SQ\t11089:ncbi:57\t57\tKU949599\t11089\tncbi\tshanghai_2016\t10237\n",
      "      @SQ\t11089:ncbi:58\t58\tMG589641\t11089\tncbi\tChina_Hsapiens_2016\t10237\n",
      "      @SQ\t11089:ncbi:59\t59\tKX268355\t11089\tncbi\tChina_Hsapiens_2016\t10237\n",
      "      @SQ\t11089:ncbi:60\t60\tKY587416\t11089\tncbi\tChina_Hsapiens_2016\t10237\n",
      "      @SQ\t11089:ncbi:61\t61\tMH633692\t11089\tncbi\tChina_Hsapiens_2016\t10237\n",
      "      @SQ\t11089:ncbi:62\t62\tMF004383\t11089\tncbi\t432429_S4_MF004383\t10237\n",
      "      @SQ\t11089:ncbi:63\t63\tMW960207\t11089\tncbi\typ\t10237\n",
      "      @SQ\t11089:ncbi:64\t64\tON323052\t11089\tncbi\tNigeria_2020\t10237\n",
      "      @SQ\t11089:ncbi:65\t65\tON323053\t11089\tncbi\tNigeria_2020\t10237\n",
      "      @SQ\t11089:ncbi:66\t66\tON323054\t11089\tncbi\tNigeria_2020\t10237\n",
      "      @SQ\t11089:ncbi:67\t67\tOM066735\t11089\tncbi\tVHF-21-014/GHA/Damongo/2021\t10237\n",
      "      @SQ\t11089:ncbi:68\t68\tOM066736\t11089\tncbi\tVHF-21-029/GHA/Daboya/2021\t10237\n",
      "      @SQ\t11089:ncbi:69\t69\tOM066737\t11089\tncbi\tVHF-21-037/GHA/Damongo/2021\t10237\n"
     ]
    }
   ],
   "source": [
    "file_stem = 'single_69seq_150bp'\n",
    "\n",
    "p2aln = pfs.data / f\"ncbi/simreads/yf/{file_stem[:-2] if file_stem[-1] in ['1', '2'] else file_stem}/{file_stem}.aln\"\n",
    "assert p2aln.exists()\n",
    "\n",
    "aln = AlnFileReader(p2aln)\n",
    "print(f\"Reading alignment file: '{p2aln.name}' (in {p2aln.parent})\\n\")\n",
    "for i, aln_read in enumerate(aln):\n",
    "    pass\n",
    "nb_kmer_reads = i\n",
    "print(f\"  - {i+1:,d} simulated reads in file '{p2aln.name}' from {len(aln.header['reference sequences'])} reference sequences.\")\n",
    "\n",
    "print('  - ART command: ',aln.header['command'])\n",
    "print('  - Reference Sequences:')\n",
    "print('     ','\\n      '.join(aln.header['reference sequences']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'single_69seq_150bp.aln' used for prediction (in /home/vtec/projects/bio/metagentools/data/ncbi/simreads/yf/single_69seq_150bp).\n"
     ]
    }
   ],
   "source": [
    "print(f\"'{aln.path.name}' used for prediction (in {aln.path.parent}).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'single_69seq_150bp.db' sqlite db used, (in /mnt/k/metagentools/ncbi/infer_results/yf-ncbi)\n"
     ]
    }
   ],
   "source": [
    "# p2db = pfs.data / 'ncbi/infer_results/yf-ncbi' / f'{p2aln.stem}.db'\n",
    "p2db = Path('/mnt/k/metagentools') / 'ncbi/infer_results/yf-ncbi' / f'{p2aln.stem}.db'\n",
    "print(f\"'{p2db.name}' sqlite db used, (in {p2db.parent})\")\n",
    "\n",
    "nl = '\\n'\n",
    "msg = f\"Are you sure you want to use this database?{nl}Database file '{p2db.name}' does not correspond to aln '{aln.path.name}'\"\n",
    "if aln.path.stem != p2db.stem: raise Warning(msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **NOTE**\n",
    ">\n",
    "> Estimated space required to save prediction and probability reports for the simreads simulated on 69 sequence is *470 Gb*. \n",
    ">\n",
    "> This is currently too large to save even on my NAS. \n",
    ">\n",
    "> Will first build a prediction dataset with 25% of the total reads: `nb_batches_to_run = int(nb_kmer_reads / b * 0.25)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Review and set following parameters\n",
    "\n",
    "# 2. Data parameters\n",
    "b = 512             # number of k-mer in a batch\n",
    "k = 150             # read length\n",
    "true_label = 118    # yellow fever virus\n",
    "top_n = 5           # n for top-n prediction to keep\n",
    "\n",
    "# 3. Inference loop parameters\n",
    "run_all_batches = False\n",
    "# nb_batches_to_run = 4\n",
    "nb_batches_to_run = int(nb_kmer_reads / b * 0.25)\n",
    "\n",
    "#====================================================================================================\n",
    "# Setup prediction Loop\n",
    "#====================================================================================================\n",
    "nb_50mer = k - 49\n",
    "uid = datetime.today().strftime('%Y-%m-%d_%H_%M_%S')\n",
    "\n",
    "aln.reset_iterator()\n",
    "model = create_model_original(path2parameters=p2model)\n",
    "print(f\"Model loaded and ready to run ...\")\n",
    "\n",
    "# Open connection to sqlite db\n",
    "conn = open_db(p2db, k=k)\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Create list of columns for prediction and probabilities reports\n",
    "pred_cols_str = 'readid refseqid refsource refseq_strand taxonomyid'.split(' ')\n",
    "pred_cols_int = 'lbl_true lbl_pred pos_true pos_pred'.split(' ')\n",
    "top_pred_cols = [f\"top_{top_n}_lbl_pred_{i}\" for i in range(top_n)]\n",
    "prob_cols = [f\"prob_{i:03d}\" for i in range(187)]\n",
    "\n",
    "def tprint(string):\n",
    "    print(f\"{datetime.now().strftime('%H:%M:%S')}    {string}\")\n",
    "\n",
    "#====================================================================================================\n",
    "# Setup prediction Loop\n",
    "#====================================================================================================\n",
    "print(f\"Run prediction loop with the following parameters:\")\n",
    "print(f\"   {b} k-mer per batch; {k} bp per sequence; keep top-{top_n} predictions\")\n",
    "tprint(f\"Starting prediction loop ...\")\n",
    "gen = aln.cnn_virus_input_generator(bs=b, label=true_label)\n",
    "\n",
    "# Skip kmer reads that are already processed\n",
    "nb_batches_skipped, nb_kmer_reads_skipped = skip_existing_predictions(gen=gen, p2db=p2db, bs=b)\n",
    "tprint(f\"Skipped {nb_batches_skipped:,d} batches ({nb_kmer_reads_skipped:,d} kmer reads)\")\n",
    "\n",
    "# Proceed with prediction inference \n",
    "for i,(metadata_batch, reads_batch) in enumerate(gen):\n",
    "    loop_start = datetime.now()\n",
    "    tprint(f\"Batch {i+1:3,d} (aln batch {nb_batches_skipped+i+1:3,d}) ...\")\n",
    "\n",
    "    reads_kmer, (labels_true, position_true) = string_input_batch_to_tensors(reads_batch, k=k)\n",
    "    reads_50mer = split_kmer_batch_into_50mers(reads_kmer)\n",
    "    assert reads_50mer.shape == ((reads_kmer.shape[1]-49) * b, 50, 5), f\"Problem with shape in batch {i+1}: {reads_50mer.shape}\"\n",
    "\n",
    "    tprint(f'  Starting prediction for {b:,} kmer reads ...')\n",
    "    label_probs, position_probs = model.predict(reads_50mer)\n",
    "\n",
    "    tprint('  Reshaping predictions ...')\n",
    "    label_probs_kmer = tf.reshape(label_probs, shape=(b,nb_50mer,-1))\n",
    "    position_probs_kmer = tf.reshape(position_probs, shape=(b,nb_50mer,-1))\n",
    "\n",
    "    tprint('  Combining predictions ...')\n",
    "    combined_predictions = tf.map_fn(\n",
    "        fn=combine_prediction_batch,\n",
    "        elems=[label_probs_kmer, position_probs_kmer], \n",
    "        fn_output_signature=tf.int64\n",
    "        )\n",
    "\n",
    "    label_predictions = combined_predictions[:,0]\n",
    "    position_predictions = combined_predictions[:,1]\n",
    "    top_preds = top_predictions(label_probs_kmer, n=top_n)\n",
    "\n",
    "    # Add results for current batch\n",
    "    tprint('  Preparing prediction report ...')\n",
    "    preds_report = np.concatenate(\n",
    "        [\n",
    "            np.expand_dims(np.array(metadata_batch['readid']), axis=1),         # readid \n",
    "            np.expand_dims(np.array(metadata_batch['refseqid']), axis=1),       # refseqid\n",
    "            np.expand_dims(np.array(metadata_batch['refsource']), axis=1),      # refsource\n",
    "            np.expand_dims(np.array(metadata_batch['refseq_strand']), axis=1),  # refseq_strand\n",
    "            np.expand_dims(np.array(metadata_batch['reftaxonomyid']), axis=1),  # taxonomyid\n",
    "            np.expand_dims(np.array([true_label]*b), axis=1),                   # lbl_true\n",
    "            np.expand_dims(label_predictions, axis=1),                          # lbl_pred\n",
    "            np.expand_dims(np.array(metadata_batch['aln_start_pos']), axis=1),  # pos_true\n",
    "            np.expand_dims(position_predictions, axis=1),                       # pos_pred\n",
    "            top_preds[:, ::-1],                                                 # top_5_lbl_pred_0, top_5_lbl_pred_1, top_5_lbl_pred_2, top_5_lbl_pred_3, top_5_lbl_pred_4\n",
    "        ],\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "    df_preds = pd.DataFrame(\n",
    "        data=preds_report, \n",
    "        columns=pred_cols_str + pred_cols_int + top_pred_cols\n",
    "        )\n",
    "    tprint('  Saving batch prediction report to db...')\n",
    "    df_preds.to_sql('predictions', conn, if_exists='append', index=False)\n",
    "\n",
    "    tprint('  Preparing label probabilities report ...')\n",
    "    df_probs = None\n",
    "    for read_50mer_nb in range(nb_50mer):\n",
    "        probs_report = np.concatenate(\n",
    "            [\n",
    "                np.expand_dims(np.array(metadata_batch['readid']), axis=1),        # readid \n",
    "                np.expand_dims(np.array([read_50mer_nb]*b), axis=1),               # read_50mer_nb\n",
    "                label_probs_kmer[:, read_50mer_nb, :]\n",
    "            ],\n",
    "            axis=1\n",
    "        )\n",
    "\n",
    "        df = pd.DataFrame(\n",
    "            data=probs_report, \n",
    "            columns=['read_kmer_id', 'read_50mer_nb'] + prob_cols\n",
    "            )\n",
    "        df_probs = df if df_probs is None else pd.concat([df_probs, df], axis=0)\n",
    "\n",
    "    tprint('  Saving batch label probabilities report to db...')\n",
    "    df_probs.to_sql('label_probabilities', conn, if_exists='append', index=False)\n",
    "\n",
    "    tprint(f\"  Batch processing time: {(datetime.now() - loop_start).total_seconds():.2f} sec\")\n",
    "    if not run_all_batches and i+1 >= nb_batches_to_run: \n",
    "        print('Stopping')\n",
    "        break\n",
    "\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>readid</th>\n",
       "      <th>refseqid</th>\n",
       "      <th>refsource</th>\n",
       "      <th>refseq_strand</th>\n",
       "      <th>taxonomyid</th>\n",
       "      <th>lbl_true</th>\n",
       "      <th>lbl_pred</th>\n",
       "      <th>pos_true</th>\n",
       "      <th>pos_pred</th>\n",
       "      <th>top_5_lbl_pred_0</th>\n",
       "      <th>top_5_lbl_pred_1</th>\n",
       "      <th>top_5_lbl_pred_2</th>\n",
       "      <th>top_5_lbl_pred_3</th>\n",
       "      <th>top_5_lbl_pred_4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>11089:ncbi:1-17000</td>\n",
       "      <td>11089:ncbi:1</td>\n",
       "      <td>ncbi</td>\n",
       "      <td>-</td>\n",
       "      <td>11089</td>\n",
       "      <td>118</td>\n",
       "      <td>10</td>\n",
       "      <td>7804</td>\n",
       "      <td>0</td>\n",
       "      <td>118</td>\n",
       "      <td>32</td>\n",
       "      <td>10</td>\n",
       "      <td>62</td>\n",
       "      <td>94</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id              readid      refseqid refsource refseq_strand taxonomyid  \\\n",
       "0   1  11089:ncbi:1-17000  11089:ncbi:1      ncbi             -      11089   \n",
       "\n",
       "   lbl_true  lbl_pred  pos_true  pos_pred  top_5_lbl_pred_0  top_5_lbl_pred_1  \\\n",
       "0       118        10      7804         0               118                32   \n",
       "\n",
       "   top_5_lbl_pred_2  top_5_lbl_pred_3  top_5_lbl_pred_4  \n",
       "0                10                62                94  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>read_kmer_id</th>\n",
       "      <th>read_50mer_nb</th>\n",
       "      <th>prob_000</th>\n",
       "      <th>prob_001</th>\n",
       "      <th>prob_002</th>\n",
       "      <th>prob_003</th>\n",
       "      <th>prob_004</th>\n",
       "      <th>prob_005</th>\n",
       "      <th>prob_006</th>\n",
       "      <th>...</th>\n",
       "      <th>prob_177</th>\n",
       "      <th>prob_178</th>\n",
       "      <th>prob_179</th>\n",
       "      <th>prob_180</th>\n",
       "      <th>prob_181</th>\n",
       "      <th>prob_182</th>\n",
       "      <th>prob_183</th>\n",
       "      <th>prob_184</th>\n",
       "      <th>prob_185</th>\n",
       "      <th>prob_186</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>11089:ncbi:1-17000</td>\n",
       "      <td>0</td>\n",
       "      <td>9.452837e-16</td>\n",
       "      <td>1.8033755e-24</td>\n",
       "      <td>3.709243e-18</td>\n",
       "      <td>3.2934015e-16</td>\n",
       "      <td>1.11393635e-13</td>\n",
       "      <td>4.909559e-09</td>\n",
       "      <td>3.2027174e-19</td>\n",
       "      <td>...</td>\n",
       "      <td>4.620669e-10</td>\n",
       "      <td>2.1956457e-23</td>\n",
       "      <td>4.973716e-17</td>\n",
       "      <td>1.2870734e-19</td>\n",
       "      <td>6.685301e-13</td>\n",
       "      <td>2.7962992e-13</td>\n",
       "      <td>5.0206733e-22</td>\n",
       "      <td>2.3932547e-15</td>\n",
       "      <td>1.2987661e-21</td>\n",
       "      <td>3.557021e-18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>513</td>\n",
       "      <td>11089:ncbi:1-17000</td>\n",
       "      <td>1</td>\n",
       "      <td>9.987287e-13</td>\n",
       "      <td>9.640493e-23</td>\n",
       "      <td>6.864665e-13</td>\n",
       "      <td>5.297059e-17</td>\n",
       "      <td>5.2123617e-09</td>\n",
       "      <td>1.9526384e-07</td>\n",
       "      <td>6.811955e-14</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00029164975</td>\n",
       "      <td>9.457506e-20</td>\n",
       "      <td>4.247617e-15</td>\n",
       "      <td>2.2502056e-20</td>\n",
       "      <td>1.3939608e-09</td>\n",
       "      <td>4.1289537e-12</td>\n",
       "      <td>1.0989764e-16</td>\n",
       "      <td>5.3598934e-11</td>\n",
       "      <td>1.09030056e-14</td>\n",
       "      <td>1.850025e-18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1025</td>\n",
       "      <td>11089:ncbi:1-17000</td>\n",
       "      <td>2</td>\n",
       "      <td>4.1150392e-14</td>\n",
       "      <td>1.0409154e-21</td>\n",
       "      <td>3.1731182e-18</td>\n",
       "      <td>4.837065e-13</td>\n",
       "      <td>1.8273633e-12</td>\n",
       "      <td>5.6225136e-13</td>\n",
       "      <td>1.5127502e-14</td>\n",
       "      <td>...</td>\n",
       "      <td>2.7924674e-08</td>\n",
       "      <td>9.881187e-25</td>\n",
       "      <td>1.2789581e-19</td>\n",
       "      <td>5.2265872e-23</td>\n",
       "      <td>3.6289507e-16</td>\n",
       "      <td>5.854743e-18</td>\n",
       "      <td>4.597412e-22</td>\n",
       "      <td>2.7675505e-16</td>\n",
       "      <td>3.6334886e-19</td>\n",
       "      <td>7.191011e-22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1537</td>\n",
       "      <td>11089:ncbi:1-17000</td>\n",
       "      <td>3</td>\n",
       "      <td>2.4102365e-13</td>\n",
       "      <td>3.9195824e-19</td>\n",
       "      <td>4.2152547e-16</td>\n",
       "      <td>5.363886e-15</td>\n",
       "      <td>7.9354416e-13</td>\n",
       "      <td>1.9598592e-10</td>\n",
       "      <td>2.0204985e-14</td>\n",
       "      <td>...</td>\n",
       "      <td>2.3507164e-12</td>\n",
       "      <td>3.4494873e-22</td>\n",
       "      <td>1.4030163e-15</td>\n",
       "      <td>5.3813346e-20</td>\n",
       "      <td>2.1669545e-18</td>\n",
       "      <td>3.9168753e-20</td>\n",
       "      <td>4.6393943e-18</td>\n",
       "      <td>2.162604e-17</td>\n",
       "      <td>5.868014e-17</td>\n",
       "      <td>3.671587e-12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2049</td>\n",
       "      <td>11089:ncbi:1-17000</td>\n",
       "      <td>4</td>\n",
       "      <td>5.402446e-15</td>\n",
       "      <td>1.3982798e-24</td>\n",
       "      <td>5.1237304e-19</td>\n",
       "      <td>2.6393469e-12</td>\n",
       "      <td>7.070969e-12</td>\n",
       "      <td>4.5762413e-08</td>\n",
       "      <td>3.941572e-13</td>\n",
       "      <td>...</td>\n",
       "      <td>2.2205168e-10</td>\n",
       "      <td>2.4770586e-20</td>\n",
       "      <td>2.2223746e-16</td>\n",
       "      <td>1.3230725e-16</td>\n",
       "      <td>1.473535e-13</td>\n",
       "      <td>1.0368611e-15</td>\n",
       "      <td>7.554584e-18</td>\n",
       "      <td>6.905381e-15</td>\n",
       "      <td>6.823121e-19</td>\n",
       "      <td>5.281248e-14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>49153</td>\n",
       "      <td>11089:ncbi:1-17000</td>\n",
       "      <td>96</td>\n",
       "      <td>1.266811e-12</td>\n",
       "      <td>1.7694516e-13</td>\n",
       "      <td>6.9926794e-11</td>\n",
       "      <td>7.442386e-16</td>\n",
       "      <td>2.1827995e-12</td>\n",
       "      <td>2.2899127e-11</td>\n",
       "      <td>6.6601347e-13</td>\n",
       "      <td>...</td>\n",
       "      <td>2.1055688e-09</td>\n",
       "      <td>2.6956715e-11</td>\n",
       "      <td>4.02884e-10</td>\n",
       "      <td>3.6730546e-06</td>\n",
       "      <td>7.332998e-13</td>\n",
       "      <td>9.214267e-17</td>\n",
       "      <td>4.41918e-18</td>\n",
       "      <td>1.0378567e-09</td>\n",
       "      <td>3.9226554e-16</td>\n",
       "      <td>1.512387e-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>49665</td>\n",
       "      <td>11089:ncbi:1-17000</td>\n",
       "      <td>97</td>\n",
       "      <td>3.6751678e-11</td>\n",
       "      <td>5.2148962e-11</td>\n",
       "      <td>1.1142021e-07</td>\n",
       "      <td>1.3979555e-13</td>\n",
       "      <td>0.00023110698</td>\n",
       "      <td>1.8854205e-11</td>\n",
       "      <td>1.128952e-07</td>\n",
       "      <td>...</td>\n",
       "      <td>3.426913e-08</td>\n",
       "      <td>3.5629816e-17</td>\n",
       "      <td>2.7815642e-11</td>\n",
       "      <td>7.9123413e-07</td>\n",
       "      <td>9.112657e-12</td>\n",
       "      <td>1.8065088e-14</td>\n",
       "      <td>2.2273976e-15</td>\n",
       "      <td>2.6934228e-07</td>\n",
       "      <td>3.2327324e-15</td>\n",
       "      <td>1.599009e-12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>50177</td>\n",
       "      <td>11089:ncbi:1-17000</td>\n",
       "      <td>98</td>\n",
       "      <td>3.987377e-10</td>\n",
       "      <td>3.995657e-08</td>\n",
       "      <td>4.0260434e-06</td>\n",
       "      <td>4.5705413e-09</td>\n",
       "      <td>0.00026610115</td>\n",
       "      <td>3.9758719e-10</td>\n",
       "      <td>1.0729461e-07</td>\n",
       "      <td>...</td>\n",
       "      <td>6.970071e-08</td>\n",
       "      <td>1.210523e-17</td>\n",
       "      <td>2.2871915e-10</td>\n",
       "      <td>3.9424535e-06</td>\n",
       "      <td>1.6976076e-09</td>\n",
       "      <td>4.1861083e-09</td>\n",
       "      <td>3.708444e-10</td>\n",
       "      <td>6.0206723e-05</td>\n",
       "      <td>4.0571067e-09</td>\n",
       "      <td>5.209376e-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>50689</td>\n",
       "      <td>11089:ncbi:1-17000</td>\n",
       "      <td>99</td>\n",
       "      <td>3.773077e-12</td>\n",
       "      <td>6.944977e-13</td>\n",
       "      <td>6.4031043e-09</td>\n",
       "      <td>3.3788874e-09</td>\n",
       "      <td>7.6050675e-08</td>\n",
       "      <td>4.657467e-08</td>\n",
       "      <td>3.358606e-07</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0940438e-08</td>\n",
       "      <td>1.2781211e-18</td>\n",
       "      <td>2.6286571e-08</td>\n",
       "      <td>1.7933372e-08</td>\n",
       "      <td>4.844457e-09</td>\n",
       "      <td>1.0032285e-13</td>\n",
       "      <td>2.9824456e-15</td>\n",
       "      <td>3.3517054e-08</td>\n",
       "      <td>4.4160476e-16</td>\n",
       "      <td>2.173458e-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>51201</td>\n",
       "      <td>11089:ncbi:1-17000</td>\n",
       "      <td>100</td>\n",
       "      <td>1.2701454e-07</td>\n",
       "      <td>2.2293822e-15</td>\n",
       "      <td>2.2755298e-12</td>\n",
       "      <td>4.4974868e-14</td>\n",
       "      <td>1.7995131e-11</td>\n",
       "      <td>7.290591e-08</td>\n",
       "      <td>8.492367e-10</td>\n",
       "      <td>...</td>\n",
       "      <td>2.5019067e-10</td>\n",
       "      <td>2.4869705e-15</td>\n",
       "      <td>9.0042565e-12</td>\n",
       "      <td>5.4628693e-08</td>\n",
       "      <td>2.1697435e-09</td>\n",
       "      <td>5.919073e-16</td>\n",
       "      <td>1.073037e-15</td>\n",
       "      <td>1.08923145e-13</td>\n",
       "      <td>1.4590386e-16</td>\n",
       "      <td>1.094292e-09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>101 rows × 190 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id        read_kmer_id  read_50mer_nb       prob_000       prob_001  \\\n",
       "0        1  11089:ncbi:1-17000              0   9.452837e-16  1.8033755e-24   \n",
       "1      513  11089:ncbi:1-17000              1   9.987287e-13   9.640493e-23   \n",
       "2     1025  11089:ncbi:1-17000              2  4.1150392e-14  1.0409154e-21   \n",
       "3     1537  11089:ncbi:1-17000              3  2.4102365e-13  3.9195824e-19   \n",
       "4     2049  11089:ncbi:1-17000              4   5.402446e-15  1.3982798e-24   \n",
       "..     ...                 ...            ...            ...            ...   \n",
       "96   49153  11089:ncbi:1-17000             96   1.266811e-12  1.7694516e-13   \n",
       "97   49665  11089:ncbi:1-17000             97  3.6751678e-11  5.2148962e-11   \n",
       "98   50177  11089:ncbi:1-17000             98   3.987377e-10   3.995657e-08   \n",
       "99   50689  11089:ncbi:1-17000             99   3.773077e-12   6.944977e-13   \n",
       "100  51201  11089:ncbi:1-17000            100  1.2701454e-07  2.2293822e-15   \n",
       "\n",
       "          prob_002       prob_003        prob_004       prob_005  \\\n",
       "0     3.709243e-18  3.2934015e-16  1.11393635e-13   4.909559e-09   \n",
       "1     6.864665e-13   5.297059e-17   5.2123617e-09  1.9526384e-07   \n",
       "2    3.1731182e-18   4.837065e-13   1.8273633e-12  5.6225136e-13   \n",
       "3    4.2152547e-16   5.363886e-15   7.9354416e-13  1.9598592e-10   \n",
       "4    5.1237304e-19  2.6393469e-12    7.070969e-12  4.5762413e-08   \n",
       "..             ...            ...             ...            ...   \n",
       "96   6.9926794e-11   7.442386e-16   2.1827995e-12  2.2899127e-11   \n",
       "97   1.1142021e-07  1.3979555e-13   0.00023110698  1.8854205e-11   \n",
       "98   4.0260434e-06  4.5705413e-09   0.00026610115  3.9758719e-10   \n",
       "99   6.4031043e-09  3.3788874e-09   7.6050675e-08   4.657467e-08   \n",
       "100  2.2755298e-12  4.4974868e-14   1.7995131e-11   7.290591e-08   \n",
       "\n",
       "          prob_006  ...       prob_177       prob_178       prob_179  \\\n",
       "0    3.2027174e-19  ...   4.620669e-10  2.1956457e-23   4.973716e-17   \n",
       "1     6.811955e-14  ...  0.00029164975   9.457506e-20   4.247617e-15   \n",
       "2    1.5127502e-14  ...  2.7924674e-08   9.881187e-25  1.2789581e-19   \n",
       "3    2.0204985e-14  ...  2.3507164e-12  3.4494873e-22  1.4030163e-15   \n",
       "4     3.941572e-13  ...  2.2205168e-10  2.4770586e-20  2.2223746e-16   \n",
       "..             ...  ...            ...            ...            ...   \n",
       "96   6.6601347e-13  ...  2.1055688e-09  2.6956715e-11    4.02884e-10   \n",
       "97    1.128952e-07  ...   3.426913e-08  3.5629816e-17  2.7815642e-11   \n",
       "98   1.0729461e-07  ...   6.970071e-08   1.210523e-17  2.2871915e-10   \n",
       "99    3.358606e-07  ...  3.0940438e-08  1.2781211e-18  2.6286571e-08   \n",
       "100   8.492367e-10  ...  2.5019067e-10  2.4869705e-15  9.0042565e-12   \n",
       "\n",
       "          prob_180       prob_181       prob_182       prob_183  \\\n",
       "0    1.2870734e-19   6.685301e-13  2.7962992e-13  5.0206733e-22   \n",
       "1    2.2502056e-20  1.3939608e-09  4.1289537e-12  1.0989764e-16   \n",
       "2    5.2265872e-23  3.6289507e-16   5.854743e-18   4.597412e-22   \n",
       "3    5.3813346e-20  2.1669545e-18  3.9168753e-20  4.6393943e-18   \n",
       "4    1.3230725e-16   1.473535e-13  1.0368611e-15   7.554584e-18   \n",
       "..             ...            ...            ...            ...   \n",
       "96   3.6730546e-06   7.332998e-13   9.214267e-17    4.41918e-18   \n",
       "97   7.9123413e-07   9.112657e-12  1.8065088e-14  2.2273976e-15   \n",
       "98   3.9424535e-06  1.6976076e-09  4.1861083e-09   3.708444e-10   \n",
       "99   1.7933372e-08   4.844457e-09  1.0032285e-13  2.9824456e-15   \n",
       "100  5.4628693e-08  2.1697435e-09   5.919073e-16   1.073037e-15   \n",
       "\n",
       "           prob_184        prob_185      prob_186  \n",
       "0     2.3932547e-15   1.2987661e-21  3.557021e-18  \n",
       "1     5.3598934e-11  1.09030056e-14  1.850025e-18  \n",
       "2     2.7675505e-16   3.6334886e-19  7.191011e-22  \n",
       "3      2.162604e-17    5.868014e-17  3.671587e-12  \n",
       "4      6.905381e-15    6.823121e-19  5.281248e-14  \n",
       "..              ...             ...           ...  \n",
       "96    1.0378567e-09   3.9226554e-16  1.512387e-09  \n",
       "97    2.6934228e-07   3.2327324e-15  1.599009e-12  \n",
       "98    6.0206723e-05   4.0571067e-09  5.209376e-10  \n",
       "99    3.3517054e-08   4.4160476e-16  2.173458e-09  \n",
       "100  1.08923145e-13   1.4590386e-16  1.094292e-09  \n",
       "\n",
       "[101 rows x 190 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with open_db(p2db=p2db, k=150) as conn:\n",
    "    display(pd.read_sql_query(\"SELECT * FROM predictions WHERE readid = '11089:ncbi:1-17000';\", conn))\n",
    "    display(pd.read_sql_query(\"SELECT * FROM label_probabilities WHERE read_kmer_id = '11089:ncbi:1-17000';\", conn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(419328, '11089:ncbi:25-5673', '25-5673')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open_db(p2db=p2db, k=150) as conn:\n",
    "    last_readid = conn.execute(\"SELECT MAX(id) FROM predictions\").fetchone()[0]\n",
    "    readid = conn.execute(f\"SELECT readid FROM predictions WHERE id = {last_readid};\").fetchone()[0]\n",
    "    regex = re.compile(r'\\d*:ncbi:(?P<read_nb>\\d*-\\d*)')\n",
    "    m = regex.search(readid)\n",
    "    read_nb = '???' if m is None else m.group('read_nb')\n",
    "\n",
    "last_readid, readid, read_nb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last id: 709,120, Nbr predictions: 709,120, last readid: 11089:ncbi:42-4881\n"
     ]
    }
   ],
   "source": [
    "with open_db(p2db=p2db, k=150) as conn:\n",
    "    last_predictions_id = conn.execute(\"SELECT MAX(id) FROM predictions\").fetchone()[0]\n",
    "    nb_predictions = conn.execute(\"SELECT COUNT(*) FROM predictions\").fetchone()[0]\n",
    "    last_readid = conn.execute(f\"SELECT readid FROM predictions WHERE id = {last_predictions_id};\").fetchone()[0]\n",
    "\n",
    "print(f\"Last id: {last_predictions_id:,d}, Nbr predictions: {nb_predictions:,d}, last readid: {last_readid}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Technical note to accelerate `skip_existing_predictions`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The slow step in `skip_existing_predictions` is the query to get the total number of rows in the table. We can accelerate this by maintaining an accurate row count in a separate column when inserting new rows into a SQLite table named \"predictions\" that has a primary key \"id\" and an indexed column \"readid\". \n",
    "\n",
    "This can be done with the following steps:\n",
    "\n",
    "1. **Create a trigger** that fires after an INSERT operation on the \"predictions\" table. The trigger will update the row count in a separate table or column.\n",
    "\n",
    "2. **Create a table** to store the row count, for example:\n",
    "```sql\n",
    "    CREATE TABLE table_stats (\n",
    "    id INTEGER PRIMARY KEY,\n",
    "    row_count INTEGER NOT NULL\n",
    "    );\n",
    "```\n",
    "\n",
    "3. **Create the trigger** to update the row count after an INSERT:\n",
    "```sql\n",
    "    CREATE TRIGGER update_prediction_count\n",
    "    AFTER INSERT ON predictions\n",
    "    BEGIN\n",
    "    UPDATE table_stats \n",
    "    SET row_count = row_count + 1\n",
    "    WHERE id = 1;\n",
    "    END;\n",
    "```\n",
    "\n",
    "This trigger assumes there is only one row in the \"prediction_stats\" table with an id of 1. If you want to store the count per \"readid\", you can modify the trigger to:\n",
    "\n",
    "```sql\n",
    "    CREATE TRIGGER update_prediction_count\n",
    "    AFTER INSERT ON predictions  \n",
    "    BEGIN\n",
    "    INSERT INTO table_stats (readid, row_count)\n",
    "    VALUES (NEW.readid, 1)\n",
    "    ON CONFLICT(readid) DO UPDATE SET row_count = row_count + 1;\n",
    "    END;\n",
    "```\n",
    "\n",
    "This will insert a new row for each unique \"readid\" with an initial count of 1, and update the row_count if the \"readid\" already exists.\n",
    "\n",
    "4. **Insert a row** into the \"prediction_stats\" table with an initial count:\n",
    "```sql\n",
    "    INSERT INTO table_stats (id, row_count) VALUES (1, 0);\n",
    "```\n",
    "\n",
    "Now, whenever a new row is inserted into the \"predictions\" table, the trigger will automatically update the row count in the \"prediction_stats\" table. This maintains an accurate count without needing to perform a full table scan with COUNT(*).\n",
    "\n",
    "Remember to handle deletions as well by creating a BEFORE DELETE trigger that decrements the row count accordingly.\n",
    "\n",
    "Citations:\n",
    "- 1. https://www.sqlitetutorial.net/sqlite-insert/\n",
    "- 2. https://www.sqlitetutorial.net/sqlite-count-function/\n",
    "- 3. https://stackoverflow.com/questions/55007800/dynamic-way-to-insert-data-into-sqlite-table-when-column-counts-change\n",
    "- 4. https://docs.python.org/es/3/library/sqlite3.html\n",
    "- 5. https://www.sql-easy.com/learn/sqlite-count/\n",
    "- 6. https://stackoverflow.com/questions/4474873/what-is-the-most-efficient-way-to-count-rows-in-a-table-in-sqlite\n",
    "- 7. https://sqlite.org/forum/info/57c04743e1b6aa10\n",
    "- 8. https://sqlite.org/forum/forumpost/f832398c19"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End of Section"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('metagentools')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "d79c725d3d254ae089d5edf9eb2dce3237f80d64dd85a8bedc17bd8054b8b312"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
