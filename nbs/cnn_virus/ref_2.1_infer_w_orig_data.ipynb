{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference using original data by paper's author"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we use the validation data from the CNN Virus paper to do inference using the pretrained model.\n",
    "\n",
    "This notebook works when run locally and also should run on Colab, as long as the file system is in line with the unified file ystem (see documentation)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Imports and setup environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install and import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "`ecutilities` already installed\n",
      "`metagentools` already installed\n"
     ]
    }
   ],
   "source": [
    "# Install required custom packages if not installed yet.\n",
    "import importlib.util\n",
    "if not importlib.util.find_spec('ecutilities'):\n",
    "    print('installing package: `ecutilities`')\n",
    "    ! pip install -qqU ecutilities\n",
    "else:\n",
    "    print('`ecutilities` already installed')\n",
    "if not importlib.util.find_spec('metagentools'):\n",
    "    print('installing package: `metagentools')\n",
    "    ! pip install -qqU metagentools\n",
    "else:\n",
    "    print('`metagentools` already installed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set autoreload mode\n",
      "Tensorflow version: 2.8.2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import all required packages\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "from ecutilities.core import files_in_tree\n",
    "from ecutilities.ipython import nb_setup\n",
    "from IPython.display import display, Markdown, HTML\n",
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "from tqdm.notebook import tqdm, trange\n",
    "\n",
    "# Setup the notebook for development\n",
    "nb_setup()\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' # or any {'0', '1', '2'}\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.client import device_lib\n",
    "from tensorflow.keras.models import load_model\n",
    "print(f\"Tensorflow version: {tf.__version__}\\n\")\n",
    "\n",
    "from metagentools.cnn_virus.data import strings_to_tensors, create_infer_ds_from_fastq\n",
    "from metagentools.cnn_virus.data import FastaFileReader, FastqFileReader, AlnFileReader\n",
    "from metagentools.core import TextFileBaseReader, ProjectFileSystem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List all computing devices available on the machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Devices:\n",
      "  - CPU  /device:CPU:0                          \n",
      "  - GPU  /device:GPU:0  NVIDIA GeForce GTX 1050 \n"
     ]
    }
   ],
   "source": [
    "devices = device_lib.list_local_devices()\n",
    "print('\\nDevices:')\n",
    "for d in devices:\n",
    "    t = d.device_type\n",
    "    name = d.physical_device_desc\n",
    "    l = [item.split(':', 1) for item in name.split(', ')]\n",
    "    name_attr = dict([x for x in l if len(x)==2])\n",
    "    dev = name_attr.get('name', ' ')\n",
    "    print(f\"  - {t}  {d.name} {dev:25s}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Setup paths to files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Key folders and system information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running linux on local computer\n",
      "Device's home directory: /home/vtec\n",
      "Project file structure:\n",
      " - Root ........ /home/vtec/projects/bio/metagentools \n",
      " - Data Dir .... /home/vtec/projects/bio/metagentools/data \n",
      " - Notebooks ... /home/vtec/projects/bio/metagentools/nbs\n"
     ]
    }
   ],
   "source": [
    "pfs = ProjectFileSystem()\n",
    "pfs.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<hr>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "ReadMe file for directory `data`:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<hr>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### Data structure for `metagentools`\n",
       "This directory includes all the data required for the project `metagentools`.\n",
       "\n",
       "```text\n",
       "data\n",
       " |--- CNN_Virus_data \n",
       " |--- ncbi           \n",
       " |--- ncov_data      \n",
       " |--- saved         \n",
       " |--- ....           \n",
       "     \n",
       "```\n",
       "#### Sub-directories\n",
       "- `CNN_Virus_data`: includes all the data related to the original CNN Virus paper, i.e. training data and validation data in a format that can be used by the CNN Virus code.\n",
       "- `ncbi`: includes data related to the use of CoV sequences from NCBI: reference sequences, simulated reads, inference datasets, inference results.\n",
       "- `ncov_data`: includes data related to the use of non Cov sequences from various sources: reference sequences, simulated reads, inference datasets, inference results.\n",
       "- `saved`: includes model saved parameters and preprocessing datasets.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<hr>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pfs.readme()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `p2model`: path to file with saved original pretrained model\n",
    "- `p2virus_labels` path to file with virus names and labels mapping for original model\n",
    "- `p2simreads`: path to folder where reads files are located (FASTQ and ALN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "p2model = pfs.data / 'saved/cnn_virus_original/pretrained_model.h5'\n",
    "assert p2model.is_file(), f\"No file found at {p2model.absolute()}\"\n",
    "\n",
    "p2virus_labels = pfs.data / 'CNN_Virus_data/virus_name_mapping'\n",
    "assert p2virus_labels.is_file(), f\"No file found at {p2virus_labels.absolute()}\"\n",
    "\n",
    "p2original = pfs.data / 'CNN_Virus_data'\n",
    "assert p2original.is_dir(), f\"No directory found at {p2original.absolute()}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<hr>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "ReadMe file for directory `data/CNN_Virus_data`:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<hr>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### CNN Virus data\n",
       "\n",
       "This directory includes data used to train and validate the initial CNN Virus model, as well as a few smaller datasets for experimenting. \n",
       "\n",
       "\n",
       "#### File list and description:\n",
       "##### 50-mer \n",
       "50-mer reads and their labels, in *text format* with one line per sample. Each line consists of three components, separated by tabs: the 50-mer read or sequence, the virus species label and the position label:\n",
       "```text\n",
       "'TTACNAGCTCCAGTCTAAGATTGTAACTGGCCTTTTTAAAGATTGCTCTA    94    5\\n'\n",
       "``` \n",
       "Files:\n",
       "- `50mer_training`: dataset with 50,903,296 reads for training\n",
       "- `50mer_validating`: dataset with 1,000,000 reads for validation\n",
       "- `50mer_ds_100_reads`: small subset of 100 reads from the validating dataset for experiments\n",
       "\n",
       "##### 150-mer\n",
       "150-mer reads and their labels in *text format* in a similar format as above:\n",
       "```text\n",
       "'TTCTTTCACCACCACAACCAGTCGGCCGTGGAGAGGCGTCGCCGCGTCTCGTTCGTCGAGGCCGATCGACTGCCGCATGAGAGCGGGTGGTATTCTTCCGAAGACGACGGAGACCGGGACGGTGATGAGGAAACTGGAGAGAGCCACAAC    6    0\\n'\n",
       "```\n",
       "Files:\n",
       "- `ICTV_150mer_benchmarking`: dataset with 10,0000 read\n",
       "- `150mer_ds_100_reads`: small subset of 100 reads from `ICTV_150mer_benchmarking`\n",
       "\n",
       "##### Longer reads\n",
       "Reads of various length with no labels, in simple *fasta format*. Each read sequence is preceded by a definition line: `> Sequence n`, where `n` is the sequence number.\n",
       "\n",
       "Files:\n",
       "- `training_sequences_300bp.fasta`: dataset with 9,000 300-mer reads\n",
       "- `training_sequences_500bp.fasta`: dataset with 9,000 500-mer reads\n",
       "- `validation_sequences.fasta`: dataset with 564 reads of mixed lengths ranging from 163-mer to 497-mer\n",
       "\n",
       "##### Other files:\n",
       "- `virus_name_mapping`: mapping between virus species and their numerical label\n",
       "- `weight_of_classes`:  weights for each virus species class in the training dataset\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<hr>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pfs.readme(dir_path=p2original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data\n",
      "  |--CNN_Virus_data\n",
      "  |    |--50mer_validating (0)\n",
      "  |    |--50mer_ds_100_reads (1)\n",
      "  |    |--validation_sequences.fasta (2)\n",
      "  |    |--ICTV_150mer_benchmarking (3)\n",
      "  |    |--readme.md (4)\n",
      "  |    |--50mer_training (5)\n",
      "  |    |--training_sequences_500bp.fasta (6)\n",
      "  |    |--weight_of_classes (7)\n",
      "  |    |--150mer_ds_100_reads (8)\n",
      "  |    |--virus_name_mapping (9)\n",
      "  |    |--training_sequences_300bp.fasta (10)\n"
     ]
    }
   ],
   "source": [
    "files_in_tree(path=p2original);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this experiment, we will use the dataset:\n",
    "- 50mer_validating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/home/vtec/projects/bio/metagentools/data/CNN_Virus_data/50mer_validating')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p2ds = p2original / '50mer_validating'\n",
    "assert p2ds.is_file()\n",
    "p2ds.absolute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Create inference dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model expect a dataset file in the following format:\n",
    "\n",
    "```text\n",
    "    AAAAAGATTTTGAGAGAGGTCGACCTGTCCTCCTAAAACGTTTACAAAAG\n",
    "    CATGTAACGCAGCTTAGTCCGATCGTGGCTATAATCCGTCTTTCGATTTG\n",
    "    AACAACATCTTGTTGATGATAACCGTCAAAGTGTTTTGGGTCTGGAGGGA\n",
    "    AGTACCTGGAGAGCGTTAAGAAACACAAACGGCTGGATGTAGTGCCGCGC\n",
    "    CCACGTCGATGAAGCTCCGACGAGAGTCGGCGCTGAGCCCGCGCACCTCC\n",
    "```\n",
    "\n",
    "`50mer_validating` is already in the correct format\n",
    "\n",
    "The mapping between code and virus specie name are in the file `virus_labels.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10-line chunk 1\n",
      "AAAAAGATTTTGAGAGAGGTCGACCTGTCCTCCTAAAACGTTTACAAAAG\t71\t0\n",
      "CATGTAACGCAGCTTAGTCCGATCGTGGCTATAATCCGTCTTTCGATTTG\t1\t7\n",
      "AACAACATCTTGTTGATGATAACCGTCAAAGTGTTTTGGGTCTGGAGGGA\t158\t6\n",
      "AGTACCTGGAGAGCGTTAAGAAACACAAACGGCTGGATGTAGTGCCGCGC\t6\t7\n",
      "CCACGTCGATGAAGCTCCGACGAGAGTCGGCGCTGAGCCCGCGCACCTCC\t71\t6\n",
      "AGCTCGTGGATCTCCCCTCCTTCTGCAGTTTCAACATCAGAAGCCCTGAA\t87\t1\n",
      "GACTCTGTGTTTATGTATCAGCATACAGAGCTTATGCAGAAGAACGCGTC\t10\t0\n",
      "CGTCATGAGGAAGTTGCTAATAATATGTGGATGCATGCATTCCTCTGGGT\t178\t7\n",
      "TTCACCTTGAGCAAGGGCAGGTTGAACACGCGGCTGACATCGCCGTCGTA\t71\t3\n",
      "CAAAACTTTCACCGGGGTTCCAATCCGCGGTGGTAATGACGTTNTGCTGT\t22\t6\n",
      "\n"
     ]
    }
   ],
   "source": [
    "reader = TextFileBaseReader(p2ds, nlines=10)\n",
    "reader.print_first_chunks(nchunks=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the data loader for the model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define batch size and create a first dataset accessing data from the dataset text file. Batch size can be adjusted depending on the memory available on the GPU. For reference, `bs = 4096` was used with a 4GB GPU. \n",
    "\n",
    "Then transform the text dataset into a tensor dataset by applying the `string_to_tensor` preprocessing function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 4096\n",
    "\n",
    "text_ds = tf.data.TextLineDataset(p2ds).batch(bs)\n",
    "ds = text_ds.map(strings_to_tensors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The bases in the read sequences are encoded as a 5-dim one-hot-encoded vector, as the model expects.\n",
    "\n",
    "In this example, each 50bp read in converted into a tensor of shape [50,5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4096, 50, 5)\n",
      "tf.Tensor(\n",
      "[[1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0.]], shape=(10, 5), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "for batch, (y1b, y2b) in ds.take(1):\n",
    "    # show the shape of one batch\n",
    "    print(batch.shape)\n",
    "    # show the forst 10 bases, after one-hot-endoding\n",
    "    print(batch[0, :10, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load and review the pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model(p2model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 50, 5)]      0           []                               \n",
      "                                                                                                  \n",
      " conv1d_1 (Conv1D)              (None, 50, 512)      13312       ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " batch_normalization_1 (BatchNo  (None, 50, 512)     2048        ['conv1d_1[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " max_pooling1d_1 (MaxPooling1D)  (None, 25, 512)     0           ['batch_normalization_1[0][0]']  \n",
      "                                                                                                  \n",
      " conv1d_2 (Conv1D)              (None, 25, 512)      1311232     ['max_pooling1d_1[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_2 (BatchNo  (None, 25, 512)     2048        ['conv1d_2[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " max_pooling1d_2 (MaxPooling1D)  (None, 13, 512)     0           ['batch_normalization_2[0][0]']  \n",
      "                                                                                                  \n",
      " conv1d_3 (Conv1D)              (None, 13, 1024)     3671040     ['max_pooling1d_2[0][0]']        \n",
      "                                                                                                  \n",
      " conv1d_4 (Conv1D)              (None, 13, 1024)     7341056     ['conv1d_3[0][0]']               \n",
      "                                                                                                  \n",
      " batch_normalization_3 (BatchNo  (None, 13, 1024)    4096        ['conv1d_4[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " max_pooling1d_3 (MaxPooling1D)  (None, 7, 1024)     0           ['batch_normalization_3[0][0]']  \n",
      "                                                                                                  \n",
      " flatten_1 (Flatten)            (None, 7168)         0           ['max_pooling1d_3[0][0]']        \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 1024)         7341056     ['flatten_1[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_4 (BatchNo  (None, 1024)        4096        ['dense_1[0][0]']                \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)            (None, 1024)         0           ['batch_normalization_4[0][0]']  \n",
      "                                                                                                  \n",
      " output1 (Dense)                (None, 187)          191675      ['dropout_1[0][0]']              \n",
      "                                                                                                  \n",
      " concatenate_1 (Concatenate)    (None, 1211)         0           ['dropout_1[0][0]',              \n",
      "                                                                  'output1[0][0]']                \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 1024)         1241088     ['concatenate_1[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_5 (BatchNo  (None, 1024)        4096        ['dense_2[0][0]']                \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " output2 (Dense)                (None, 10)           10250       ['batch_normalization_5[0][0]']  \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 21,137,093\n",
      "Trainable params: 21,128,901\n",
      "Non-trainable params: 8,192\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Present the inference dataset to the model and collect prediction.\n",
    "\n",
    "The model returns two sets of probabilities:\n",
    "- `prob_preds_species`: a vector of 187 values representing the probability that each of the 187 species are the correct ones, for each input read\n",
    "- `prob_preds_pos`: a vector of 10 values representing the probability that the read is from the corresponding segment of the original sequence (1 to 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2 µs, sys: 1e+03 ns, total: 3 µs\n",
      "Wall time: 5.96 µs\n",
      "245/245 [==============================] - 229s 912ms/step\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "prob_preds_species, prob_preds_pos = model.predict(ds, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1000000, 187), (1000000, 10))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob_preds_species.shape, prob_preds_pos.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find the prediction, we pick the argmax probability, which gives us the index/code for the predicted virus species."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 71,   1, 158,   6,  71,  87,  10, 178,  71,  22])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_preds = np.argmax(prob_preds_species, axis=1)\n",
    "class_preds.shape\n",
    "class_preds[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Simple evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Original model was trained with 187 different virus species."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New Section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## end of section"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('metagentools')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "d79c725d3d254ae089d5edf9eb2dce3237f80d64dd85a8bedc17bd8054b8b312"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
