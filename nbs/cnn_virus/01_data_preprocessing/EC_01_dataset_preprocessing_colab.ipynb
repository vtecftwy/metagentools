{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/vtecftwy/metagenomics/blob/refactor_cnn_virus/nbs/2_02_EC_preprocess_data_dataset_colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use `tf.data.Dataset` to preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow version: 2.8.2\n",
      "\n",
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "\n",
      "Devices:\n",
      "  - CPU  /device:CPU:0                          \n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import psutil\n",
    "import os\n",
    "import shutil\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "from datetime import datetime, timedelta\n",
    "from pathlib import Path\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.python.client import device_lib\n",
    "print(f\"Tensorflow version: {tf.__version__}\\n\")\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "devices = device_lib.list_local_devices()\n",
    "print('\\nDevices:')\n",
    "for d in devices:\n",
    "    t = d.device_type\n",
    "    name = d.physical_device_desc\n",
    "    l = [item.split(':', 1) for item in name.split(', ')]\n",
    "    name_attr = dict([x for x in l if len(x)==2])\n",
    "    dev = name_attr.get('name', ' ')\n",
    "    print(f\"  - {t}  {d.name} {dev:25s}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on colab\n",
      "Installing custom project code\n",
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting git+https://github.com/vtecftwy/metagenomics.git@refactor_cnn_virus\n",
      "  Cloning https://github.com/vtecftwy/metagenomics.git (to revision refactor_cnn_virus) to /tmp/pip-req-build-rlocd821\n",
      "  Running command git clone -q https://github.com/vtecftwy/metagenomics.git /tmp/pip-req-build-rlocd821\n",
      "  Running command git checkout -b refactor_cnn_virus --track origin/refactor_cnn_virus\n",
      "  Switched to a new branch 'refactor_cnn_virus'\n",
      "  Branch 'refactor_cnn_virus' set up to track remote branch 'refactor_cnn_virus' from 'origin'.\n",
      "Building wheels for collected packages: src\n",
      "  Building wheel for src (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for src: filename=src-1.0.2-py3-none-any.whl size=14773 sha256=60f848650ced82a9fd766df29a177070a589ec97896d8b3b07d53b57e7e4ffce\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-rvivvrr8/wheels/10/e4/2b/3924f57326dbc088c93138fb676f8dac09dde9d0cb9f270a15\n",
      "Successfully built src\n",
      "Installing collected packages: src\n",
      "Successfully installed src-1.0.2\n",
      "Mounted at /content/gdrive\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    from google.colab import drive\n",
    "    ON_COLAB = True\n",
    "    print('Running on colab')\n",
    "    print('Installing custom project code')\n",
    "    try:\n",
    "        from src.architecture import build_model\n",
    "    except:\n",
    "        !pip install -U git+https://github.com/vtecftwy/metagenomics.git@refactor_cnn_virus\n",
    "    \n",
    "    drive.mount('/content/gdrive')\n",
    "    p2drive = Path('/content/gdrive/MyDrive/Metagenonics')\n",
    "    assert p2drive.is_dir()\n",
    "    p2data =  p2drive / 'CNN_Virus_data'\n",
    "    assert p2data.is_dir()\n",
    "\n",
    "except:\n",
    "    ON_COLAB = False\n",
    "    print('Running locally')\n",
    "    print('Make sure you have installed the custom project code in your environment')\n",
    "    pdata = Path('data/cnn_virus')\n",
    "    assert p2data.is_dir()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import custom code and setup paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.architecture import build_model\n",
    "from src.preprocessing import get_learning_weights, get_params_50mer, get_kmer_from_50mer\n",
    "from src.preprocessing import DataGenerator_from_50mer\n",
    "from src.utils import TrainingExperiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#path for the training file\n",
    "filepath_train= p2data /\"50mer_training\"\n",
    "assert filepath_train.is_file()\n",
    "#path for the validating file\n",
    "filepath_val= p2data / \"50mer_validating\"\n",
    "assert filepath_val.is_file()\n",
    "#path for the learning weights file\n",
    "filepath_weights=p2data / \"weight_of_classes\"\n",
    "assert filepath_weights.is_file()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Review training data\n",
    "\n",
    "- check size, structure, time to load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TCAAAATAATCAGAAATGTTGAACCTAGGGTTGGACACATAATGACCAGC\t76\t0\n",
      "ATTGTTTAACAATTTGTGCTCGTCCCGGTCACCCGCATCCAATCTTGATG\t4\t9\n",
      "AATCTTGTCCTATCCTACCCGCAGGGGAATTGATGATAGANGTGCTTTTA\t181\t0\n",
      "GGAGCGGAGCCAACCCCTATGCTCACTTGCAACCCAAGGGGCGTTCCAGT\t74\t3\n",
      "TGGATCCTGCGCGGGACGTCCTTTGTCTACGTCCCGTCGGCGCATCCCGC\t60\t3\n",
      "GAGAGACTTACTAAAAAGCTGGCACTTACCATCAGTGTTTCACCTACATG\t44\t0\n",
      "ACACACGACACTAGAGATAATGTGTCAGTGGATTATAAACAAACCAAGTT\t43\t7\n",
      "TTGTAGCATAAGAACTGGTCTTCGCTGAAATTCTTGTCTTGATCTCATCT\t35\t2\n",
      "TGGCCCTGCGGTCTGGGGCCCAGAAGCATATGTCAAGTCCTTTGAGAAGT\t73\t4\n",
      "TAGATTTAGTGGTTAGGTAGTAAGGCTACAATGTAAACACGTAGTGGCAA\t11\t6\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open(filepath_train, 'r') as fp:\n",
    "    lines = []\n",
    "    for i in range(10):\n",
    "        lines.append(fp.readline())\n",
    "print(''.join(lines))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare reading time to pass the full file if on gdrive and on server disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50,903,296\n",
      "CPU times: user 14.1 s, sys: 1.6 s, total: 15.7 s\n",
      "Wall time: 29.5 s\n"
     ]
    }
   ],
   "source": [
    "# From gdrive\n",
    "%%time\n",
    "nlines = 0\n",
    "with open(filepath_train, 'r') as fp:\n",
    "    while True:\n",
    "        line = fp.readline()\n",
    "        if line == '':\n",
    "            break\n",
    "        else:\n",
    "            nlines += 1\n",
    "print(f\"{nlines:,d}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From gdrive:\n",
    "```\n",
    "    50,903,296\n",
    "    CPU times: user 17.4 s, sys: 1.49 s, total: 18.9 s\n",
    "    Wall time: 19.5 s\n",
    "\n",
    "    50,903,296\n",
    "    CPU times: user 14.1 s, sys: 1.6 s, total: 15.7 s\n",
    "    Wall time: 29.5 s\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('train')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p2train_txt = Path('train')\n",
    "shutil.copy(filepath_train, p2train_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50,903,296\n",
      "CPU times: user 11.5 s, sys: 719 ms, total: 12.3 s\n",
      "Wall time: 12.3 s\n"
     ]
    }
   ],
   "source": [
    "# Locally on server\n",
    "%%time\n",
    "nlines = 0\n",
    "with open(p2train_txt, 'r') as fp:\n",
    "    while True:\n",
    "        line = fp.readline()\n",
    "        if line == '':\n",
    "            break\n",
    "        else:\n",
    "            nlines += 1\n",
    "print(f\"{nlines:,d}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From \"local\" file:\n",
    "```\n",
    "    50,903,296\n",
    "    CPU times: user 12.3 s, sys: 752 ms, total: 13.1 s\n",
    "    Wall time: 12.9 s\n",
    "\n",
    "    50,903,296\n",
    "    CPU times: user 11.5 s, sys: 719 ms, total: 12.3 s\n",
    "    Wall time: 12.3 s\n",
    "```\n",
    "Seems that copying the file locally before processing will accelerate a little the process. But not very clear."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review Data in `50mer_training` and `50mer_validating`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data is provided as long tab separated text, organized as follows:\n",
    "- one line per sample\n",
    "- three columns per sample: *sequence*, *virus label* , *position label*\n",
    "```\n",
    "TCAAAATAATCAGAAATGTTGAACCTAGGGTTGGACACATAATGACCAGC\t76\t0\n",
    "ATTGTTTAACAATTTGTGCTCGTCCCGGTCACCCGCATCCAATCTTGATG\t4\t9\n",
    "AATCTTGTCCTATCCTACCCGCAGGGGAATTGATGATAGANGTGCTTTTA\t181\t0\n",
    "GGAGCGGAGCCAACCCCTATGCTCACTTGCAACCCAAGGGGCGTTCCAGT\t74\t3\n",
    "TGGATCCTGCGCGGGACGTCCTTTGTCTACGTCCCGTCGGCGCATCCCGC\t60\t3\n",
    "GAGAGACTTACTAAAAAGCTGGCACTTACCATCAGTGTTTCACCTACATG\t44\t0\n",
    "ACACACGACACTAGAGATAATGTGTCAGTGGATTATAAACAAACCAAGTT\t43\t7\n",
    "TTGTAGCATAAGAACTGGTCTTCGCTGAAATTCTTGTCTTGATCTCATCT\t35\t2\n",
    "TGGCCCTGCGGTCTGGGGCCCAGAAGCATATGTCAAGTCCTTTGAGAAGT\t73\t4\n",
    "TAGATTTAGTGGTTAGGTAGTAAGGCTACAATGTAAACACGTAGTGGCAA\t11\t6\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use \n",
    "- `tf.data.TextLineDataset` to define the initial dataset pointing to the file and returning strings\n",
    "- then apply a transform to convet the string into 3 tensors datasets: `train_ds`, `label_ds`, `pos_ds`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load data from text file using a dataset: `text_ds`\n",
    "```python\n",
    "    tf.data.TextLineDataset(\n",
    "        filenames,\n",
    "        compression_type=None,\n",
    "        buffer_size=None,\n",
    "        num_parallel_reads=None,\n",
    "        name=None\n",
    "    )\n",
    "```\n",
    "\n",
    "Create `text_ds` and print a few small batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TextLineDatasetV2 element_spec=TensorSpec(shape=(), dtype=tf.string, name=None)>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_ds = tf.data.TextLineDataset(\n",
    "    filepath_val,\n",
    "    compression_type='',\n",
    "    name='text_ds'\n",
    ")\n",
    "\n",
    "text_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorSpec(shape=(), dtype=tf.string, name=None)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_ds.element_spec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "TensorSpec(shape=(), dtype=tf.string, name=None)\n",
    "```\n",
    "- `shape=()` means each element is a scalar (a single value)\n",
    "- `dtype=tf.string` means that each element is of type string.\n",
    "\n",
    "Each element is a single string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use `.take(n)` method to retrieve n elements one by one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TakeDataset element_spec=TensorSpec(shape=(), dtype=tf.string, name=None)>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_ds.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=string, numpy=b'CCATCGGCGTCCCGGAATCGTATACCGGGCACACGAAGCGTTATAACAAT\\t6\\t8'>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=string, numpy=b'GTGGCTATGGTGAGGAGTTTGGAGGAAATAATATACATCATATATTCTGA\\t6\\t4'>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=string, numpy=b'CCCTCTTCTGCAGACTGCTTACGGTTTCGTCCGTGTTGCAGTCGATTATC\\t117\\t0'>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=string, numpy=b'GGAACGCGAACACGCCCGGAAGATTCTTCATCGTAATAAATGGACAGGTA\\t2\\t3'>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=string, numpy=b'CAAACTGATATTCTTAGTGAAGAAAGACCACCTAATCATCATACCTACAT\\t20\\t4'>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for element in text_ds.take(5):\n",
    "    display(element)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we want is to work with batches of elements. We can do that using the `.batch(n)` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4,), dtype=string, numpy=\n",
       "array([b'CCATCGGCGTCCCGGAATCGTATACCGGGCACACGAAGCGTTATAACAAT\\t6\\t8',\n",
       "       b'GTGGCTATGGTGAGGAGTTTGGAGGAAATAATATACATCATATATTCTGA\\t6\\t4',\n",
       "       b'CCCTCTTCTGCAGACTGCTTACGGTTTCGTCCGTGTTGCAGTCGATTATC\\t117\\t0',\n",
       "       b'GGAACGCGAACACGCCCGGAAGATTCTTCATCGTAATAAATGGACAGGTA\\t2\\t3'],\n",
       "      dtype=object)>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4,), dtype=string, numpy=\n",
       "array([b'CAAACTGATATTCTTAGTGAAGAAAGACCACCTAATCATCATACCTACAT\\t20\\t4',\n",
       "       b'TTTACTATTTATATGNCTACTTGGATATCTTCAGTGTTGTGTGCGTCAGT\\t32\\t9',\n",
       "       b'CTCTTATGTTTGAGCAACATCAAAATTCACTTATGAAGGAGAGGACACAT\\t26\\t1',\n",
       "       b'GTGGTCTAGATATTTAAATAAATTTGACTATTATTTTCGGAGGTAAAATA\\t52\\t1'],\n",
       "      dtype=object)>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4,), dtype=string, numpy=\n",
       "array([b'CCGCCGCCCGCCGCCCGGACACGTGCGCCCGGAGCGCCGCCGCCTCCTCG\\t21\\t6',\n",
       "       b'GATGACTTGATAATAAAGTTGATACCTTCCACGTTCGATTTATGATCGGT\\t18\\t4',\n",
       "       b'GCAGCCATGTCTATGGGTGCACAAGAACCCAGAAACATATTCCTGTTGTG\\t13\\t0',\n",
       "       b'TTGTGAGCGAGGTTACCGGTGTCCAAGTCGTAATCCGAAAATCATATATT\\t12\\t7'],\n",
       "      dtype=object)>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for batch in text_ds.batch(4).take(3):\n",
    "    display(batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `text_ds.batch(n)` returns batches as a tensor  of shape (4,) and `dtype=tf.string`, that is, a tensor of 4 rows, each row being one single string.\n",
    "- each string corresponds to one line in the dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create a function to transform the data into the desired formats\n",
    "The function must take in a tensor of strings and returns three tensors. \n",
    "\n",
    "For a batch of `n` samples, the three tensors are:\n",
    "- `x_seqs`: the input tensor, of shape (n, 50, 5) where each base letter in \"base-hot-encoded\" (BHE), i.e. \"A\":[1,0,0,0,0], \"C\":[0,1,0,0,0], \"G\":[0,0,1,0,0], \"T\":[0,0,0,1,0], \"N\":[0,0,0,0,1].\n",
    "- `y_labels`: the virus target tensor, of shape (n, 187) with \"one-hot-encoded\" (OHE) virus labels\n",
    "- `y_pos`: the position target tensor, of shape (n, 10) with \"one-hot-encoded\" (OHE) position labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Steps to design this function\n",
    "1. First, experiment with the transform, step by step. To do this, we pick a single batch from the dataset, then build the three tensors step by step.\n",
    "2. When it works, create a function and test it on one batch of data\n",
    "3. Finally, test it again by applying directly to the text_ds dataset\n",
    "\n",
    "> Techical Note: \n",
    ">\n",
    "> When applied to the dataset, the function may not make any assumption on the size of the batch, and cannot retrieve it either."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First attempt. It is rather slow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create text line dataset\n",
      ">> 0.021305\n",
      "\n",
      "Load and split lines in three sections:\n",
      "(1024, 3)\n",
      "tf.Tensor(\n",
      "[[b'CCATCGGCGTCCCGGAATCGTATACCGGGCACACGAAGCGTTATAACAAT' b'6' b'8']\n",
      " [b'GTGGCTATGGTGAGGAGTTTGGAGGAAATAATATACATCATATATTCTGA' b'6' b'4']\n",
      " [b'CCCTCTTCTGCAGACTGCTTACGGTTTCGTCCGTGTTGCAGTCGATTATC' b'117' b'0']], shape=(3, 3), dtype=string)\n",
      ">> 0.013077000000000002\n",
      "\n",
      "Split string tensor into three tensors:\n",
      "(1024, 50) (1024,) (1024,)\n",
      ">> 0.042208999999999997\n",
      "\n",
      "One-Hot-Encode labels\n",
      "(1024, 187)\n",
      "tf.Tensor(\n",
      "[[0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]], shape=(3, 15), dtype=float32)\n",
      ">> 0.021422999999999998\n",
      "\n",
      "One-Hot-Encode positions\n",
      "(1024, 10)\n",
      "tf.Tensor(\n",
      "[[0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]], shape=(3, 10), dtype=float32)\n",
      ">> 0.04852000000000001\n",
      "\n",
      "Base-Hote-Encode the sequences\n",
      "(51200, 5)\n",
      "(1024, 50, 5)\n",
      ">> 108.536776\n"
     ]
    }
   ],
   "source": [
    "RUN_SLOW_CODE = False\n",
    "# can be run for output comparison with new method\n",
    "\n",
    "if RUN_SLOW_CODE:\n",
    "    now = datetime.now()\n",
    "    batch_size = 1024\n",
    "\n",
    "    # Create a text dataset and test each step on it\n",
    "    print('Create text line dataset')\n",
    "    ds = text_ds.batch(batch_size)\n",
    "    it = iter(ds)\n",
    "    b = next(it)\n",
    "    duration = (datetime.now()-now).total_seconds()\n",
    "    print('>>', duration)\n",
    "\n",
    "    # EXPERIMENT WITH THE TRANSFORM\n",
    "    # 1. Split the string in three: \n",
    "    # Notes:\n",
    "    # tf.split returns a ragged tensor. It must be converted into a normal tensor\n",
    "    # Tensor shape is (batch size, 3), one sequences, one for labels and one for position. Dtype is all tf.strings\n",
    "    t = tf.strings.split(b, '\\t').to_tensor(default_value = '', shape=[None, 3])\n",
    "    print('\\nLoad and split lines in three sections:')\n",
    "    print(t.shape)\n",
    "    print(t[:3, :])\n",
    "    duration = (datetime.now()-now).total_seconds() - duration\n",
    "    print('>>', duration)\n",
    "\n",
    "    # 2. Split into sequences, labels and positions\n",
    "    print('\\nSplit string tensor into three tensors:')\n",
    "    # Split string sequence in a sequence of single base strings \n",
    "    seqs = tf.strings.bytes_split(t[:, 0]).to_tensor(shape=(None, 50))\n",
    "    # Labels and Posisionts are converted from tf.string to tf.int32\n",
    "    labels = tf.strings.to_number(t[:, 1], out_type=tf.int32)\n",
    "    pos = tf.strings.to_number(t[:, 2], out_type=tf.int32)\n",
    "    print(seqs.shape, labels.shape, pos.shape)\n",
    "    duration = (datetime.now()-now).total_seconds() - duration\n",
    "    print('>>', duration)\n",
    "\n",
    "    # 3. One-Hot-Encode labels, using only tf functions for performance\n",
    "    print('\\nOne-Hot-Encode labels')\n",
    "    n_labels = 187\n",
    "    y_labels = tf.gather(tf.eye(n_labels), labels)\n",
    "    print(y_labels.shape)\n",
    "    print(y_labels[:3, :15])\n",
    "    duration = (datetime.now()-now).total_seconds() - duration\n",
    "    print('>>', duration)\n",
    "\n",
    "    # 4. One-Hot-Encode positions, using only tf functions for performance\n",
    "    print('\\nOne-Hot-Encode positions')\n",
    "    n_pos = 10\n",
    "    y_pos= tf.gather(tf.eye(n_pos), pos)\n",
    "    print(y_pos.shape)\n",
    "    print(y_pos[:3])\n",
    "    duration = (datetime.now()-now).total_seconds() - duration\n",
    "    print('>>', duration)\n",
    "\n",
    "    # 5. Base-Hot-Encode sequences\n",
    "    # Each base letter (A, C, G, T, N) is replaced by a OHE vector\n",
    "    # Notes:\n",
    "    # a. the batch of sequence seqs has a shape (batch_size, 50) after splitting each byte. \n",
    "    #    need to flatten it to apply the transform on each base, then reshape to original shape\n",
    "    # b. We need to map each letter to one vector/tensor. \n",
    "    #    Would normally use a dict for that, but it does not work in this case because the key comes\n",
    "    #    from the incoming tensor, and tensors are not hashable. Instead, we will use tf.case()\n",
    "    print('\\nBase-Hote-Encode the sequences')\n",
    "    flattened_seqs = tf.reshape(seqs, shape=[-1])\n",
    "\n",
    "    def base_hot_encoder(a):\n",
    "\n",
    "        # Define the encoding functions returning the encoding tensor for each base\n",
    "        def encode_A(): return tf.constant([1,0,0,0,0])\n",
    "        def encode_C(): return tf.constant([0,1,0,0,0])\n",
    "        def encode_G(): return tf.constant([0,0,1,0,0])\n",
    "        def encode_T(): return tf.constant([0,0,0,1,0])\n",
    "        def encode_N(): return tf.constant([0,0,0,0,1])\n",
    "\n",
    "        # Define the mapping, as a list of tuples: (condition, encoding function)\n",
    "        # values of the strings are tested against b'A', ... as bytes\n",
    "        case_mapping = [\n",
    "            (tf.math.equal(a, b'A'), encode_A),\n",
    "            (tf.math.equal(a, b'C'), encode_C),\n",
    "            (tf.math.equal(a, b'G'), encode_G),\n",
    "            (tf.math.equal(a, b'T'), encode_T),\n",
    "            (tf.math.equal(a, b'N'), encode_N)\n",
    "        ]\n",
    "\n",
    "        return tf.case(pred_fn_pairs = case_mapping)\n",
    "\n",
    "    # Process seqs tensor\n",
    "    processed_seqs = tf.map_fn(base_hot_encoder, flattened_seqs, fn_output_signature=tf.int32)\n",
    "    print(processed_seqs.shape)\n",
    "    x_seqs = tf.reshape(processed_seqs, shape=(-1, 50, 5))\n",
    "    print(x_seqs.shape)\n",
    "    duration = (datetime.now()-now).total_seconds() - duration\n",
    "    print('>>', duration)\n",
    "\n",
    "    # print(x_seqs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: This Method is very slow. \n",
    "Saved output for batch size of 1024\n",
    "```\n",
    "    Create text line dataset\n",
    "    >> 0.020242\n",
    "\n",
    "    Load and split lines in three sections:\n",
    "    (1024, 3)\n",
    "    tf.Tensor(\n",
    "    [[b'CCATCGGCGTCCCGGAATCGTATACCGGGCACACGAAGCGTTATAACAAT' b'6' b'8']\n",
    "    [b'GTGGCTATGGTGAGGAGTTTGGAGGAAATAATATACATCATATATTCTGA' b'6' b'4']\n",
    "    [b'CCCTCTTCTGCAGACTGCTTACGGTTTCGTCCGTGTTGCAGTCGATTATC' b'117' b'0']], shape=(3, 3), dtype=string)\n",
    "    >> 0.014626000000000004\n",
    "\n",
    "    Split string tensor into three tensors:\n",
    "    (1024, 50) (1024,) (1024,)\n",
    "    >> 0.04225999999999999\n",
    "\n",
    "    One-Hot-Encode labels\n",
    "    (1024, 187)\n",
    "    tf.Tensor(\n",
    "    [[0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
    "    [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
    "    [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]], shape=(3, 15), dtype=float32)\n",
    "    >> 0.023466000000000015\n",
    "\n",
    "    One-Hot-Encode positions\n",
    "    (1024, 10)\n",
    "    tf.Tensor(\n",
    "    [[0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
    "    [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
    "    [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]], shape=(3, 10), dtype=float32)\n",
    "    >> 0.049815999999999985\n",
    "\n",
    "    Base-Hote-Encode the sequences\n",
    "    (51200, 5)\n",
    "    (1024, 50, 5)\n",
    "    >> 154.10547400000002\n",
    "```\n",
    "See next cell for much faster method, "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other approach, faster by avoiding case function, and using simple operations. Typical results are:\n",
    "\n",
    "```\n",
    "    Create text line dataset\n",
    "    >> 0.018218\n",
    "\n",
    "    Load and split lines in three sections:\n",
    "    (1024, 3)\n",
    "    >> 0.026377999999999995\n",
    "\n",
    "    Split string tensor into three tensors:\n",
    "    (1024, 50) (1024,) (1024,)\n",
    "    >> 0.054369\n",
    "\n",
    "    One-Hot-Encode labels\n",
    "    (1024, 187)\n",
    "    >> 0.041302000000000005\n",
    "\n",
    "    One-Hot-Encode positions\n",
    "    (1024, 10)\n",
    "    >> 0.063554\n",
    "\n",
    "    Base-Hote-Encode the sequences\n",
    "    (1024, 50, 5)\n",
    "    >> 0.095141\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create text line dataset\n",
      ">> 0.03397\n",
      "\n",
      "Load and split lines in three sections:\n",
      "(1024, 3)\n",
      "tf.Tensor(\n",
      "[[b'CCATCGGCGTCCCGGAATCGTATACCGGGCACACGAAGCGTTATAACAAT' b'6' b'8']\n",
      " [b'GTGGCTATGGTGAGGAGTTTGGAGGAAATAATATACATCATATATTCTGA' b'6' b'4']\n",
      " [b'CCCTCTTCTGCAGACTGCTTACGGTTTCGTCCGTGTTGCAGTCGATTATC' b'117' b'0']\n",
      " ...\n",
      " [b'AAGTGCATTCAAGTTTTAATTGATATTTAGTTATGTAGTCATTTAGAGTA' b'115' b'3']\n",
      " [b'AGAAGCTGGCTCCGGAGCAGCAGTAGAGGGAAAACCACGGAGGCNGACAG' b'62' b'7']\n",
      " [b'CCTTGGTGAAGGTATTAACAAATCGATTAAGTTGGGAGGGATGCATGCGA' b'27' b'1']], shape=(1024, 3), dtype=string)\n",
      ">> 0.008703000000000002\n",
      "\n",
      "Split string tensor into three tensors:\n",
      "(1024, 50) (1024,) (1024,)\n",
      ">> 0.06675\n",
      "\n",
      "One-Hot-Encode labels\n",
      "(1024, 187)\n",
      "tf.Tensor(\n",
      "[[0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]], shape=(3, 15), dtype=float32)\n",
      ">> 0.014312999999999992\n",
      "\n",
      "One-Hot-Encode positions\n",
      "(1024, 10)\n",
      "tf.Tensor(\n",
      "[[0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]], shape=(3, 10), dtype=float32)\n",
      ">> 0.07048900000000001\n",
      "\n",
      "Base-Hote-Encode the sequences\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 50), dtype=string, numpy=\n",
       "array([[b'C', b'C', b'A', b'T', b'C', b'G', b'G', b'C', b'G', b'T', b'C',\n",
       "        b'C', b'C', b'G', b'G', b'A', b'A', b'T', b'C', b'G', b'T', b'A',\n",
       "        b'T', b'A', b'C', b'C', b'G', b'G', b'G', b'C', b'A', b'C', b'A',\n",
       "        b'C', b'G', b'A', b'A', b'G', b'C', b'G', b'T', b'T', b'A', b'T',\n",
       "        b'A', b'A', b'C', b'A', b'A', b'T'],\n",
       "       [b'G', b'T', b'G', b'G', b'C', b'T', b'A', b'T', b'G', b'G', b'T',\n",
       "        b'G', b'A', b'G', b'G', b'A', b'G', b'T', b'T', b'T', b'G', b'G',\n",
       "        b'A', b'G', b'G', b'A', b'A', b'A', b'T', b'A', b'A', b'T', b'A',\n",
       "        b'T', b'A', b'C', b'A', b'T', b'C', b'A', b'T', b'A', b'T', b'A',\n",
       "        b'T', b'T', b'C', b'T', b'G', b'A'],\n",
       "       [b'C', b'C', b'C', b'T', b'C', b'T', b'T', b'C', b'T', b'G', b'C',\n",
       "        b'A', b'G', b'A', b'C', b'T', b'G', b'C', b'T', b'T', b'A', b'C',\n",
       "        b'G', b'G', b'T', b'T', b'T', b'C', b'G', b'T', b'C', b'C', b'G',\n",
       "        b'T', b'G', b'T', b'T', b'G', b'C', b'A', b'G', b'T', b'C', b'G',\n",
       "        b'A', b'T', b'T', b'A', b'T', b'C']], dtype=object)>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1024, 50, 5)\n",
      ">> 0.02928199999999999\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(TensorShape([1024, 50, 5]), TensorShape([1024, 187]), TensorShape([1024, 10]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "now = datetime.now()\n",
    "batch_size = 1024\n",
    "\n",
    "# Create a text dataset with bach and retrieve the first batch with an iterator\n",
    "print('Create text line dataset')\n",
    "ds = text_ds.batch(batch_size)\n",
    "it = iter(ds)\n",
    "b = next(it)\n",
    "\n",
    "duration = (datetime.now()-now).total_seconds()\n",
    "print('>>', duration)\n",
    "\n",
    "# EXPERIMENT WITH THE TRANSFORM\n",
    "\n",
    "# 1. Split the string in three: \n",
    "# Notes:\n",
    "# tf.strings.split returns a ragged tensor. It must be converted into a normal tensor\n",
    "# Tensor shape is (batch size, 3), one sequences, one for labels and one for position. Dtype is all tf.strings\n",
    "t = tf.strings.split(b, '\\t').to_tensor(default_value = '', shape=[None, 3])\n",
    "print('\\nLoad and split lines in three sections:')\n",
    "print(t.shape)\n",
    "print(t[:, :])\n",
    "duration = (datetime.now()-now).total_seconds() - duration\n",
    "print('>>', duration)\n",
    "\n",
    "# 2. Split into sequences, labels and positions\n",
    "print('\\nSplit string tensor into three tensors:')\n",
    "# Split string sequence in a sequence of single base strings \n",
    "seqs = tf.strings.bytes_split(t[:, 0]).to_tensor(shape=(None, 50))\n",
    "# Labels and Posisionts are converted from tf.string to tf.int32\n",
    "labels = tf.strings.to_number(t[:, 1], out_type=tf.int32)\n",
    "pos = tf.strings.to_number(t[:, 2], out_type=tf.int32)\n",
    "print(seqs.shape, labels.shape, pos.shape)\n",
    "duration = (datetime.now()-now).total_seconds() - duration\n",
    "print('>>', duration)\n",
    "\n",
    "# 3. One-Hot-Encode labels, using only tf functions for performance\n",
    "print('\\nOne-Hot-Encode labels')\n",
    "n_labels = 187\n",
    "y_labels = tf.gather(tf.eye(n_labels), labels)\n",
    "print(y_labels.shape)\n",
    "print(y_labels[:3, :15])\n",
    "duration = (datetime.now()-now).total_seconds() - duration\n",
    "print('>>', duration)\n",
    "\n",
    "# 4. One-Hot-Encode positions, using only tf functions for performance\n",
    "print('\\nOne-Hot-Encode positions')\n",
    "n_pos = 10\n",
    "y_pos= tf.gather(tf.eye(n_pos), pos)\n",
    "print(y_pos.shape)\n",
    "print(y_pos[:3])\n",
    "duration = (datetime.now()-now).total_seconds() - duration\n",
    "print('>>', duration)\n",
    "\n",
    "# 5. Base-Hot-Encode sequences\n",
    "# Each base letter (A, C, G, T, N) is replaced by a OHE vector\n",
    "# Notes:\n",
    "# a. the batch of sequence seqs has a shape (batch_size, 50) after splitting each byte. \n",
    "#    need to flatten it to apply the transform on each base, then reshape to original shape\n",
    "# b. We need to map each letter to one vector/tensor. \n",
    "#    Using tf.case seems slow. Trying other approach: \n",
    "#       - Convert bytes seqs in integer sequence (uint8 to work byte by byte)\n",
    "#       - Make a matrix with one column for each of the 5 base letters\n",
    "#       - Create BHE tensor as a matrix multiplication with the 5 base tensors [1, 0, 0, 0, 0] ...\n",
    "# \n",
    "print('\\nBase-Hote-Encode the sequences')\n",
    "display(seqs[:3])\n",
    "seqs_uint8 = tf.io.decode_raw(seqs, out_type=tf.uint8)\n",
    "    # note: tf.io.decode_raw adds one dimension at the end in the process\n",
    "    #       [b'C', b'A', b'T'] will return [[67], [65], [84]] and not [67, 65, 84]\n",
    "    #       this is actually what we want to contatenate the values for each base letter\n",
    "# display(seqs_uint8[:3, :])\n",
    "\n",
    "A, C, G, T, N = 65, 67, 71, 84, 78\n",
    "\n",
    "# Create 5 tensors seqs_X where the value of a base in the sequence is 1 if the base is X and 0 otherwise.\n",
    "# We do that with a boolean slicing casted as a float32 (0.0 or 1.0)\n",
    "# The final tensor is the concatenation of these 5 tensors\n",
    "seqs_A = tf.cast(seqs_uint8 == A, tf.float32)\n",
    "seqs_C = tf.cast(seqs_uint8 == C, tf.float32)\n",
    "seqs_G = tf.cast(seqs_uint8 == G, tf.float32)\n",
    "seqs_T = tf.cast(seqs_uint8 == T, tf.float32)\n",
    "seqs_N = tf.cast(seqs_uint8 == N , tf.float32)\n",
    "x_seqs_2 = tf.concat([seqs_A, seqs_C, seqs_G, seqs_T, seqs_N], axis=2)\n",
    "print(x_seqs_2.shape)\n",
    "duration = (datetime.now()-now).total_seconds() - duration\n",
    "print('>>', duration)\n",
    "\n",
    "x_seqs_2.shape, y_labels.shape, y_pos.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note on iterators in python doc\n",
    "- [iterator](https://docs.python.org/3.8/glossary.html#term-iterator)\n",
    "```\n",
    "    The iterator objects themselves are required to support the following two methods, which together \n",
    "    form the iterator protocol:\n",
    "\n",
    "    iterator.__iter__()\n",
    "    Return the iterator object itself. This is required to allow both containers and iterators to be used with \n",
    "    the for and in statements. This method corresponds to the tp_iter slot of the type structure \n",
    "    for Python objects in the Python/C API.\n",
    "\n",
    "    iterator.__next__()\n",
    "    Return the next item from the container. If there are no further items, raise the StopIteration exception. \n",
    "    This method corresponds to the tp_iternext slot of the type structure for Python objects in the Python/C API.\n",
    "```\n",
    "- [`iter()`](https://docs.python.org/3.8/library/functions.html?highlight=iter#iter) `.__iter__()`\n",
    "```\n",
    "    iter(object[, sentinel])\n",
    "    Return an iterator object. The first argument is interpreted very differently depending on the presence of the \n",
    "    second argument. Without a second argument, object must be a collection object which supports the iteration \n",
    "    protocol (the __iter__() method), or it must support the sequence protocol (the __getitem__() method with integer \n",
    "    arguments starting at 0). If it does not support either of those protocols, TypeError is raised. If the second \n",
    "    argument, sentinel, is given, then object must be a callable object. The iterator created in this case will call \n",
    "    object with no arguments for each call to its __next__() method; if the value returned is equal to sentinel, \n",
    "    StopIteration will be raised, otherwise the value will be returned.\n",
    "```\n",
    "- [`next()`](https://docs.python.org/3.8/library/functions.html?highlight=iter#next) `.__next__()`\n",
    "``` \n",
    "    next(iterator[, default])\n",
    "    Retrieve the next item from the iterator by calling its __next__() method. \n",
    "    If default is given, it is returned if the iterator is exhausted, otherwise StopIteration is raised.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<list_iterator>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequence = [1, 2, 3, 4, 5, 6]\n",
    "it = iter(sequence)\n",
    "it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<list_iterator>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "it.__iter__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 2)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(it), next(it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 4)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "it.__next__(), it.__next__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "6\n",
      "Iterator reached the end of the sequence\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    print(next(it))\n",
    "    print(next(it))\n",
    "    print(next(it))\n",
    "    print(next(it))\n",
    "    print(next(it))\n",
    "except StopIteration:\n",
    "    print('Iterator reached the end of the sequence')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare slow and fast methods to show result are the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.array_equal(x_seqs.numpy().astype('float'), x_seqs_2.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is much faster and provides the same results:\n",
    "- former method: > 150 sec for batch of 1024\n",
    "- new method: 0.03 sec for same batch size\n",
    "\n",
    "And the result is the same"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strings_to_tensors(b):\n",
    "    \"\"\"Convert batch of strings into three tensors: (x_seqs, (y_labels, y_pos))\"\"\"\n",
    "    \n",
    "    # Split the string in three : returns a ragged tensor which needs to be converted into a normal tensor\n",
    "    t = tf.strings.split(b, '\\t').to_tensor(default_value = '', shape=[None, 3])\n",
    "\n",
    "    # Split string sequence into a sequence of single base strings \n",
    "    seqs = tf.strings.bytes_split(t[:, 0]).to_tensor(shape=(None, 50))\n",
    "\n",
    "    # OHE labels\n",
    "    n_labels = 187\n",
    "    y_labels = tf.strings.to_number(t[:, 1], out_type=tf.int32)\n",
    "    y_labels = tf.gather(tf.eye(n_labels), y_labels)\n",
    "\n",
    "    # OHE positions\n",
    "    n_pos = 10\n",
    "    y_pos = tf.strings.to_number(t[:, 2], out_type=tf.int32)\n",
    "    y_pos= tf.gather(tf.eye(n_pos), y_pos)\n",
    "\n",
    "    # BHE sequences\n",
    "    # Each base letter (A, C, G, T, N) is replaced by a OHE vector\n",
    "    # Notes:\n",
    "    # a. the batch of sequence seqs has a shape (batch_size, 50) after splitting each byte. \n",
    "    #    need to flatten it to apply the transform on each base, then reshape to original shape\n",
    "    # b. We need to map each letter to one vector/tensor. \n",
    "    #    Using tf.case seems slow. Trying other approach: \n",
    "    #       - Convert bytes seqs in integer sequence (uint8 to work byte by byte)\n",
    "    #       - For each base letter (A, C, G, T, N) create one tensor (batch_size, 50)\n",
    "    #         Value is 1 if it is the base in the sequence, otherwise\n",
    "    #       - Concatenate these 5 tensors into a tensor of shape (batch_size, 50, 5)\n",
    " \n",
    "    seqs_uint8 = tf.io.decode_raw(seqs, out_type=tf.uint8)\n",
    "        # note: tf.io.decode_raw adds one dimension at the end in the process\n",
    "        #       [b'C', b'A', b'T'] will return [[67], [65], [84]] and not [67, 65, 84]\n",
    "        #       this is actually what we want to contatenate the values for each base letter\n",
    "\n",
    "    # base_codes = tf.constant(['A', 'C', 'G', 'T', 'N'], dtype=tf.string)\n",
    "    # A, C, G, T, N = tf.reshape(tf.io.decode_raw(base_codes, out_type=tf.uint8), shape=(-1)).numpy()\n",
    "    A, C, G, T, N = 65, 67, 71, 84, 78\n",
    "\n",
    "    seqs_A = tf.cast(seqs_uint8 == A, tf.float32)\n",
    "    seqs_C = tf.cast(seqs_uint8 == C, tf.float32)\n",
    "    seqs_G = tf.cast(seqs_uint8 == G, tf.float32)\n",
    "    seqs_T = tf.cast(seqs_uint8 == T, tf.float32)\n",
    "    seqs_N = tf.cast(seqs_uint8 == N , tf.float32)\n",
    "\n",
    "    x_seqs = tf.concat([seqs_A, seqs_C, seqs_G, seqs_T, seqs_N], axis=2)\n",
    "\n",
    "    return (x_seqs, (y_labels, y_pos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2,), dtype=string, numpy=\n",
       "array([b'CCATCGGCGTCCCGGAATCGTATACCGGGCACACGAAGCGTTATAACAAT\\t6\\t8',\n",
       "       b'GTGGCTATGGTGAGGAGTTTGGAGGAAATAATATACATCATATATTCTGA\\t6\\t4'],\n",
       "      dtype=object)>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(TensorShape([1024]),\n",
       " TensorShape([1024, 50, 5]),\n",
       " TensorShape([1024, 187]),\n",
       " TensorShape([1024, 10]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "display(b[:2])\n",
    "x_seqs, (y_labels, y_pos) = strings_to_tensors(b)\n",
    "b.shape, x_seqs.shape, y_labels.shape, y_pos.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strings_to_tensors_old(b):\n",
    "    \"\"\"Convert batch of strings into three tensors: (x_seqs, (y_labels, y_pos))\"\"\"\n",
    "    \n",
    "    # Split the string in three : returns a ragged tensor which needs to be converted into a normal tensor\n",
    "    t = tf.strings.split(b, '\\t').to_tensor(default_value = '', shape=[None, 3])\n",
    "\n",
    "    # Split string sequence into a sequence of single base strings \n",
    "    seqs = tf.strings.bytes_split(t[:, 0]).to_tensor(shape=(None, 50))\n",
    "\n",
    "    # OHE labels\n",
    "    n_labels = 187\n",
    "    y_labels = tf.strings.to_number(t[:, 1], out_type=tf.int32)\n",
    "    y_labels = tf.gather(tf.eye(n_labels), y_labels)\n",
    "\n",
    "    # OHE positions\n",
    "    n_pos = 10\n",
    "    y_pos = tf.strings.to_number(t[:, 2], out_type=tf.int32)\n",
    "    y_pos= tf.gather(tf.eye(n_pos), y_pos)\n",
    "\n",
    "    # BHE sequences\n",
    "\n",
    "    # flatten the sequence tensor to map bytes (b'A', ...) to BHE vector\n",
    "    flattened_seqs = tf.reshape(seqs, shape=[-1])\n",
    "\n",
    "    def base_hot_encoder(a):\n",
    "       \n",
    "        # Define the encoding functions returning the encoding tensor for each base\n",
    "        def encode_A(): return tf.constant([1,0,0,0,0])\n",
    "        def encode_C(): return tf.constant([0,1,0,0,0])\n",
    "        def encode_G(): return tf.constant([0,0,1,0,0])\n",
    "        def encode_T(): return tf.constant([0,0,0,1,0])\n",
    "        def encode_N(): return tf.constant([0,0,0,0,1])\n",
    "\n",
    "        # Define the mapping, as a list of tuples: (condition, encoding function)\n",
    "        # values of the strings are tested against b'A', ... as bytes\n",
    "        case_mapping = [\n",
    "            (tf.math.equal(a, b'A'), encode_A),\n",
    "            (tf.math.equal(a, b'C'), encode_C),\n",
    "            (tf.math.equal(a, b'G'), encode_G),\n",
    "            (tf.math.equal(a, b'T'), encode_T),\n",
    "            (tf.math.equal(a, b'N'), encode_N)\n",
    "        ]\n",
    "\n",
    "        return tf.case(pred_fn_pairs = case_mapping)\n",
    "\n",
    "    # Process seqs tensor\n",
    "    processed_seqs = tf.map_fn(base_hot_encoder, flattened_seqs, fn_output_signature=tf.int32)\n",
    "    x_seqs = tf.reshape(processed_seqs, shape=(-1, 50, 5))\n",
    "\n",
    "    return (x_seqs, (y_labels, y_pos))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Apply transformation to text_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(TensorSpec(shape=(None, 50, None), dtype=tf.float32, name=None), (TensorSpec(shape=(None, 187), dtype=tf.float32, name=None), TensorSpec(shape=(None, 10), dtype=tf.float32, name=None)))\n"
     ]
    }
   ],
   "source": [
    "batch_size = 1024\n",
    "text_ds = tf.data.TextLineDataset(\n",
    "    filepath_train,\n",
    "    compression_type='',\n",
    "    name='text_ds'\n",
    ").batch(batch_size)\n",
    "\n",
    "tensor_ds = text_ds.map(strings_to_tensors)\n",
    "print(tensor_ds.element_spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1024, 50, 5) (1024, 187) (1024, 10)\n",
      "(1024, 50, 5) (1024, 187) (1024, 10)\n",
      "CPU times: user 63.2 ms, sys: 50.9 ms, total: 114 ms\n",
      "Wall time: 137 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for seqs_b, (labels_b, pos_b) in tensor_ds.take(2):\n",
    "    print(seqs_b.shape, labels_b.shape, pos_b.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Check that the output are the same as in the original code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the validation set and pick the first `bs` values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2048,) (2048, 50, 5) (2048, 187) (2048, 10)\n"
     ]
    }
   ],
   "source": [
    "bs = 2048\n",
    "text_ds = tf.data.TextLineDataset(\n",
    "    filepath_val,\n",
    "    compression_type='',\n",
    "    name='text_ds'\n",
    ").batch(bs)\n",
    "\n",
    "tensor_ds = text_ds.map(strings_to_tensors)\n",
    "it = iter(zip(text_ds, tensor_ds))\n",
    "b_text, (b_seqs, (b_labels, b_pos)) = next(it)\n",
    "print(b_text.shape, b_seqs.shape, b_labels.shape, b_pos.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the original code `src.preprocessing.DataGenerator_from_50mer`\n",
    "\n",
    "```python\n",
    "def __data_generation(self, index):\n",
    "        x_train=[]\n",
    "        for i in index:\n",
    "            seq=self.matrix[i]\n",
    "            seq_list=[j for j in seq]\n",
    "            x_train.append(seq_list)\n",
    "        x_train=np.array(x_train)\n",
    "        x_tensor=np.zeros(list(x_train.shape)+[5])\n",
    "        for row in range(len(x_train)):\n",
    "            for col in range(50):\n",
    "                x_tensor[row,col,d_nucl[x_train[row,col]]]=1\n",
    "        y_pos=[]\n",
    "        y_label=[self.labels[i] for i in index]\n",
    "        y_label=np.array(y_label)\n",
    "        y_label=to_categorical(y_label, num_classes=self.n_classes)\n",
    "        y_pos=[self.pos[i] for i in index]\n",
    "        y_pos=np.array(y_pos)\n",
    "        y_pos=to_categorical(y_pos, num_classes=10)\n",
    "        return x_tensor,{'output1': y_label, 'output2': y_pos}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2048, 50, 5) (2048, 187) (2048, 10)\n"
     ]
    }
   ],
   "source": [
    "def data_generation(index):\n",
    "    d_nucl={\"A\":0,\"C\":1,\"G\":2,\"T\":3,\"N\":4}\n",
    "    x_train=[]\n",
    "    for i in index:\n",
    "        seq=f_matrix[i]\n",
    "        seq_list=[j for j in seq]\n",
    "        x_train.append(seq_list)\n",
    "    x_train=np.array(x_train)\n",
    "    x_tensor=np.zeros(list(x_train.shape)+[5])\n",
    "    for row in range(len(x_train)):\n",
    "        for col in range(50):\n",
    "            x_tensor[row,col,d_nucl[x_train[row,col]]]=1\n",
    "    y_pos=[]\n",
    "    y_label=[f_labels[i] for i in index]\n",
    "    y_label=np.array(y_label)\n",
    "    y_label=to_categorical(y_label, num_classes=187)\n",
    "    y_pos=[f_pos[i] for i in index]\n",
    "    y_pos=np.array(y_pos)\n",
    "    y_pos=to_categorical(y_pos, num_classes=10)\n",
    "    return x_tensor, y_label, y_pos\n",
    "\n",
    "f_matrix,f_labels,f_pos=get_kmer_from_50mer(filepath_val, max_seqs=bs)\n",
    "x_seqs_orig, y_label_orig, y_pos_orig = data_generation(list(range(bs)))\n",
    "print(x_seqs_orig.shape, y_label_orig.shape, y_pos_orig.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test for the BHE encoded sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed Values for x_seqs from Dataset:\n",
      "\n",
      "b'CCATCGGCGTCCCGGAATCGTATACCGGGCACACGAAGCGTTATAACAAT\\t6\\t8'\n",
      "[[0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "sample_idx = 0\n",
    "\n",
    "print('Processed Values for x_seqs from Dataset:\\n')\n",
    "print(b_text[sample_idx].numpy())\n",
    "print(b_seqs.numpy( )[sample_idx, :10, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed Values for x_seqs_orig from original code:\n",
      "\n",
      "CCATCGGCGTCCCGGAATCGTATACCGGGCACACGAAGCGTTATAACAAT\n",
      "[[0 1 0 0 0]\n",
      " [0 1 0 0 0]\n",
      " [1 0 0 0 0]\n",
      " [0 0 0 1 0]\n",
      " [0 1 0 0 0]\n",
      " [0 0 1 0 0]\n",
      " [0 0 1 0 0]\n",
      " [0 1 0 0 0]\n",
      " [0 0 1 0 0]\n",
      " [0 0 0 1 0]]\n"
     ]
    }
   ],
   "source": [
    "print('Processed Values for x_seqs_orig from original code:\\n')\n",
    "print(f_matrix[sample_idx])\n",
    "print(x_seqs_orig[sample_idx, :10, :].astype('int'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2048, 50, 5)\n",
      "(2048, 50, 5)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(b_seqs.numpy().shape)\n",
    "print(x_seqs_orig.astype('int').shape)\n",
    "np.array_equal(b_seqs.numpy(),x_seqs_orig.astype('int') )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test for the labels and Position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2048, 187) (2048, 187)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(b_labels.shape, y_label_orig.shape)\n",
    "np.array_equal(b_labels.numpy(), y_label_orig.astype('int'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2048, 10) (2048, 10)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(b_pos.shape, y_pos_orig.shape)\n",
    "np.array_equal(b_pos.numpy(), y_pos_orig.astype('int'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
